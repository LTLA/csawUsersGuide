\documentclass[12pt]{report}
\usepackage{fancyvrb,graphicx,natbib,url,comment,import,bm}
\usepackage{tikz}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
\setlist{parsep=0pt,listparindent=\parindent}

% Margins
\topmargin -0.1in
\headheight 0in
\headsep 0in
\oddsidemargin -0.1in
\evensidemargin -0.1in
\textwidth 6.5in
\textheight 8.3in

% Sweave options
\usepackage{Sweave}
\SweaveOpts{keep.source=TRUE,prefix.string=plots-com/ug,png=TRUE,pdf=FALSE}

\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl,fontsize=\footnotesize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\footnotesize}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontsize=\footnotesize}
\renewenvironment{Schunk}{\vspace{0pt}}{\vspace{0pt}}

\DefineVerbatimEnvironment{Rcode}{Verbatim}{fontsize=\footnotesize}
\newcommand{\edger}{edgeR}
\newcommand{\pkgname}{csaw}
\newcommand{\code}[1]{{\small\texttt{#1}}}
\newcommand{\R}{\textsf{R}}

<<results=hide,echo=FALSE>>=
#picdir <- "plots-com"
#if (file.exists(picdir)) { unlink(picdir, recursive=TRUE) }
#dir.create(picdir)
require(edgeR)
@

\begin{document}

\subsection*{General comments on DB}

\subsection*{Motivation for DB}

Looking for DB is more relevant, as you get changes in binding that you can associate with differences in biology (so you can start poking around at mechanisms). 
It's also easier than looking for absolute binding, as the null hypothesis is well defined.
You don't need to construct a background model, as you would do in single-sample peak calling.
This is laden with assumptions, which don't sound so bad qualitatively but result in uninterpretable significance statistics if you get them wrong quantitatively.
For example, MACS(2) typically churns out extraordinarily low $p$-values, mostly due to its failure to consider replicates. 
There's also the issue of conservativeness if the peak region overflows the window and gets counted as part of the background (or the same with adjacent peaks);
    and liberalness, if the surrounding regions are not a good estimate of the null enrichment, e.g., due to unmappability or because you're sitting on a repeat unit.

You don't need to compare to a negative control, as you would do in two-sample peak calling.
In particular, comparisons to input are not like-for-like, as you miss IP-specific biases when you skip the IP.
This shows up as reproducible differences in the TapeStation profiles (recall Alikesei's H3K27me3 data).
It manifests most clearly as differences in coverage across repeat regions where biases are amplified, e.g., mitochondria.
While the worst offenders can easily be filtered out, the real problem is where the biases are subtle such that they confound the actual differences.
For example, non-specific binding would show up as a putative peak as there's nothing to cancel it out in the input.
IgG's are better but they face a conundrum in that they are harder to generate as your IP procedure improves, i.e., becomes cleaner.

Of course, you could level the accusation that DB also detects a number of false positives, due to changes in chromatin structure, fragmentation, etc. between conditions.
This problem balances out the disadvantage of non-``like-for-likeness'' for a comparison to input.
Both sets of false positives are the cost of doing business in an imperfect system.
However, I would argue that the false positives in DB are \textit{slightly} less confounding.
Changes in chromatin structure are still biologically interesting, even if it wasn't quite what you were looking for with respect to DB.
In comparison, the false positives in a comparison to input would be purely technical in origin. 
In any case, these should form a minority of the detected regions in a successful experiment.

\subsubsection*{Justifying the \textit{de novo} focus}

With a new data set, it's generally a good strategy to do a \textit{de novo} analysis.
If most of the peaks occur around the TSS or genes, then you can switch to an approach using pre-defined regions. 
Otherwise, you'd be throwing out a lot of information. 
I'm guessing over 90\% of reads would be lost if you stuck to TSS-based regions.

While you might say that we are only interested in genic binding anyway, that's a narrow-minded view.
You should first see what the data has to say before diving into assumptions like that, especially for uncharacterized proteins.
There's also the issue of the difficulty in defining the location and width of the TSS and promoter.
Improper definitions may result in loss of power if the binding site is missed or includes too much background.

Besides, what's the harm? Worst is, you spend an hour or so looking at non-interesting regions. 
At best, you might be able to find something new and exciting.

\subsection*{Comments on counting}

\subsubsection*{Fragment length}

One might think of allowing the fragment length to vary within the genome, e.g., to handle heterogeneity in chromatin openness.
However, location-specific estimates for the fragment length wouldn't be very precise because you're calculating it from limited data. 
It would really only work for large peaks where the large number of reads stabilises the estimate. 
However, the advantage wouldn't be that great as results for large peaks should be fairly stable anyway.

On real data, the cross-correlation plots are one smooth peak (excluding the read-length spike). 
This suggests that there's no gross segregation of fragment lengths in the population. 
Changing the fragment length doesn't seem to change the results too much either. 
Of course, this whole discussion is moot for diffuse marks, where the extension should be irrelevant compared to the bin size.

\subsubsection*{Strand bimodality}

An anticipated attack on this approach is why I don't filter for bimodality to identify good candidate sites (a la MACS). 
This could reduce the severity of the MTC procedure. 
However, every difference is interesting, even if it doesn't necessarily occur in a nice bimodal shape (e.g. histones + other proteins).
For example, you mightn't get pronounced bimodality if one half of the peak is sitting somewhere that's unmappable or difficult to sequence. 

Defining bimodality is also difficult. 
Arbitrary thresholds for bimodality don't necessarily have a good physical interpretation. 
Of course, abundance-based filtering also requires an arbitary threshold, but I'd argue that the two cases are different.
Filtering on abundance is critical so we have to put up with it, whereas we can do without filtering on bimodality.
This is complicated by the fact that you have to combine background and enriched samples to avoid data snooping; this can dilute bimodality for smaller peaks.
A problem specific to MACS is that single-sample mode won't pick up two peaks within $\sim$10 kbp; each peak will be increase the observed background for the other.

Read extension means that bimodal structures will have higher counts than regions with the same number of reads but random read directions. 
This brings looking for bimodality under the framework of row sum filtering. 
It is also smoother; if you're not bimodal but there is a difference, you still get picked up.
This allows us to keep an open mind, and to find things that we weren't expecting to see.
Unimodal events are only problematic if they are high-abundance, which shouldn't occur if you've removed repeats.

Finally, you can do whatever bimodal filtering you want implicitly, by running your desired peak caller and then picking only those windows that overlap called peaks. 
This will give you the MTC advantage without requiring too much extra code on my part.
Alternatively, you can use the \code{checkBimodality} function.
However, a couple of notes:
\begin{itemize}
\item In addition to the reasons mentioned above, I wouldn't recommend filtering on the bimodality scores here, because I don't think it's an independent statistic. 
For purposes of convenience, I pooled the reads before counting them, such that we use Poisson means to compute the ratios.
Selection for large scores might result in selection for high abundances (due to the prior count) and selection for DB sites when one library is much larger than the other.
There's also the issue that the counts are computed on a per-base basis, rather than using window-level counts; I don't know whether this'll compromise independence.
\item Currently, \code{checkBimodality} is recommended for use as a diagnostic.
The idea is determine whether a DB region is likely to be a genuine TF binding site.
However, one counter-argument is that you could get a large region containing a bimodal site that is distinct from the DB subinterval.
In such cases, the best approach would be to reduce the clustering tolerance so that such sites are separated out.
Otherwise, we'd have to filter directly on bimodality at the window level, which I don't want to do.
\item Watch out for large peaks, because even in TF data, the strand-specific subpeaks tend to spread such that you end up with little bimodality.
The same applies for situations with many adjacent binding events, such that you end up with a histone-like broad peak.
\end{itemize}

\subsubsection*{Window correlations}

Read extension and general spatial proximity results in strong correlations between adjacent sites.
However, these correlations will not affect the estimates of the various EB statistics (only the precision of the estimates, but we don't use that anyway).
Consider a subset of sites randomly sampled throughout the genome. 
Correlations within the subset are weaker as the sites are less likely to be affected by the same reads. 
An (approximately) independent subset of sites can then be obtained by infrequent sampling. 
However, the expected composition of the sampled set is just that of the entire genome.
Thus, an independent subset will provide the same results (on average) as those obtained with the use of the entire genome.  
This is similar to the outcome of thinning in Markov chain Monte Carlo procedures.

More generally, the dispersion for each window can be considered to be sampled from the same distribution (dependent on the mean, if trended).
EB shrinkage requires estimates of the mean of this distribution, for the dispersion to shrink towards; and the variance, for the prior degrees of freedom.
For a large number of windows, you'll have enough observations such that the empirical distribution should be pretty similar to the real distribution. 
This will be true even if the windows are correlated with each other, as they should still be identically distributed.
Thus, the mean/variance estimates from the empirical distribution should be close to the true values, even in the presence of correlations.
Such correlations can be anything, e.g., due to underlying sequence features, biological correlations in binding affinity.
Of course, if the correlations are too strong (e.g., due to batch effects or other systematic biases), then all the windows in the genome might not be enough.

\subsubsection*{Windows vs. peak calling}

Peak calling fundamentally involves the clustering of windows to get a peak interval for counting.
Clustering can adjust to variable size widths, whereas the use of a large bin does not (and can encounter edge effects). 
However, this depends heavily on the decisions made by the aggregation routine, as there's often insufficient data to direct unambiguous cluster boundaries at any single site.
Results might not be robust to tuning of parameters or even new datasets (cluster groupings can change which will change the manner in which counts are collected). 
Similarly, inappropriate inclusion of background regions can be problematic for DB detection.

Clustering must also be done independently of DB, such that changes to spatial resolution also may not be appropriate, e.g., for complex DB events.
This includes DB shifting or shrinkage, or even noisy background where the surrounding non-NB regions contaminate the DB enriched signal.  
Such events are potentially interesting, with some examples below:
\begin{itemize}
\item Spreading of a repressive mark may prevent binding of regulatory factors, whereas spreading of an activating mark may allow access of specific TFs to their binding sites. 
\item A more esoteric example involves RNPII studies, if you want to distinguish between elongating and poised events. 
\item Peak shifting may be indicative of changes in nucleosome phasing and the consequences thereof, e.g., if binding sites are immobilized within nucleosomes.
\end{itemize}
The only way to find these buggers is to look for DE'ness at every place. 
In other words, we're interested in differential binding rather than absolute enrichment, and peak counts will give us the former rather than the latter.
Which is okay in other cases as you get a boost in power from larger counts (though you can twiddle the width to deal with that - and then you get into parameter dogfights).

All in all, clustering will give higher counts and better performance for clusters where the DB status is homogenous throughout the cluster. 
(That said, clustering will not guarantee that the DB status is homogenous inside the cluster
    -- you could still suboptimally detect clusters involving shifting, so it's not correct to say that you're using clustering to focus on homogenous changes. 
    The only way to guarantee homogeneity would be to do IUTs on all windows throughout the regions.)
For more complex events, clustering will probably be suboptimal.
Widths are generally a simpler alternative. 
However, it is unable to deal with variable sizes of enriched regions, requiring testing of multiple bin sizes.
Using a suboptimal value might reduce power but it shouldn't change the rankings of the regions. 

You could also get methods designed to create regions based on differences in density.
But, the magnitude of the difference required for the creation of a new region is going to be arbitrary.
A slow decline in density from high to low won't get picked up if you're looking for big differences.
MACS2 does try to identify subpeaks in large binding regions.
However, for interpretation, you'll want to report the entire region, and to do that you'll have to combine the subpeak statistics anyway, e.g., with \code{combineTests}.
In addition, the behaviour of the subpeak identification algorithm behaves may not have predictable properties regarding resolution and count size.
We might as well make our lives simpler and use windows directly.

% One might propose to call peaks in each condition, and then do a setdiff on the genomic intervals between conditions.
% This would get the unique (and shared) subintervals, which you could then count over for actual testing.
% However, this has some technical problems, e.g., how large does the interval have to be for it to be considered?
% Some threshold is required, as ambiguity in the boundaries will result in spurious ``unique'' subintervals for identical peaks.
% Moreover, there are a number of statistical problems, not least of which is the fact that you're defining condition-specific intervals.
% This will surely lead to an inflated false positive rate during testing.
%
% You could do the same thing with libraries (being blind to the condition), where you select for intervals present in at least N replicates.
% N needs to be chosen so that it is no larger than the smallest group, otherwise you would lose power to detect DB.
% However, under the null, there is no reason for the detected intervals to be in the same group, thus inflating the variance of the counts.
% This will propagate to other genomic regions if empirical Bayes shrinkage is strong, thus reducing power to detect genuine DB.
% You will also enrich for false positives where the "at least N" peaks all belong in one condition, though power is probably reduced enough to avoid liberalness.
% (The exception is if genuinely DB regions are present and drag down the common variance towards the true value, as they are not skewed by the filter.
% This will result in detection of more of these false positives.)

While I'm here, I might as well note some drawbacks with hidden Markov models.
HMMs are great for describing the correlations between states, and for using information across the genome to improve inferences.
However, I don't think they work well for DB because the state distribution of the log-fold change will vary from site to site.
In other words, different DB regions will have different (expected) log-fold changes.
Trying to model them as a single state is probably inappropriate if you want site-specific inferences.
You could treat the empirical distribution of all DB regions as a state distribution, but this means that the assignment for each site gets affected by the effect size of other sites.
For example, many DB sites with strong log-fold changes would skew up the empirical distribution for the DB state.
This would reduce the posterior probability of the DB state for a site with a lower log-fold change, even if that log-fold change is large in an absolute sense.

\subsubsection*{Window resolution}

The argument for arbitrary resolution with the window width is akin to that when using a microscope. 
Do you want to focus on the details or on the big picture? 
For example, consider a cluster of DB regions separated by empty spaces.
One could classify this cluster as part of the same region, so you would use a large width to increase counts and power. 
However, you could equally define each region as being of interest in its own right.

In the absence of any knowledge, the solution is to try multiple widths and to consolidate results.
This is usually the case with broad marks where you need a large window, but you don't know exactly how large.
With some knowledge (e.g., TF, sharp marks), it is better to just stick with a small window, with size motivated by the DNA:protein contact area.
Otherwise, additional use of larger windows (which are underpowered for such markers) would lead to increased correction penalties during consolidation and loss of power. 

I don't think using gargantuan widths to maximize power for very diffuse regions is a great idea. 
Strange things happen at low abundances (e.g. correlations found during background normalization) which complicates validation.
Ideally, local regions should be able to carry their own weight in terms of power, which makes it much more convincing when trying to eyeball the results.

Ironically, estimation of the width is easy for sharp peaks where it's irrelevant (not much practical difference between a small value and zero) but hard for diffuse regions where it is most needed.
My attempts to estimate the width include:

\begin{itemize}
\item Autocorrelations. 
These are quite ineffective in guiding the choice of width. 
This is because for diffuse marks where they are most needed, the poor density of reads in enriched regions reduces the strength of the correlations. 
This means that you almost start off at zero (whereas for more punctate marks, the correlations at small lags are much higher). 
In short, you can't distinguish between diffuse marks and random reads. 

More generally, autocorrelations aren't that great because there's nothing to compare it to. 
It keeps dropping rather than becoming maximal at any point. 
Trying to find when it elbows off is fairly arbitrary. 
Comparing it to a threshold is also difficult i.e. distinguishing between negligble' and 'even more negligble'. 
It also ceases to be robust to the choice of threshold when relative scales are used, because the function begins asymptoting way above zero.
Small changes on the x-axis correspond to very large changes on the estimated width.

The autocorrelation function also depends on other factors such as the number of peaks.
This complicates interpretation relative to a fixed threshold.  
For example, a mark with lots of small peaks will have a higher and more persistent autocorrelation than a mark with fewer peaks, even if the latter's peaks are larger. 

\item Masking reads from each strand with those from the other strand, and computing cross-correlations using the remainders. 
The maximum point should then depend on the value of the width. 
Problem is, the exact relationship depends on the shape of the clusters. 
The effectiveness of masking also depends on the shape of the cluster e.g. a triangular distribution for each peak will have fewer overlaps and reduced masking. 
Increasing the aggressiveness of the masking then raises problems with the reliability of the results (as you start hitting the isolated forward/reverse flanks).

\item Alternatively, peak-calling programs like MACS can be used in single sample mode on pooled libraries to determine the width of enriched regions.
Pooling is necessary to avoid snooping on the DB status of regions.
Note that the width of enrichment may not necessarily be the optimal width for detecting DB regions, especially for the more diffuse histone marks.
There is also an implicit assumption that the widths of the binding sites are reasonably constant across the genome. 
Finally, it depends on the resolution of the peak-caller, and whether you need to subtract the average fragment length, etc.
\end{itemize}

The current justification for the width is that it represents the protein's footprint, such that any overlapping fragment (even by only 1 bp) will be pulled down after cross-linking.
Both forward- and reverse-mapping reads are counted inside the window, in addition to strand-specific reads flanking the window and within the fragment length.
This assumes that the protein doesn't protect the bound DNA from sonication, such that fragmentation can occur inside the footprint.
That assumption seems fair enough, otherwise we would routinely see empty spaces between the forward/reverse strand peaks, and we don't (for ChIP-seq data, anyway).
If such protection was present, the width should probably be set to 10-50 bp, as the estimated fragment length would already include the size of the footprint.

\subsubsection*{Window spacing}

The obvious case is when ext=1, where you would lose reads when windows are shifted by a spacing larger than the window size.
This can only be avoided by setting width=spacing.
If ext is large, you won't lose reads so long as spacing is less than ext. 
However, if spacing is much greater than the width, a binding site between two windows will not be represented ideally by either window.
All reads associated with that binding site (+- ext) will not be counted at either point.

This is mitigated by putting an upper bound of ext/N on the spacing, such that the maximum distance of the site to any window is ext/(2N).
I think that N=2 is a good compromise between resolution and computational efficiency.
If you set the spacing too low, the penalty from Simes will be such that you'll end up using the p-value from a suboptimal window anwyay.
You could avoid this with findMaxima(), in which case you can use any spacing you like (as you'll throw out > 99\% of the windows at the end).

In general, don't obsess about the window not getting all reads associated with a binding site.
You'll lose some even if the window was right on top of the site, due to some fragments having greater-than-average fragment lengths.
Practical usability (i.e., within memory constraints) is more important than some loss of power.

\subsubsection*{Cross-correlations}

The use of CCF plots to get the average fragment length makes some assumptions about the shape of each peak. 
In particular, it assumes that the peaks are symmetrical so that the distance between the modes is equal to the average fragment length. 
I mean, it's probably not even the mean, it's just the mode of the fragment lengths. 

The spike in the plot is due to uniqueness. 
For a given background read, most aligners will prefer (or demand) unique mapping locations. 
Any unique sequence in the genome will also have a unique reverse complement sequence which is also favoured by the aligner. 
Reads end up ``stacking'' on top of these unique sequences in both the forward and reverse directions. 
This results in an increase in correlations equal to the distance between the 5' ends of the forward and reverse reads i.e. the read length. 
A spike at zero is not observed as duplicates are removed.

For enrichment with any reasonably large width, we've assumed that the binding event provides no protection during sonication. 
If it does, you should see bimodality where there's an empty gap between the forward/reverse clusters. 
The estimated value from the cross-correlations then represents the length of the gap (i.e. the width of the binding site) plus a bit extra from variability in fragmentation sites at each end. 
If this is true, you could just use the estimated fragment length without any width value because it is modelled for free.

\subsubsection*{Variable fragment lengths}

There's no satisfactory way of dealing with differences in the strand-specific subpeak widths.
You'll need to shrink wide subpeaks towards the center of the subpeak.
This requires knowledge of where the read is in the subpeak, which is difficult to say without an explicit model.
Fortunately, I don't think is a major issue.
Variability in fragment lengths precludes identification of significant differences in lengths, if you were to treat forward/reverse strand peaks as distributions. 
In the worst case, with no variability, you'd just have read stacks where differences are clearcut.
As you add more variability, the waters become muddled and obvious differences disappear.

\subsubsection*{Mapping quality and read stacks}

Read stacks are observed as large pile-ups of (slightly offset) alignments within a small genomic interval.
These can be effectively eliminated by setting a non-negligble threshold on the mapping quality.
I suspect they're formed by repeat regions, and so MAPQ scores are generally low due to non-uniqueness.
Alternatively, they might be contaminants that are just trying to find a (poorly aligned) home on the genome build.
Use of \code{discard} to ignore these regions is less effective as they regions tend not to overlap unannotated repeats.
At the very least, you'd have to use RepeatMasker annotations, which are too aggressive for my taste.

In the worst case, direct duplicate removal may be required to get rid of stacks.
It'd be nice to have a softer removal algorithm that accounts for the neighbouring coverage to determine if reads are genuine duplicates or not.
However, I think this pre-processing is beyond the scope of csaw's responsibilities.
Alignment and read quality control is done before entry into csaw, so I'd like to avoid mission creep.
Besides, pure duplicate stacks (i.e., not driven by repeats) are only really problematic in cases where you don't have biological replicates.

\subsubsection*{What to do about duplicate reads}

For single-end data, duplicate reads should not be removed.
Consider a binding site on which there is a Beta(2, 2) peak of width 200 bp.
The maximum depth on each subpeak can be simulated below, where we quickly deviate from the unity line if we remove duplicates.
Decently sized peaks (counts over 200) will already get a 2-fold decrease in coverage, which is hardly satisfactory.

<<>>=
depth <- 0:40*10
collected <- list()
for (i in seq_along(depth)) { 
    collected[[i]] <- length(table(round(200*rbeta(depth[i], 2, 2)))) 
}
plot(depth, unlist(collected))
abline(0, 1, col="red")
@

For paired-end data, the problem is much reduced.
In the case of the binding site above, you need to have exact overlaps on both ends of the pair.
Deviations are largely negligible, even at very deep coverage (4000 counts, which rarely occurs even at the strongest binding sites).
Thus, duplicate removal is probably safe here.

<<>>=
depth <- 0:40*100
collected <- list()
for (i in seq_along(depth)) { 
    end1 <- round(200*rbeta(depth[i], 2, 2))
    end2 <- round(200*rbeta(depth[i], 2, 2))
    collected[[i]] <- length(table(sprintf("%i.%i", end1, end2))) 
}
plot(depth, unlist(collected))
abline(0, 1, col="red")
@

For most analyses, we are protected against duplicates because they should occur randomly between replicates. 
This inflates the dispersion and ensures that such regions don't get spuriously detected as DB, especially when robust QL methods are used.
However, there are two caveats to this.
The first is that such regions get reduced power because we throw them away rather than attempt to salvage them.
The second is that the protection of the NB model only works for low-ish duplicate rates where stacks are small/rare.
If large read stacks are present in the same region for all libraries of a group, it \textit{will} get detected as DB.

\subsubsection*{Dealing with huge data sets}

Read depth isn't much of an issue, because everything gets counted into the same windows.
You might have more windows that don't get filtered, but that's capped at the total number of windows, and filtering afterwards will get rid of the background majority.
The bigger problem occurs when you have many samples (e.g., more than 100) such that the count matrix becomes uncomfortably large.
In such cases, it may be best to analyze each chromosome separately.
You'll lose some information for dispersion estimation because you don't get EB shrinkage across the genome, but when you have a lot of samples that's not a major problem.

\subsection*{Comments on normalization}

\subsubsection*{TMM on the binned counts}

It's better to get larger counts in the first place by using larger bins for the purpose of normalization.
Small counts have a number of problems when TMM normalization is applied:

\begin{itemize}
\item They have greater variance in the resulting fold changes.
This is because it's more possible for smaller counts to achieve a large fold change than it is for larger counts (as the former is less stable). 
That said, it's probably not the major issue when you have thousands of genes for estimation.

<<>>=
set.seed(238472)
out <- (log(rnbinom(1000, mu=2, size=100)/rnbinom(1000, mu=2, size=100)))
var(out[is.finite(out)])
var(log(rnbinom(1000, mu=20, size=100)/rnbinom(1000, mu=20, size=100)))
@

\item M-values are not defined when zero counts are around.
Currently, these are removed, which is suboptimal as it results in loss of information when you have a non-zero count paired with a zero count. 
The non-zero count can provide some evidence for undersampling (e.g. lots of [1, 0] pairs probably indicate undersampling). 
Conversely, smaller libraries with no undersampling will have more zeroes that get implicitly trimmed.
This truncates just one side of the fold change distribution prior to the actual trimming, which yields a biased estimate of the normalization factor.  

\item The solutions to zeroes are worse than the problem itself.
Adding an pseudo-count doesn't help as the M-value will change dramatically with the value of the pseudo-count when the counts are small.
This is because the choice of offset is a massive contributor to the magnitude of the resulting fold change, and thus determines where that FC is placed in the distribution.
Zero counts can only be overcome by incorporating a probability distribution to model its frequency. 
However, this results in more pain due to the need to estimate distribution parameters and the possible lack of robustness to violation of the distributional assumptions.

\item Discreteness reduces the accuracy of the normalization factor estimate -see \verb!lowcount_norm.R! for a demonstration.
This is due to reduced precision of trimming when you have blocks of genes at a particular M-value.
The trimmed mean also probably ceases to be an unbiased estimator of the true mean for non-symmetric distributions.
\end{itemize} 

If you have libraries of similar size, you \emph{could} just throw in a bunch of bin sizes and take the most extreme (i.e. different from 1) factors that you can find.
This is based on the fact that bins that are too large or too small will underestimate the magnitude of the logarithm of the scaling factor.
If bins are too large, inclusion of many DB regions squeezes the estimates towards unity, as the DB offsets the undersampling in adjacent regions.
If bins are too small, there will be more zero counts in the undersampled library; these are evidence for undersampling but get removed, shifting the estimate towards unity.
Under this reasoning, the most extreme intermediate is most likely to be closest to the true values.
However, this falls apart when the library sizes are different, as small bins will overestimate the normalization factor (see \verb!lowcount_norm.R!).

\subsubsection*{Handling the bias of the TMM method}
Here I shall list the assorted failures in my attempts to truly solve this problem for low counts.

\begin{itemize}
\item Any sort of sitewise trimming procedure seemed to fail. 
I've tried trimming on the deviance and on the p-value, neither have worked well.
I suspect it is because of the sensitivity to the dispersion estimate, which I want to avoid estimating because it's a pain. 
In particular, different dispersions will mean that tags with the same fold change but different count sizes will get trimmed differently (whereas really they should be trimmed together).
Also, the use of site-wise counts results in a highly discrete distribution where you trim off big chunks at a time. 
Instability results as you will start to remove big chunks, each corresponding to opposite sides of the fold change distribution.

\item Finding the offset with the minimal sum of deviances across sites, with re-estimation of the tagwise dispersion for an intercept-only design matrix.
The hope is that, for DB sites, the dispersion would already be so large that the deviance wouldn't change much when you altered the offsets. 
However, this didn't work out too well as the influence of the truly DB sites would still be enough to underestimate the normalization factor. 
(Funnily enough, the code below behaves without \code{prior.df=0}, due to vagaries of prior d.f. estimation -- but it makes no sense to shrink in this context!)

<<>>=
set.seed(1233)
a <- matrix(rnbinom(10000*2, mu=20, size=10), ncol=2)
a[1:2000,1] <- a[1:2000,1] * 2

design <- cbind(c(1,1))
truth <- DGEList(a, lib.size=rep(1e6, 2))
truth <- estimateDisp(truth, design, prior.df=0)
fit.truth <- glmFit(truth, design)
sum(fit.truth$deviance) # Deviance for the true offsets

fake <- DGEList(a, lib.size=c(1.1e6, 1e6)) 
fake <- estimateDisp(fake, design, prior.df=0)
fit.fake <- glmFit(fake, design)
sum(fit.fake$deviance) # Deviance for other offsets
@

An alternative would be to use a design matrix appropriate to the experimental design, and maximize the log-likelihood with a ridge regression penalization. 
This would shrink coefficients to zero for most genes but still allow DE genes with non-zero log-fold changes to be present.
The trouble is that the coefficients for the DE genes are quite large and ridge regression will also attempt to shrink them to zero.
This results in a biased outcome again -- as one might expect, because we're basically summing the squared log-fold changes.

<<>>=
set.seed(123123)
a <- matrix(rnbinom(10000*2, mu=20, size=10), ncol=2)
a[1:1000,1] <- a[1:1000,1] * 4

design <- cbind(1, 0:1)
truth.fit <- glmFit(a, design, dispersion=0.1, offset=c(0, 0))
sum(truth.fit$coefficients[,2]^2) # Deviance is implicitly zero.

fake.fit <- glmFit(a, design, dispersion=0.1, offset=c(0.1, 0))
sum(fake.fit$coefficients[,2]^2) # Lower than the truth.
@

So, it looks like we have to remove them directly to avoid problems.

\item Finding the offset where the number of sites with increasing deviance matches the number of sites with decreasing deviance.
I've also tested variants thereof, all involving numbers of sites rather than the magnitude of the difference. 
The use of the number of sites was designed to be more robust to the presence of DB sites, assuming that the majority of sites where non-DB. 
However, this turned out to be quite imprecise as you could easily get a range of offsets with the same difference in the number of increasing/decreasing sites. 
The  presence of tied counts exacerbates this for small offsets, because a little change results in (potentially) a big number of sites switching from an increase to a decrease in their deviance.

\item Summing across background enriched regions. 
The idea was that background regions should be constant, so identifying and crunching them would give us the normalization factor fairly easily. 
Basically, we identify those bins that are likely to be background regions based on their average abundance in all libraries (trim away those high-abundance bins).
We then sum across those bins in each library, and use the ratio of those sums as the effective library sizes.
As we're dealing with a sum of counts, we can get a precise value without having to worry about discreteness. 

This works great for simulated data, but strange things happen in real data. 
The proportions doesn't change monotonically with increasing abundance, as you'd expect from increasing DE contamination. 
Rather, it peaks/troughs at a row sum of 3-5 before returning to the center line. 
This isn't just some random phenomenon - the standard errors of the proportions indicate that they are reliable (assuming independence), and it can be observed in at least three separate datasets. 
My best guess to why this is happening is that there is some correlation between the regions corresponding to those abundances.
This means that they change all at once which results in a smooth peak. 
Finally, I did these for biological replicates for which no undersampling should occur at all (though that might be optimistic given the vagaries of immunoprecipitation). 

All this results in considerable instability in the normalization procedure. 
An excellent example is with the mitochondrial genome, reads for which must be due to non-specific enrichment. 
Thus, the read count of the entire genome (without duplicate removal, because it's a bit small) can be used to get the normalization factor. 
However, in practice this is a mess, with massive scaling factors often proposed between replicates.  
The total counts are high enough for a precise estimate (~ several thousand) given independence; this suggests that our inaccuracies are due to strong correlations between counts.
While there may also be additional problems for mitochondria over nuclear DNA, I think this is reasonably demonstrative of what happens with low counts.

You can try to maximize the differences in the normalization factors within a range of thresholds to define the enriched regions.
This could get weird, because of the instability in the curves at low abundances.
Thus, you'll have to set a minimum on the threshold to avoid this.
The curve should pass through (and stay at) 0 as it switches from suppressed background to DE, so the greatest difference in the normalization factor will occur at the minimum threshold.
This means that there won't be any difference from the static method just using the minimum threshold directly.
\end{itemize}

\subsubsection*{TMM on high-abundance regions}

You can also run TMM on high-abundance bins instead of windows.
This uses larger counts that will be easier to visualize in a MA plot (can also see the behaviour of the entire genome on the plot).
However, I prefer running it on the same windows you're going to use for the analysis.
This synchronizes the normalization with the filtering, and guarantees that you're eliminating the biases in the counts of interest.

Using window counts for TMM is also a step closer to the procedure for non-linear normalization.
This means you can easily switch to that, e.g., if you didn't filter away all of the background and you want to normalize them as well.
However, I generally find that it's quite difficult to identify a trend when you have window-based counts, as the covariate range is usually too small.
This motivates the use of scaling normalization, even in the likely presence of some background windows.
(Non-linear methods also have some other problems that are discussed below.)

Note that each read is counted multiple times with windows instead of bins, though this isn't a real problem.
The only thing to worry about is that it will effectively upweight high-abundance peaks which get caught by multiple windows, but everyone should have high abundances so it shouldn't matter.

\subsubsection*{How \textit{not} to decide between normalization methods}

Just because a high-abundance mass at a non-zero M-value is discrete, doesn't mean it's caused by efficiency differences.
You could imagine a consistent fold increase in the counts from a genuine increase in binding, if the protein level or deposition rate doubled in one condition.
Obviously, if you don't see a non-zero blob, then the argument is academic, as there's no difference between the two normalization methods.

You might also imagine that the presence of a high-abundance mass at a non-zero M-value between replicates would represent efficiency bias.
This is true, but it doesn't mean that binding is constant between conditions.
The variability might mask any DB, and absence of evidence is not evidence of absence, especially at low numbers of replicates.
Eliminating the efficiency bias will compromise detection of systematic DB, so you'll have to pick the lesser of two evils.
If you've enough replicates, you could make reliable inferences about the true overall DB, but that's rare for ChIP-seq data.

Conversely, the presence of a non-zero mass between conditions but not between replicates does not mean that efficiency bias is absent.
While stochastic efficiency bias mightn't be generated between replicates, a systematic bias might be present between conditions.
This may be caused by condition-specific factors, e.g., differences in nuclear shape, changes in cross-linking due to serum concentrations.
The optimal sonication conditions are particularly dependent on the cell type, see PMC3541830 and PMC4151291.

\subsubsection*{The lesser of two evils in normalization choice}

Normalization to remove composition biases may be undoubtedly correct in some cases, e.g., where you reduce the levels of the target protein.
However, if the IP step is highly variable, you may end up failing to detect anything at all, as the dispersion will be inflated by this normalization strategy.
In such cases, it may be preferable to normalize to remove efficiency biases.
Then, only the DB sites that change in the expected direction should be retained.
This normalizes across the majority of the expected DB binding sites, sacrificing them in order to detect the top set of DB.
For experiments with variable IP efficiency, this may still detect more sites than normalizing for composition bias.

Another approach, if you have enough replicates, is to normalize for composition biases and use methods like \textit{RUVseq} or \textit{sva}.
If the design matrix is supplied, these methods should be able to detect systematic differences in binding efficiency across enriched regions.
This will allow you to block on efficiency in the design matrix, while preserving the mean shift in binding intensity between conditions.
The idea is that the estimated blocking factor should be more-or-less orthogonal to the condition factor,
though obviously this requires a large number of samples to get a precise estimate of the mean shift in the first place.

\subsubsection*{Some thoughts on non-linear normalization with NB-loess}

The current implementation of NB-loess uses a fast approach, whereby all samples are compared to an average library.
This is valid as you're just using the average as a proxy between which libraries are compared.
Fitting a trend to the "(log-count - average)" against "average" for each library is equivalent to fitting a trend to "log-count" against "average" and then subtracting "average" from the fitted value.
However, subtraction of "average" cancels out between libraries anyway -- so, to save time, we just fit a trend to "log-count" against "average" in \code{normOffsets}.
You don't need to worry about comparing each library to, e.g., the average of all other libraries, which gets complicated when you try to make the offsets comparable.

The average count is more robust to zeroes than the A-value in conventional fast loess.
A DB window with zero counts in samples for one condition will have a low A-value, possibly dropping below the range of covariates for non-DB windows.
As a result, the assumption that most windows are non-DB at each covariate interval would be false.
This is especially true after filtering such that the bulk of remaining non-DB windows are of relatively high abundance.
(Not filtering at all would cause problems as the bulk of low-abundance windows would dominate the loess span calculations, as well as causing problems with discreteness.)
The average count doesn't drop as quickly and cannot go below the supported range if you filtered on the average count in the first place.

Another way to look at it is to consider a 2-vs-2 design, in which the counts are zero in one group.
Computing the A-value would be equivalent to taking the square root of the non-zero binding intensity.
In contrast, computing the log-average count would be equivalent to halving the non-zero binding intensity.
For coverage greater than 2, the result of the latter will always be larger than that of the former.
This reasoning also explains why you can't just add a prior count;
it is easy to show that the prior required to bring it up to the filter threshold would be comparable to the threshold itself, thus distorting the trend fit.

Of course, one could argue that the drop in the A-value is beneficial as high-abundance DB sites would be dragged down towards the majority of non-DB windows.
This is true but less of a concern as we have filtered out low-abundance windows.
There may or may not be non-DB support at high abundances, but there is definitely no support at low abundances after filtering.

\subsubsection*{Caveats with using the overall average in non-linear methods}

When you have DE, the concept of the overall average makes no sense.
This means that the point will get misplaced and normalized in strange ways.
Normally, this isn't a problem, as the correct normalization for DE genes isn't well defined.
However, if you have more than two groups, DE in the third group would affect normalization for non-DE genes between the first two groups.

This is shown in the example below.
The first gene displayed should have no differences between the first two samples upon normalization, but DE in the third sample prevents this from happening.
A similar case can be made for the second and third genes, in which there is DE in the second and first samples respectively.
In contrast, the last gene is non-DE across all groups and behaves properly.

<<echo=FALSE>>=
set.seed(231232)
@

<<>>=
ngenes <- 10000
true.means <- runif(ngenes, 1, 10)
y <- matrix(0, nrow=ngenes, ncol=3)
y[,1] <- rnorm(ngenes, mean=true.means)
y[,2] <- rnorm(ngenes, mean=2*true.means)
y[,3] <- rnorm(ngenes, mean=1.5*true.means)
y <- rbind(y, c(2, 4, 10), c(2, 10, 3), c(10, 4, 3), c(2,4,3))	
y.fast <- normalizeCyclicLoess(y, method="fast")
tail(y.fast, 4)
@

One might think to overcome this problem by using covariates computed in a pairwise manner.
This fixes the first gene, but the second and third genes still have spurious DE.
This is because the behaviour of this procedure depends on the order with which the libraries are normalized.
In the first gene, the first two libraries are equalized first, which results in consistent behaviour for those two libraries across the remaining normalization steps.
This is not the case for the second and third genes, where DE inflates the covariate in the first pairwise normalization.
This distorts the normalized value and affects the outcome of all remaining normalization steps.

<<>>=
y.pair <- normalizeCyclicLoess(y, method="pair") 
tail(y.pair, 4)
@

Affymetrix-based normalization is a slight modification of the pairwise strategy, where the adjustment to the values is made after all pairwise normalization steps are performed.
This does somewhat better, though the essence of the problem still remains.
Note that changing the number of iterations in both cases doesn't seem to help.

<<>>=
y.affy <- normalizeCyclicLoess(y, method="affy") 
tail(y.affy, 4)
@

Comparisons to a reference don't help consistently, as it depends on the choice of reference.
In the code below, each reference fails for the gene in which DE is present in that reference.
Another disadvantage of pairwise covariates is that they are less stable than the overall mean.
This results in a modest increase in the normalization error, though nothing to really panic about.

<<>>=
for (ref in 1:3) {
    y.x <- y
    cat("Current reference is", ref, "\n")
    for (x in 1:3) { 
        if (x==ref) { next }
        fit <- loessFit(y[,x] - y[,ref], rowMeans(y[,c(x, ref)]))
        y.x[,x] <- y.x[,x] - fit$fitted
    }
    print(tail(y.x, 4))
}
@

In short, there's no obvious solution to this problem.
To resolve it, you would need to know the true abundance for each observation in order to estimate the offset from the most appropriate point in the trend.
However, we wouldn't need to normalize if we knew this!
I'm going to stick with the what I've got right now, as it's easy to use and understand. 

Additionally, if we consider efficiency bias to be the cause of the trend, then abundance is effectively assumed to be proportional to binding intensity.
This is probably true in general, but deviations from this relationship will reduce the accuracy of normalization.

\subsubsection*{Dealing with other biases}

Note that Cheung (i.e. Ahringer) go about dividing ChIP samples by input controls and stating that there are differences in the called regions between replicates. 
If they had used a model that accounted for replicates in the first place, different results should be avoided as the dispersion would be higher.  
Also, their method for avoiding biological signal in their background model depends on a powerful peak caller to mark enriched regions. 
Absence of evidence isn't evidence of absence, so if your peak caller is weak, the method will be confounded by biological signal that leaks through your filtering step.  
Risso's definition of bias involves comparisons between qPCR and RNA-seq fold changes. 
I suppose this is fair enough if you want to improve comparisons between technologies. 
However, this isn't particularly relevant to the correction of biases between samples.

Generally, it's worth dividing GC bias into two types:
\begin{itemize}
\item The first is where the true variances of independent sites are correlated with GC content. 
This suggests that you might get more appropriate smoothing if you partitioned the sites according to GC content first. 
This isn't too hard (find regions with desired GC content and split windows accordingly) but you will bleed degrees of freedom at high counts where the number of sites isn't that high to begin with. 
GC content is mostly interesting because of its effect on read abundance, and we can smooth directly on that anyway. 
Otherwise, it's just another metric. 
We could also smooth on other (equally arbitrary) parameters such as genomic context, position to promoter, etc. but we have to draw the line somewhere. 

\item The second type is that there is a systematic bias in the means of sites with similar GC contents.
This is what you usually see when someone talks about GC bias correction i.e. the boxplot of ratios of counts with similar GC contents need to be scaled to the y=0 line. 
It is reasonable to correct for these, though it may result in loss of power if GC content is correlated with DB.
Also, the correction uses offsets which might not be compatible with non-linear normalization.
Given a choice, I'd prefer to remove any trended bias directly, as I \textit{know} it would mess with my mean-dispersion trend estimation.
Besides, the GC content vs. M-value plots are pretty tame; there's not much of a trend as the bulk of points occur in a limited range of GC contents.
(Specifically, the noise around the trend is larger than the range of covariates, making it difficult to pull out a trend.)
\end{itemize}

The second type of GC biases will just be absorbed into the dispersion estimates, inflating them for affected windows along with the variability of the dispersions.
(The latter assumes that most windows are unaffected by GC biases after scaling normalization, such that the inflated dispersions will decrease the prior degrees of freedom.)
As such, we'd be erring on the side of conservativeness if we failed to normalize out GC content.
One could argue that the EB shrinkage statistics won't be precisely estimated if there's a stochastic sample-specific effect that hasn't been normalized out.
However, if only a minority of windows are affected (at the ends of the trend, generally with unusually high or low content), then this shouldn't be a problem.
In such cases, the GC bias would just introduce some correlations between a small subset of windows - no problems.

\subsubsection*{Why spike-ins work}

The key part of the spike-in procedure is that whatever spike-in you're using, it needs to contain the same target as your sample of interest.
Moreover, the antibody must be able to recognise the target in both species (see Discussion of Bonhoure \textit{et al.}, 2014).
This ensures that any differences in IP efficiency are reflected in the coverage of the spike-ins.
Otherwise, it wouldn't provide any advantage over normalizing across background regions.
This is fine for histone marks that tend to be ubiquitous across species, but less so for specific proteins, e.g., transcription factors with species-specific epitopes.

\subsection*{Comments on filtering}

\subsubsection*{Further on the motivation for filtering}

The main argument for filtering - that low-abundance windows don't have enough power for DB - is true to a point, but not near the thresholds used in actual analyses.
You'll still be able reject the null for windows with consistently low counts in one group and zeroes in the other group.
Even accounting for the fact that the trended NB dispersion is very high at low counts, you can get low p-values if the sample size is large enough.
The real motivations include the need to avoid discreteness (and other oddities at low counts) during mean-variance modelling, where discreteness can inflate the prior d.f.;
    avoiding discreteness during normalization, which relies on some level of smoothness in the M-values;
    and removing windows that are unlikely (for biological/experimental reasons) to reject the null hypothesis, or are uninteresting when they do so, e.g., background regions.

\subsubsection*{Circular dependencies between normalization and filtering}
The NB mean depends on the effective library size, so filtering for efficiency bias becomes a bit circular.
However, it shouldn't be too bad, as the normalization procedure isn't so sensitive to irregularities at the filter boundary.
In practice, there's little-to-no effect; I'd only expect something if the normalization factors are really crazy.

<<echo=FALSE,eval=FALSE>>=
for (it in 1:3) {
	ab <- aveLogCPM(assay(me.demo), lib.sizes=me.demo$total*me.norm)
	keep <- rank(ab) > 0.99*length(ab)
	me.norm <- normalize(me.demo[keep,])
	cat("Iteration is", it, "\n")
	print(me.norm)
}
@

Filtering is also sensitive to the NB dispersion when library sizes are different.
However, dispersion estimation can only proceed after normalization and filtering. 
This results in another circular dependency that is resolved by using a sensible (but otherwise arbitrary) value for the NB dispersion in \code{aveLogCPM}. 
The alternative would be to iterate over the entire DB analysis, which would be prohibitively time-consuming in most circumstances.
Of course, this is not an issue when library sizes are the same.

\subsubsection*{Choosing the filter with maximal discoveries}

Choosing the filter which gets you the greatest number of DE regions isn't the best idea, simply because it means that it's not independent of the p-values any more. 
At any given filter value, the expected FDR is controlled by the BH method rather than the actual FDR. 
Trying to maximize the number of significant tests will result in loss of FDR control as it's not an unbiased expectation anymore. 
It's worst with strong correlations between the nulls.

<<echo=FALSE>>=
set.seed(1478343)
@

<<>>=
collected <- list()
max.collect <- list()
for (y in 1:50) {
	subcol <- NULL
	subfdr <- NULL
	n <- 10000
	infilter <- rep(runif(n, 0, 1), 10)
	allps <- rep(runif(n, 0, 1), 10)
	spike <- 500
	for (x in 1:9/10) {
		keep <- infilter>=x
		combined <- c(rep(0, spike), allps[keep])
		sig <- sum(p.adjust(combined, method="BH")<0.05)
		subcol <- append(subcol, sig)
		subfdr <- append(subfdr, 1-spike/sig)
	}
	collected[[y]] <- subfdr
	max.collect[[y]] <- subfdr[which.max(subcol)]
}
out <- do.call(cbind, collected)
rowMeans(out)
mean(unlist(max.collect))
@

In general, you'd need to know the number of true/false positives \textit{a priori} to pick the "optimal" filter beforehand.
Alternatively, the regularization stuff presented by Wolfgang at Bioconductor 2015 might help here.
However, this would assume that you're trying to maximize discoveries without caring what they actually are.
It's possible that some of the discoveries at low abundances are uninteresting (e.g., changes in chromatin state).
Lowering the filter to pick these up and maximize discoveries would end up reducing power for the enriched regions that we're interested in.

\subsubsection*{Using the FDR over the log-fold change for filtering} 

It is possible to develop a more sophisticated background-based filter where each window is tested against a null background model.
Windows with low adjusted $p$-values can then be selected to obtain a set of potentially bound intervals at a certain false discovery rate.
While this can be done in \pkgname{}, the philosophy of the background-based filters in \code{filterWindows} is to use the log-fold increase of each window relative to the background.
This is easier to compute, and the threshold is easier to pick and interpret for users.

\begin{itemize}
\item Consider the purest case, where you determine significance based on the enrichment values (use a constant variance, so same selection order).
If you pick a set based on the FDR, the selection of one region becomes dependent on how many other regions have higher enrichments.
This might be annoying if you're trying to use a stable pre-set threshold for ease of interpretation.
High FDR thresholds might also start to run into problems with monotonicity, where filter resolution is lost.

\item The use of a constant threshold for the p-value focuses on control of type I error and elimination of uninteresting features. 
This is fair enough but you're not controlling for the type II error rate, i.e., retention of interesting features.
For example, if the variability under the null is high, then all features will have large p-values and nothing will be retained.
Applying a constant threshold on the enrichment ensures retention of interesting features (with sufficient enrichment), no matter what the variability may be.
This is more generally applicable to different experiments and systems where the null model may change in its variability.
The test statistic is only more stable if effect size is positively correlated with the null variability, and it's hard to see how that would happen.

\item Another problem is that you're requiring rejection at a standard FDR of 5\% in order to get into the retained set.
This may be too conservative for preliminary selection, where windows that cannot reject here might have a lot of evidence for DB.
In the end, you might end up having to fiddle with non-conventional FDR thresholds (e.g., 10-20\%), which gets us back to the problem of arbitrariness.
Remember that the aim of filtering is not to do rigorous peak-calling, but to trim down the total number in order to detect DB regions.

\item Getting a FDR for your filter doesn't have any effect on FDR control for differential binding.
It will affect power, but not in a way that's easy to interpret, because it depends on how many true and false positives (re. DB, not absolute calling) were retained.
Conversely, having a minimum log-enrichment tells us a bit about the maximum scope of DB change (assuming that background is non-DB).
For example, a log-enrichment of 2 suggests that you can get, at most, a DB fold change of 2 for your regions that scrape past the filter.

\item Most analyses use a Poisson model with lambda set to the background coverage, but how do you model variability between replicates, or variability between genomic regions?
Using a NB model seems better but that assumes you can get a sensible dispersion estimate without biasing edgeR's inferences.
In both cases, the filter outcome is dependent on sequencing depth, as the NB model will eventually reject all locations if the counts are large enough.
Using an enrichment threshold (or \code{treat}) is more stable with respect to depth, and possibly gives more relevant regions by focusing on large enrichments.
For example, we could avoid detecting DB due to changes in chromatin state, as most of these shouldn't have strong enrichment above background (unless you do DNase/ATAC-seq).
\end{itemize}

% Using a local estimate of background will also result in correlations between the window count and the background.
% This will likely result in sub-Poisson variation and conservative p-values. 
% Not really a problem in itself, but it will affect the interpretation of the p-values if you then have to go and adjust the threshold again.
% It might be better to compute the average count across all libraries, and use this in a Poisson model against the count for an independent negative control.

\subsubsection*{Independence of the average abundance as a filter statistic}

The analogy to the normal case suggests that the average abundance is the independent filter statistic.
This seems to hold up for different library sizes, unbalanced designs and correlations (see \url{https://github.com/LTLA/IndependentFilter2015}).
Note that this assumes that the null design matrix can be parametrized with an intercept column.
The independence of the filter is important as most windows are low-abundance, which means that the density at the filter boundary is high relative to the number of retained windows.
Thus, any biases introduced during filtering will have a large effect on the DB analysis.
(Incidentally, this is another justification for the stringent peak-calling threshold in the simulations of 2014 NAR paper.)

With respect to clustering; it should be valid to cluster retained windows, as the clustering depends on coordinates and not the statistics of each window.
Or, you can think about it as conditioning on cluster formation where each constituent window must pass the filter threshold.
Independence ensures that the $p$-values of all windows are still uniformly distributed within each cluster, so it should be okay.
This doesn't work for clusters formed from the sign of the log-fold changes, though.
Even though the sign is independent of the $p$-value for each window, it's not so for correlated windows.
Regions with (spurious) DB are more likely to have windows with similar signs that form a single cluster, whereas regions with no DB will have random signs.

The situation is also more difficult for the local filtering methods.
Here, the filter statistic is defined as the difference in correlated abundances between each window and its neighbourhood.
Independence between the $p$-value and each abundance is not enough, as the example with the signs suggests.
Fortunately, analogies with the normal case suggest that any statistic computed from the overall means of correlated windows is independent of the $p$-values for those windows.

% Cut up the uniform distribution into two halves length-wise, and each of those halfs into two triangular slices.
% This gives you four slices; two upper triangles, and two lower triangles.
% Assume that each slice corresponds to a combination of two filter statistics, where both must be okay to retain.
% Filtering on either statistic alone could acquire one upper and lower triangle, such that the $p$-value distribution is still uniform.
% However, filtering on both statistics could yield only one triangle and a non-uniform distribution.

\subsubsection*{Misuse of the negative control libraries for filtering}

Some people have proposed filters based on detecting DB between the ChIP libraries and the input libraries.
This is unwise, because if you treat all ChIP smples as replicates, then those regions with DB between ChIP samples will have large dispersions.
This means that they will get filtered as they are not significantly enriched over the input, which makes the filter rather useless.
An alternative approach is to fit the proper model to the ChIP samples, and then test whether the average enrichment is greater than the input.
This avoids the problem with inflated dispersions -- however, a related but more subtle effect arises where regions with large dispersions are more likely to be filtered out.
This is an implicit form of filtering on the variance, which is not healthy for edgeR and friends which need to model the full range of dispersions.

The proper way of doing it would be to filter independently on the average abundance of all libraries (input and ChIP).
We then detect windows that were DB between ChIP and input in each group, using an appropriate paired-sample design matrix.
(We would probably normalize each input against the ChIP separately using the binned method, blocking factors avoid us having to do anything else.)
This yields two or more p-values for each retained window indicating enrichment over input.
Next, we detect windows between the ChIP libraries using a one-way layout, and possibly with a different set of normalization factors.
The independence of the filtering still applies to different design matrices (putting aside the effect of normalization on filtering).
Each input library gets its own coefficient to ensure that the correlations between ChIP/input don't affect dispersion estimation.
Finally, we compute a combined IUT p-value for all contrasts to identify regions that are peaks \textit{and} DB.

\subsubsection*{Selective filtering based on the contrast}

One common complaint with the average abundance is that performance will deteriorate as the number of groups increases.
For example, if a peak was only present in one group, the average abundance of that peak would drop as the number of groups increases.
This would affect the ability of the filter to retain that peak.
To get around this, you can filter using the average abundance of the libraries involved in the DB contrast.
This ensures that you're only selecting for high abundances in the libraries of interest.

Selective filtering does require some care.
You need to re-filter for each contrast, which necessitates re-estimation of the NB dispersion and repeated GLM fitting.
Independence also requires that the null design has an all-ones column for those libraries involved in the contrast.
The behaviour of the other libraries shouldn't matter, as they will only contribute to EB shrinkage.
If libraries are independent, filtering on the involved subset shouldn't affect the information extracted from the other subset.

% Cleanest under the null, due to correlations in the underlying means between groups under the alternative.
% Selection of strong DB peaks might favour selection of other strong DB peaks in other groups that have lower dispersions in the mean-variance trend.
% Though, for this to be an actual problem, it would need to break the assumption of constant dispersions across all libraries.

\subsubsection*{Role of the prior count}

The prior count has two roles - to stabilize both the numerator and denominator in the calculation of the log-fold change.
This is because both of them are randomly sampled values.
For the denominator, stabilization is necessary to avoid spuriously large fold changes when the sampled value is near zero.
Application to the numerator is less obvious but is still required.
Consider a region with a low but precisely estimated (if not known) mean of the read counts. 
Most subintervals of this region are empty, but the subinterval with the fortune of obtaining a single count will get a very high log-fold change.
A prior count protects against this stochasticity.

For the global and local enrichment methods, the prior count focuses on stabilization of the numerator.
This is because the denominator is estimated from large bins and can be expected to be precise.
For the control method, both the numerator and denominator are estimated from small windows.
This means that the variability of the final log-fold change will be doubled, which will affect the discriminative power of the filter.
A larger prior count is used to prevent this.
More practically, it mandates a minimum count, e.g., a fold change of 3 with a prior count of 5 is only possible if the count is at least 10.

\subsection*{Comments about the statistical testing}

\subsubsection*{Batch effects and the prior d.f.}
The prior d.f. will be large if there is a systematic batch effect, e.g., due to efficiency biases.
This is because the variability of the dispersions will be low when all of them are consistently large to reflect the constant difference between replicates.
In contrast, the prior d.f. will be small if there is a stochastic batch effect, e.g., due to hidden factors of variation.
This is because the dispersions will be absorbing a batch effect that is differing randomly between windows.
As a result, the variability in the dispersions will be increased.
There mightn't be a large increase in the dispersions themselves due to a stochastic batch effect, as not all features will be heavily affected by the batch to cause a substantial increase in the fitted trend or the common estimate.
However, small prior d.f. can result in almost as much damage as large dispersions, so watch out.

\subsubsection*{Choosing the number of replicates}
ChIP-seq is pretty troublesome, so it seems that the most logistically feasible choice is to go with two replicates in each group.
Three per group would probably be preferable, as it gives us an idea of which sample to toss out if one of them goes wrong.
We \textit{could} go on to do more replicates, but I don't think this is a good use of time and money.
At best, with many replicates, you'll end up detecting everything as DB with small effect sizes, and most of these would be uninteresting and/or hard to validate.
At worst, you'll end up with a pile of batch effects or efficiency biases due to logistical issues. 

There may be an argument for doing many replicates if you expect the effect size to be small.
However, most applications deal with fairly large effect sizes for at least a number of genomic regions.
Moreover, the regions with the largest effect sizes are likely to be most interesting, or at least, most strongly prioritized.
This means that there often isn't a great deal of incentive to spend money to improve power at the lower ranks.
As long as there are good candidate regions to follow up -- \textit{and you do follow them up!} -- a small number of replicates is fine.

\subsection*{Comments about FDR control}

\subsubsection*{Algorithm for splitting large clusters}

I suppose we could choose the number of clusters that maximises the average density. 
It's the most logical way to do it, as we are interested in high-density regions. 
This will also avoid chaining as clusters with a lot of empty space will be broken up. 
However, it also requires an arbitrary compromise between the number of clusters and the density. 
Specifically, there's liable to be a point at which chaining is acceptable, e.g., satellite window adjacent to a peak, regions of diffuse continuous enrichment. 
And that's without considering the fact that multiple peaks might need to be clustered together for proper FDR interpretation. 
So, the question becomes, how big of a density penalty should be applied to a new cluster? 
I think that this is more difficult to interpret compared to the existing method, where parameters are expressed as distances. 
And besides, the existing method is fast enough to run a couple of times to check the parameters.

\subsubsection*{Filtering on the sign of the log-fold change}
There are also provisions for clustering based on the sign of the log-fold change. 
The idea is that clusters will be broken up wherever the sign changes. 
This will separate binding sites that are close together but are changing in opposite directions. 
It will also make cluster definitions easier, as windows corresponding to a single event are likely to have the same fold change (as distinct from background, which should be more-or-less random).
A vector can be supplied in \code{sign} to indicate whether each window has a positive log-fold change.

<<eval=FALSE>>=
merged.sign <- mergeWindows(rowRanges(data), tol=1000L, sign=(results$table$logFC > 0))
@

For routine analyses, sign-based filtering is not recommended as the sign of windows within each cluster is not independent of their DB status. 
Windows in a genuine DB region will form one cluster (consistent sign) whereas those in non-DB regions will form many clusters (inconsistent sign, as the log-fold change is small).
This results in conservativeness as more clusters will have large $p$-values.
Furthermore, any attempt to filter away small clusters will cause liberalness if too many large $p$-values are lost.

If you want to work on the sign of the log-fold change, it's best to convert each two-sided $p$-value into a one-sided counterpart.
This can be done by setting the $p$-value to $p/2$ for the correct sign, and $1-p/2$ otherwise.
I'm not sure that filtering on signs would be sensible as it wouldn't be independent for correlated windows.
If you want to work on the signs of multiple log-FCs, the best approach might be to do each comparison independently and compute an IUT $p$-value across all one-sided $p$-values.
It is difficult to show that filtering on multiple signs for an ANOVA-like comparison would be independent.
In any case, the intersection is easier to interpret as it must be significant across all comparisons.

\subsubsection*{Arguments for proper consolidation}

The \textit{ad hoc} alternative is to just merge regions from multiple DB lists together.  
You'll end up with overlapping results, which means that you wouldn't be controlling the FDR across the merged set. 
In addition, merging DB lists that have been controlled at a given FDR assumes that there is at least one true positive in each list. 
This mightn't be true, in which case FDR control will be lost (you'll be increasing the FDR by adding to both the numerator and denominator).

To illustrate, split a bunch of $p$-values into two sets, and control the FDR within each set.
If you then merge the sets, the FDR should be controlled within the merged sets, right?
However, if you take this to its extreme, you could split those $p$-values into trivially small sets, e.g., of size 1.
Controlling the FDR within each set would not change the size of the $p$-values.
So, upon merging, you'd be defining significance from the raw $p$-values, which doesn't control the FDR.

<<echo=FALSE>>=
set.seed(141233)
@

<<>>=
all.p <- c(rep(1e-8, 100), runif(900))
sig <- p.adjust(all.p, method="BH") <= 0.05
sum(sig[-(1:100)])/sum(sig)
alt.sig <- all.p <= 0.05
sum(alt.sig[-(1:100)])/sum(alt.sig)
@

\subsubsection*{Using the highest-abundance window as a representative}

Alternatively, the best window can be defined as the one with the highest average abundance in each cluster.
This represents the window with the strongest binding for the target protein, though not necessarily the strongest DB.
As the average abundance is (roughly) independent of the $p$-value, there is no need to correct for multiple testing within each cluster.

<<>>=
tab.ave <- getBestTest(merged$id, results$table, by.pval=FALSE)
head(tab.ave)
@

For sharp binding events, it may be preferable to restrict the DB analysis to these windows.
This will usually provide more power as it avoids the conservativeness of Simes' method.
The use of the highest-abundance window will also favor detection of simple DB events, where DB occurs at the most strongly bound loci in each cluster.
This may be preferable for some studies, given that complex DB events are often difficult to interpret.

The obvious drawback is that information is lost when the analysis is restricted to a single window.
Detection will fail for complex DB events, e.g., clusters of TF binding sites, spreading of diffuse marks.
Loss of information also increases the sensitivity of the analysis to the clustering procedure.
For example, a DB site will be ignored if it is clustered alongside a stronger non-DB site, as the window covering the latter will represent the cluster.

A less formal approach is to simply report the log-fold change of the most abundant window in each cluster.
This will indicate whether any DB is present at the strongest binding site within the corresponding genomic interval.
If not, the DB event may be complex, e.g., involving multiple peaks or peaks that change in shape or position between conditions.
These require more careful interpretation than simpler ``binary'' (i.e., on/off) changes.

<<>>=
tabcom$mab.logFC <- tab.ave$logFC # 'mab' stands for most abundant.
@

The use of the most abundant window explicitly ignores the possibility of complex DB events by looking for regions that are changing at the site of maximum binding.
For TF data, this may be beneficial as you avoid detecting (possibly irrelevant) changes in shoulders or subpeaks of the main binding event.
This still has some advantages over straight-up peak calling, as you still maintain high resolution, i.e., you don't get contamination from miscalled peak boundaries.
I guess it turns into something similar to a summit-based analysis of peaks.

On a more philosophical note, why should we bother with complex events?
Simple DB will probably give a better correlation with transcriptional activity, as large-scale changes in marking should have some effect.
I'd argue that it's unwise to focus on this correlation when interpreting ChIP-seq data -- if you wanted a direct measure of transcription, then you might as well do RNA-seq.
It's worth tolerating some complex DB changes that are irrelevant, in order to find those that affect your genes (or more generally, regions) of interest.
You'd have to do so anyway with simple DB, as transcription is dependent on many things other than binding of a particular target protein.
In more mechanistic terms, changes outside of the highest-abundance window may expose or sequester binding motifs for other proteins, which would have important effects.
Complex DB may also result from simpler DB where the increase in enrichment results in an apparent spread of the mark.
The subtle change at the center of the site is missed, but the larger relative changes at the edges can still be detected.

\subsubsection*{Annotating regions for complex DB}

If you annotate a complex DB region, there's no guarantee that the DB subinterval actually overlaps the annotated feature of interest.
The better approach would be to combine $p$-values across the feature of interest, to get a direct summary for DB at that feature.
Trying to do so with annotation of \textit{de novo} clusters is not easy.
You'd need to overlap the features with significant windows within each cluster.
This would require some definition of a window-level error rate, but this makes no sense if you end up interpreting the results in terms of clusters or genes. 

Another related problem would be when someone, in looking inside a cluster, wants an error rate for a specific window.
The obvious approach would be to use a Bonferroni/Holm-corrected window-level $p$-value, where the correction is applied based on the number of windows in that cluster.
This maintains strong FWER control for any arbitrary selection of hypotheses.
The resulting value should also  be larger than the combined $p$-value of the cluster (for Bonferroni, at least).
The real question is whether it is necessary to also correct across clusters.
You can do so by replacing the combined $p$-value with the corrected value during the BH adjustment, for this cluster only.
This effectively replaces ``there is DB somewhere in the cluster'' with ``there is DB at this location in the cluster''.
We preserve a region-level interpretation, as it would be problematic to try to control the window-level FDR.

\subsubsection*{Explaining the lack of rigour in non-standard FDR control methods}

When controlling the FDR with \code{clusterWindows}, we calculate an upper bound on the number of false positive windows.
This is done based on the total number of detected windows at a particular window-level FDR threshold.
An upper bound on the number of false positive clusters is computed by filling up as many clusters with false positive windows (starting from the smallest clusters).
Division by the total number of clusters yields an upper bound on the cluster-level FDR.
However, this depends on obtaining a good upper bound on the number of false positive windows.
If the number of detected windows is low and/or they are correlated, sampling stochasticity will affect the precision of the upper bound.
There are also some theoretical issues with multiplying expected and observed values, which probably become pertinent when precision decreases.

A similar problem is encountered with \code{empiricalFDR}, where the number of false positives is estimated from the number of rejections in the wrong direction.
This exploits the fact that the sign is independent of significance under the equality null, such that the distribution of p-values in each direction should be equal.
Thus, at any given threshold, the number of rejections in the wrong direction should be the same as the number of rejections (false positives) in the right direction.
However, this estimate will not be precise if there are not many rejections, especially at low $p$-value thresholds.
The accuracy of the estimate also assumes that there are no condition-specific artifacts that bias rejections in one direction or another.
This is possible if the negative control does not capture the same type of false positives that are present in the ChIP sample.

For example, $p$-values for changes in the wrong direction might be skewed away from zero if it's harder to reject in the negative control.
This would reduce the estimated number of false positives and lead to underestimation of the empirical FDR.
In contrast, the BH method would be able to control the FDR correctly.
This is because it accounts for the absolute size of the $p$-values, rather than their size relative to other $p$-values (i.e., those changing in the wrong direction).

<<>>=
right <- c(rep(0, 100), runif(1000))
wrong <- rbeta(1000, 2, 1)
emp <- findInterval(right, sort(wrong))/rank(right)
o <- order(right, decreasing=TRUE)
emp[o] <- cummin(emp[o])
sum(which(emp<=0.05)>100)/sum(emp <= 0.05)
bh <- p.adjust(right, method="BH")
sum(which(bh<=0.05)>100)/sum(bh <= 0.05)
@

Note that the set of p-values in the wrong direction is not completely representative of false positive regions.
It includes p-values from regions with one or more true positive windows, which should not be considered false positives, even if they contain false positive windows.
This results in some conservativeness as the number of false positives at any given threshold is overestimated.
There's no obvious way to avoid this, as you'd need to know which regions were true positives to prune them out of the set.

\subsubsection*{Filtering twice prior to the FDR correction}

For example, say the user wants to focus on local maxima.
Assume that the test statistics are computed from \code{filtered.data}, as generated with the mild \code{keep.simple} filter in Chapter~\ref{chap:filter}.
These filtered windows are re-filtered using the \code{maxed} vector from Section~\ref{sec:localmax}, to identify those that are local maxima.
The re-filtered windows and their corresponding test statistics can then be used for all FDR-controlling procedures described in this chapter.

<<>>=
keep.new <- maxed[keep.simple]
refilt.data <- filtered.data[keep.new,]
refilt.res <- results[keep.new,]
@

Any filter vector from Chapter~\ref{chap:filter} can be used in place of \code{maxed}, so long as it was generated with the original \code{data}.
Generating the filter vectors with \code{data} instead of \code{filtered.data} is generally recommended, as it avoids any unforseen interactions between filters.
In particular, the local maxima filter depends on the abundance of adjacent windows.
It may behave incorrectly if it is applied after another filter that removes high-abundance windows.

\subsubsection*{Cherry-picking regions from the DB list}
A common use of the DB list is to scroll down the list and pick interesting regions (e.g., around particular genes) based on domain expertise.
In theory, we should use the local FDR to provide control for arbitrary selections of regions from the DB list.
However, this is difficult as it requires knowledge of the distribution of p-values under the alternative.

As long as the selection is evenly spread or skewed towards lower p-values, the FDR across the selection should still be controlled below the specified threshold.
This is likely if we assume that the domain knowledge is reasonable, because regions involved in affected processes should be near the top of the list.
(And if they're not, the domain knowledge is probably wrong, and they should be looking at the top-ranking DB regions instead.)


\end{document}

