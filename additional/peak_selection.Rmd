---
title: Anticonservativeness in peak selection strategies
author: Aaron Lun
date: 19 August 2017
output: 
   html_document:
     fig_caption: false
---

```{r, echo=FALSE}
knitr::opts_chunk$set(error=FALSE, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(fig.path="figures-peak/")
```

# Background 

The 2014 NAR paper (https://doi.org/10.1093/nar/gku351) showed that many _ad hoc_ peak selection strategies result in conservativeness.
One could consider erring on the side of conservativeness to be acceptable, especially if more DB sites can pass the filter.
However, this document demonstrates that the same strategies can also result in loss of type I error control.
The possibility of anticonservativeness means that any increased detection from _ad hoc_ strategies cannot be trusted.

# Setting up the experimental design

Consider an experimental design with two replicates in each of two conditions.

```{r}
group <- rep(c("A", "B"), each=2)
nlibs <- length(group)
design <- model.matrix(~group)
```

We set up a simulator for a count matrix where a certain proportion (10% by default) of the sites are DB.
Note that the grand mean for DB and non-DB sites are the same, otherwise the filtering problem would be trivial.

```{r}
simulateCounts <- function(nlibs, n.sites=1e5, prop.db=0.1, 
                           dispfun=function(x) { 0.1 }, 
                           n.mu=50, db.mu=rep(c(100, 0), each=2)) {
    P.n <- 1/dispfun(n.sites)
    db.sites <- n.sites*prop.db
    P.db <- 1/dispfun(db.sites)
    is.null <- seq_len(n.sites)
    counts <- rbind(matrix(rnbinom(n.sites*nlibs, mu=n.mu, size=P.n), ncol=nlibs, byrow=TRUE),
        matrix(rnbinom(db.sites*nlibs, mu=db.mu, size=P.db), ncol=nlibs, byrow=TRUE))
    return(list(counts=counts, null=is.null))
}
```

We set up a function to perform the differential analysis with _edgeR_ to obtain _p_-values.
We use equal library sizes here, assuming that normalization has already been performed to correct for composition biases.

```{r}
library(edgeR)
detectDiff <- function(counts, design, coef=ncol(design), lib.size=rep(1e6, ncol(counts))) {
    y <- DGEList(counts, lib.size=lib.size)
    y <- estimateDisp(y, design)
    fit <- glmQLFit(y, design, robust=TRUE)
    res <- glmQLFTest(fit, coef=coef)
    return(list(common.dispersion=y$common.dispersion, PValue=res$table$PValue))
}
```

We'll also set up a function to assess type I error control.

```{r}
plotAlpha <- function(pvals, ylab="Observed/specified", xlab="Specified", xlim=NULL, ...) {
    for (i in seq_along(pvals)) { 
        cur.p <- pvals[[i]]
        exp <- (seq_along(cur.p) - 0.5)/length(cur.p)
        n <- findInterval(exp, sort(cur.p))
        obs <- n/length(cur.p)
        if (is.null(xlim)) { # Stable at 20 observations.
            xlim <- c(exp[which(n >= 20)[1]], 1)
        }
        if (i==1L) {
            plot(exp, obs/exp, log="xy", xlim=xlim, type="l", ...)
        } else {
            lines(exp, obs/exp, ...)
        }
    }
}
```

# Applying the "at least 2" filter

Let's see what happens when we apply the "at least two" filter to retain the top proportion of sites.
First we set up a filtering function.

```{r}
set.seed(10001)
AL2Filter <- function(counts) { 
    top.al2 <- apply(counts, 1, FUN=function(x) { sort(x, decreasing=TRUE)[2] })
    rank(-top.al2, ties.method="random")
}
```

Now we run through repeated simulations and collect the results.

```{r}
retained <- dispersions <- numeric(10)
null.p <- vector("list", 10)
for (it in 1:10) {
    out <- simulateCounts(nrow(design))
    keep <- AL2Filter(out$counts) <= 20000
    kept.null <- which(keep) %in% out$null
    retained[it] <- sum(kept.null)
    res <- detectDiff(out$counts[keep,], design)
    dispersions[it] <- res$common.dispersion
    null.p[[it]] <- res$PValue[kept.null]
}
```

There is some inflation, but the presence of correct dispersion estimates for the DB sites keeps the common dispersion low.

```{r}
summary(retained)
summary(dispersions)
```

We observe loss of type I error control at low _p_-values.
This is because the dispersion inflation is minimized _and_ the "at least two" filter selects for spurious DB sites.

```{r}
summary(sapply(null.p, FUN=function(x) { mean(x <= 1e-3) }))
plotAlpha(null.p)
```

# Applying a union filter.

Repeating the dose with a union filter.
Here we retain fewer sites, which ensures that the DB percentage in the retained set is higher (see below).

```{r}
set.seed(20002)
UnionFilter <- function(counts) {
    top.u <- apply(counts, 1, FUN=max)
    rank(-top.u, ties.method="random") 
}
```

Now we run through repeated simulations and collect the results.
We use a more stringent filter to obtain a higher DB percentage, which keeps the dispersion inflation low.

```{r}
retained <- dispersions <- numeric(10)
null.p <- vector("list", 10)
for (it in 1:10) {
    out <- simulateCounts(nrow(design))
    keep <- UnionFilter(out$counts) <= 5000
    kept.null <- which(keep) %in% out$null
    retained[it] <- sum(kept.null)
    res <- detectDiff(out$counts[keep,], design)
    dispersions[it] <- res$common.dispersion
    null.p[[it]] <- res$PValue[kept.null]
}
```

Some inflation occurs, but all in all, the dispersions are kept reasonably low.

```{r}
summary(retained)
summary(dispersions)
```

Testing again results in the loss of type I error control.
Normally, the union approach enriches outliers and inflates the dispersion.
However, enough DB sites ensures that the inflation is minimized, encouraging spurious rejection of the null.

```{r}
summary(sapply(null.p, FUN=function(x) { mean(x <= 1e-2) }))
plotAlpha(null.p)
```

# Applying the mean filter

Now, to demonstrate the correct way of doing it, we use a filter on the mean count.

```{r}
set.seed(30003)
MeanFilter <- function(counts) { 
    top.m <- rowMeans(counts)
    rank(-top.m, ties.method="random")
}
```

Running these counts through _edgeR_.

```{r}
retained <- dispersions <- numeric(10)
null.p <- vector("list", 10)
for (it in 1:10) {
    out <- simulateCounts(nrow(design))
    keep <- MeanFilter(out$counts) <= 10000
    kept.null <- which(keep) %in% out$null
    retained[it] <- sum(kept.null)
    res <- detectDiff(out$counts[keep,], design)
    dispersions[it] <- res$common.dispersion
    null.p[[it]] <- res$PValue[kept.null]
}
```

Dispersions are equal to their expected value.

```{r}
summary(retained)
summary(dispersions)
```

Testing indicates that type I error control is mostly maintained.

```{r}
summary(sapply(null.p, FUN=function(x) { mean(x <= 1e-2) }))
plotAlpha(null.p, ylim=c(0.5, 2))
```

# Saving _ad hoc_ filters with FDR control

One could argue that anticonservativeness with _ad hoc_ filters is not a problem in practice, because the enrichment for DB sites ensures that FDR control is still preserved.
If you enrich for enough DB sites, even complete loss of type I error control among the true nulls will not breach the FDR threshold.
However, this assumes that you have enough power to detect all of the DB sites.
If you don't have enough power, FDR control is lost:

```{r}
set.seed(40004)
out <- simulateCounts(nrow(design), db.mu=c(70, 70, 30, 30))
keep <- AL2Filter(out$counts) <= 20000
res <- detectDiff(out$counts[keep,], design)
sig <- p.adjust(res$PValue, method="BH") <= 0.05
sum(sig & which(keep) %in% out$null)/sum(sig)
```

There are also other ways of mitigating the variance inflation that don't involve introducing DB sites between two conditions.
For example, if an orthogonal batch effect increases binding in some sites, those sites will get preferentially enriched by an "at least 2" filter.
This will suppress the dispersion inflation in the other sites and encourage loss of type I error control.
FDR control is irrelevant here as there are no DB sites to the contrast between the first two groups.

```{r}
design.b <- model.matrix(~c(0,1,0,1)+c(0,0,1,1))
null.p <- vector("list", 10)
for (it in 1:10) {
    out <- simulateCounts(nrow(design.b), db.mu=rep(c(20, 80), 2)) # not really DB.
    keep <- AL2Filter(out$counts) <= 10000
    res <- detectDiff(out$counts[keep,], design.b)
    null.p[[it]] <- res$PValue # look at all p-values, as all nulls are true.
}
plotAlpha(null.p)
```

Alternatively, you could imagine a situation with three groups, and DB sites in the third group can suppress dispersion inflation.
This is done without contributing differences to the contrast between first two groups.

```{r}
group.3 <- factor(rep(LETTERS[1:3], each=2))
design.3 <- model.matrix(~group.3)
null.p <- vector("list", 10)
for (it in 1:10) {
    out <- simulateCounts(nrow(design.3), db.mu=rep(c(25, 100), c(4, 2))) # not really DB.
    keep <- AL2Filter(out$counts) <= 10000
    kept.null <- which(keep) %in% out$null
    res <- detectDiff(out$counts[keep,], design.3, coef=2)
    null.p[[it]] <- res$PValue # look at all p-values, as all nulls are true.
}
plotAlpha(null.p)
```

# Wrapping up

```{r}
sessionInfo()
```

