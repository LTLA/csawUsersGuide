\documentclass[12pt]{report}
\usepackage{fancyvrb,graphicx,natbib,url,comment,import,bm}
\usepackage{tikz}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
\setlist{parsep=0pt,listparindent=\parindent}

% Margins
\topmargin -0.1in
\headheight 0in
\headsep 0in
\oddsidemargin -0.1in
\evensidemargin -0.1in
\textwidth 6.5in
\textheight 8.3in

% Sweave options
\usepackage{Sweave}
\SweaveOpts{keep.source=TRUE,prefix.string=plots-com/ug,png=TRUE,pdf=FALSE}

\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl,fontsize=\footnotesize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\footnotesize}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontsize=\footnotesize}
\renewenvironment{Schunk}{\vspace{0pt}}{\vspace{0pt}}

\DefineVerbatimEnvironment{Rcode}{Verbatim}{fontsize=\footnotesize}
\newcommand{\edger}{edgeR}
\newcommand{\pkgname}{csaw}
\newcommand{\code}[1]{{\small\texttt{#1}}}
\newcommand{\R}{\textsf{R}}

<<results=hide,echo=FALSE>>=
#picdir <- "plots-com"
#if (file.exists(picdir)) { unlink(picdir, recursive=TRUE) }
#dir.create(picdir)
require(edgeR)
@

\begin{document}

\subsection*{General comments on DB}

\subsection*{Motivation for DB}

Looking for DB is more relevant, as you get changes in binding that you can associate with differences in biology (so you can start poking around at mechanisms). 
It's also easier than looking for absolute binding, as the null hypothesis is well defined.
You don't need to construct a background model, as you would do in single-sample peak calling.
This is laden with assumptions, which don't sound so bad qualitatively but result in uninterpretable significance statistics if you get them wrong quantitatively.
For example, MACS(2) typically churns out extraordinarily low $p$-values, mostly due to its failure to consider replicates. 
There's also the issue of conservativeness if the peak region overflows the window and gets counted as part of the background (or the same with adjacent peaks);
    and liberalness, if the surrounding regions are not a good estimate of the null enrichment, e.g., due to unmappability or because you're sitting on a repeat unit.

You don't need to compare to a negative control, as you would do in two-sample peak calling.
In particular, comparisons to input are not like-for-like, as you miss IP-specific biases when you skip the IP.
This shows up as reproducible differences in the TapeStation profiles (recall Alikesei's H3K27me3 data).
It manifests most clearly as differences in coverage across repeat regions where biases are amplified, e.g., mitochondria.
While the worst offenders can easily be filtered out, the real problem is where the biases are subtle such that they confound the actual differences.
For example, non-specific binding would show up as a putative peak as there's nothing to cancel it out in the input.
IgG's are better but they face a conundrum in that they are harder to generate as your IP procedure improves, i.e., becomes cleaner.

Of course, you could level the accusation that DB also detects a number of false positives, due to changes in chromatin structure, fragmentation, etc. between conditions.
This problem balances out the disadvantage of non-``like-for-likeness'' for a comparison to input.
Both sets of false positives are the cost of doing business in an imperfect system.
However, I would argue that the false positives in DB are \textit{slightly} less confounding.
Changes in chromatin structure are still biologically interesting, even if it wasn't quite what you were looking for with respect to DB.
In comparison, the false positives in a comparison to input would be purely technical in origin. 
In any case, these should form a minority of the detected regions in a successful experiment.

\subsubsection*{Comments on correcting for changes in chromatin state}

One could theoretically use control data to correct for chromatin state when searching for DB. 
The most obvious is to subtract the control coverage from the ChIP coverage.
The idea is to avoid detecting spurious DB due to changes in chromatin state.
However, this obviously requires sequencing of a negative control sample for each condition, which might be substantially more costly (or not even possible for complex designs without a one-way layout).
It also assumes that the controls are accurate estimators of background, which mightn't be the case, e.g., for inputs.

Other problems also occur when you have high ChIP and control coverage (where subtraction actually has an effect). 
Subtraction will mess up the mean-variance relationship as the absolute size of the counts is lost.
In particular, a small relative difference between libraries at large counts would be amplified in the subtracted counts, such that you'd get false positives (if between groups) or inflated variance (if within groups). 
You could also end up with negative counts. 
These need to be coerced to zeros, resulting in an excess of zeros that aren't easily handled by the NB model.
Using the zero-inflated NB would result in a fair amount of grief.
Simply filtering out low-abundance windows won't remove all zeros, as you would retain windows with zeros where the other subtracted counts (e.g., for the replicate) are large due to variability for high raw counts.

The other approach is to do log-subtraction, where you include the controls as part of the model.
You could then ask if log-fold change over the control is the same in the two conditions.
This avoids the statistical problems with pure subtraction, but can result in loss of detection power.
For example, consider a case where both ChIP and control coverage increases two-fold. 
This would not be detected after log-correction, as the fold change cancels out.
However, there is DB here if you assume an additive model, e.g., $x$ genuine binding plus $y$ background in one condition, $2(x+y)$ in the other condition, resulting in $x$ against $2x$ binding between conditions.
This seems to be most problematic when DB coincides with changes in chromatin accessibility.
This is biologically sensible but will not be detected if the latter cancels out the former (personal comm. from Gordon, Mike Love).

% The ideal solution would be to switch to an identity-link GLM.
% This would allow for subtraction-based contrasts, while preserving the mean-variance relationship.
% However, there's the scaling effect of library size differences, which isn't easy to model with identity offsets.
% There are also computational difficulties with enforcing positive fitted values.
 
So, in short, it is difficult to rigorously adjust for changes in chromatin state.
I'd say that several hundred false positives is an acceptable cost for keeping things simple, especially if you have thousands of detected regions.
Protection is generally provided by filtering, as most changes in state should be depleted after the IP step (unlike, e.g., DNase-seq).
Of course, if there are likely to be extreme changes, then control correction may be necessary. 
Subtraction may be okay if there aren't a lot of regions with high control coverage, as the other high-abundance regions will buffer any damage to the variance and EB statistics (though, if those regions are rare, then there mightn't be much point in correcting anyway; you might also get more false positives if variance inflation is prevented).

\subsubsection*{Justifying the \textit{de novo} focus}

With a new data set, it's generally a good strategy to do a \textit{de novo} analysis.
If most of the peaks occur around the TSS or genes, then you can switch to an approach using pre-defined regions. 
Otherwise, you'd be throwing out a lot of information. 
I'm guessing over 90\% of reads would be lost if you stuck to TSS-based regions.
While you might say that we are only interested in genic binding anyway, that's a narrow-minded view.
You should first see what the data has to say before diving into assumptions like that, especially for uncharacterized proteins.
There's also the issue of the difficulty in defining the location and width of the TSS and promoter.
Improper definitions may result in loss of power if the binding site is missed or includes too much background.
Besides, what's the harm? Worst is, you spend an hour or so looking at non-interesting regions. At best, you might be able to find something new and exciting.

\subsection*{Comments on counting}

\subsubsection*{Fragment length}

One might think of allowing the fragment length to vary within the genome, e.g., to handle heterogeneity in chromatin openness.
However, location-specific estimates for the fragment length wouldn't be very precise because you're calculating it from limited data. 
It would really only work for large peaks where the large number of reads stabilises the estimate. 
However, the advantage wouldn't be that great as results for large peaks should be fairly stable anyway.

On real data, the cross-correlation plots are one smooth peak (excluding the read-length spike). 
This suggests that there's no gross segregation of fragment lengths in the population. 
Changing the fragment length doesn't seem to change the results too much either. 
Of course, this whole discussion is moot for diffuse marks, where the extension should be irrelevant compared to the bin size.

\subsubsection*{Strand bimodality}

An anticipated attack on this approach is why I don't filter for bimodality to identify good candidate sites (a la MACS). 
This could reduce the severity of the MTC procedure. 
However, every difference is interesting, even if it doesn't necessarily occur in a nice bimodal shape (e.g. histones + other proteins).
For example, you mightn't get pronounced bimodality if one half of the peak is sitting somewhere that's unmappable or difficult to sequence. 

Defining bimodality is also difficult. 
Arbitrary thresholds for bimodality don't necessarily have a good physical interpretation. 
Of course, abundance-based filtering also requires an arbitary threshold, but I'd argue that the two cases are different.
Filtering on abundance is critical so we have to put up with it, whereas we can do without filtering on bimodality.
This is complicated by the fact that you have to combine background and enriched samples to avoid data snooping; this can dilute bimodality for smaller peaks.
A problem specific to MACS is that single-sample mode won't pick up two peaks within $\sim$10 kbp; each peak will be increase the observed background for the other.

Read extension means that bimodal structures will have higher counts than regions with the same number of reads but random read directions. 
This brings looking for bimodality under the framework of row sum filtering. 
It is also smoother; if you're not bimodal but there is a difference, you still get picked up.
This allows us to keep an open mind, and to find things that we weren't expecting to see.
Unimodal events are only problematic if they are high-abundance, which shouldn't occur if you've removed repeats.

Finally, you can do whatever bimodal filtering you want implicitly, by running your desired peak caller and then picking only those windows that overlap called peaks. 
This will give you the MTC advantage without requiring too much extra code on my part.
Alternatively, you can use the \code{checkBimodality} function.
However, a couple of notes:
\begin{itemize}
\item In addition to the reasons mentioned above, I wouldn't recommend filtering on the bimodality scores here, because I don't think it's an independent statistic. 
For purposes of convenience, I pooled the reads before counting them, such that we use Poisson means to compute the ratios.
Selection for large scores might result in selection for high abundances (due to the prior count) and selection for DB sites when one library is much larger than the other.
There's also the issue that the counts are computed on a per-base basis, rather than using window-level counts; I don't know whether this'll compromise independence.
\item Currently, \code{checkBimodality} is recommended for use as a diagnostic.
The idea is determine whether a DB region is likely to be a genuine TF binding site.
However, one counter-argument is that you could get a large region containing a bimodal site that is distinct from the DB subinterval.
In such cases, the best approach would be to reduce the clustering tolerance so that such sites are separated out.
Otherwise, we'd have to filter directly on bimodality at the window level, which I don't want to do.
\item Watch out for large peaks, because even in TF data, the strand-specific subpeaks tend to spread such that you end up with little bimodality.
The same applies for situations with many adjacent binding events, such that you end up with a histone-like broad peak.
\end{itemize}

\subsubsection*{Window correlations}

Read extension and general spatial proximity results in strong correlations between adjacent sites.
However, these correlations will not affect the estimates of the various EB statistics (only the precision of the estimates, but we don't use that anyway).
Consider a subset of sites randomly sampled throughout the genome. 
Correlations within the subset are weaker as the sites are less likely to be affected by the same reads. 
An (approximately) independent subset of sites can then be obtained by infrequent sampling. 
However, the expected composition of the sampled set is just that of the entire genome.
Thus, an independent subset will provide the same results (on average) as those obtained with the use of the entire genome.  
This is similar to the outcome of thinning in Markov chain Monte Carlo procedures.

More generally, the dispersion for each window can be considered to be sampled from the same distribution (dependent on the mean, if trended).
EB shrinkage requires estimates of the mean of this distribution, for the dispersion to shrink towards; and the variance, for the prior degrees of freedom.
For a large number of windows, you'll have enough observations such that the empirical distribution should be pretty similar to the real distribution. 
This will be true even if the windows are correlated with each other, as they should still be identically distributed.
Thus, the mean/variance estimates from the empirical distribution should be close to the true values, even in the presence of correlations.
Such correlations can be anything, e.g., due to underlying sequence features, biological correlations in binding affinity.
Of course, if the correlations are too strong (e.g., due to batch effects or other systematic biases), then all the windows in the genome might not be enough.

\subsubsection*{Windows vs. peak calling}

Peak calling fundamentally involves the clustering of windows to get a peak interval for counting.
Clustering can adjust to variable size widths, whereas the use of a large bin does not (and can encounter edge effects). 
However, this depends heavily on the decisions made by the aggregation routine, as there's often insufficient data to direct unambiguous cluster boundaries at any single site.
Results might not be robust to tuning of parameters or even new datasets (cluster groupings can change which will change the manner in which counts are collected). 
Similarly, inappropriate inclusion of background regions can be problematic for DB detection.

Clustering must also be done independently of DB, such that changes to spatial resolution also may not be appropriate, e.g., for complex DB events.
This includes DB shifting or shrinkage, or even noisy background where the surrounding non-NB regions contaminate the DB enriched signal.  
Such events are potentially interesting, with some examples below:
\begin{itemize}
\item Spreading of a repressive mark may prevent binding of regulatory factors, whereas spreading of an activating mark may allow access of specific TFs to their binding sites. 
\item A more esoteric example involves RNPII studies, if you want to distinguish between elongating and poised events. 
\item Peak shifting may be indicative of changes in nucleosome phasing and the consequences thereof, e.g., if binding sites are immobilized within nucleosomes.
\end{itemize}
The only way to find these buggers is to look for DE'ness at every place. 
In other words, we're interested in differential binding rather than absolute enrichment, and peak counts will give us the former rather than the latter.
Which is okay in other cases as you get a boost in power from larger counts (though you can twiddle the width to deal with that - and then you get into parameter dogfights).

All in all, clustering will give higher counts and better performance for clusters where the DB status is homogenous throughout the cluster. 
(That said, clustering will not guarantee that the DB status is homogenous i
    -- you could still suboptimally detect clusters involving shifting, so it's not correct to say that you're using clustering to focus on homogenous changes. 
    The only way to guarantee homogeneity would be to do IUTs on all windows throughout the regions.)
For more complex events, clustering will probably be suboptimal.
Widths are generally a simpler alternative. 
However, it is unable to deal with variable sizes of enriched regions, requiring testing of multiple bin sizes.
Using a suboptimal value might reduce power but it shouldn't change the rankings of the regions. 

You could also get methods designed to create regions based on differences in density.
But, the magnitude of the difference required for the creation of a new region is going to be arbitrary.
A slow decline in density from high to low won't get picked up if you're looking for big differences.
MACS2 does try to identify subpeaks in large binding regions.
However, for interpretation, you'll want to report the entire region, and to do that you'll have to combine the subpeak statistics anyway, e.g., with \code{combineTests}.
In addition, the behaviour of the subpeak identification algorithm behaves may not have predictable properties regarding resolution and count size.
We might as well make our lives simpler and use windows directly.

% One might propose to call peaks in each condition, and then do a setdiff on the genomic intervals between conditions.
% This would get the unique (and shared) subintervals, which you could then count over.
% However, this has some technical problems, e.g., how large does the interval have to be for it to be considered?
% Some threshold is required, as ambiguity in the boundaries will result in spurious ``unique'' subintervals for identical peaks.
% Moreover, there are a number of statistical problems, not least of which is the fact that you're defining condition-specific intervals.
% This will surely lead to an inflated false positive rate during testing.
% You could do the same thing with libraries (being blind to the condition), but then you'll get an inflated variance if intervals are unique to one replicate.

While I'm here, I might as well note some drawbacks with hidden Markov models.
HMMs are great for describing the correlations between states, and for using information across the genome to improve inferences.
However, I don't think they work well for DB because the state distribution of the log-fold change will vary from site to site.
In other words, different DB regions will have different (expected) log-fold changes.
Trying to model them as a single state is probably inappropriate if you want site-specific inferences.
You could treat the empirical distribution of all DB regions as a state distribution, but this means that the assignment for each site gets affected by the effect size of other sites.
For example, many DB sites with strong log-fold changes would skew up the empirical distribution for the DB state.
This would reduce the posterior probability of the DB state for a site with a lower log-fold change, even if that log-fold change is large in an absolute sense.

\subsubsection*{Window resolution}

The argument for arbitrary resolution with the window width is akin to that when using a microscope. 
Do you want to focus on the details or on the big picture? 
For example, consider a cluster of DB regions separated by empty spaces.
terms, one would classify this cluster as part of the same region, so you would use a large width to increase counts and power. 
However, you could equally define each region as being of interest in its own right.

I don't think using gargantuan widths to maximize power for very diffuse regions is a great idea. 
Strange things happen at low abundances (e.g. correlations found during background normalization) which complicates validation.
Ideally, local regions should be able to carry their own weight in terms of power, which makes it much more convincing when trying to eyeball the results.

Ironically, estimation of the width is easy for sharp peaks where it's irrelevant (not much practical difference between a small value and zero) but hard for diffuse regions where it is most needed.
My attempts to estimate the width include:

\begin{itemize}
\item Autocorrelations. 
These are quite ineffective in guiding the choice of width. 
This is because for diffuse marks where they are most needed, the poor density of reads in enriched regions reduces the strength of the correlations. 
This means that you almost start off at zero (whereas for more punctate marks, the correlations at small lags are much higher). 
In short, you can't distinguish between diffuse marks and random reads. 

More generally, autocorrelations aren't that great because there's nothing to compare it to. 
It keeps dropping rather than becoming maximal at any point. 
Trying to find when it elbows off is fairly arbitrary. 
Comparing it to a threshold is also difficult i.e. distinguishing between negligble' and 'even more negligble'. 
It also ceases to be robust to the choice of threshold when relative scales are used, because the function begins asymptoting way above zero.
Small changes on the x-axis correspond to very large changes on the estimated width.

The autocorrelation function also depends on other factors such as the number of peaks.
This complicates interpretation relative to a fixed threshold.  
For example, a mark with lots of small peaks will have a higher and more persistent autocorrelation than a mark with fewer peaks, even if the latter's peaks are larger. 

\item Masking reads from each strand with those from the other strand, and computing cross-correlations using the remainders. 
The maximum point should then depend on the value of the width. 
Problem is, the exact relationship depends on the shape of the clusters. 
The effectiveness of masking also depends on the shape of the cluster e.g. a triangular distribution for each peak will have fewer overlaps and reduced masking. 
Increasing the aggressiveness of the masking then raises problems with the reliability of the results (as you start hitting the isolated forward/reverse flanks).

\item Alternatively, peak-calling programs like MACS can be used in single sample mode on pooled libraries to determine the width of enriched regions.
Pooling is necessary to avoid snooping on the DB status of regions.
Note that the width of enrichment may not necessarily be the optimal width for detecting DB regions, especially for the more diffuse histone marks.
There is also an implicit assumption that the widths of the binding sites are reasonably constant across the genome. 
Finally, it depends on the resolution of the peak-caller, and whether you need to subtract the average fragment length, etc.
\end{itemize}

The current justification for the width is that it represents the protein's footprint, such that any overlapping fragment (even by only 1 bp) will be pulled down after cross-linking.
Both forward- and reverse-mapping reads are counted inside the window, in addition to strand-specific reads flanking the window and within the fragment length.
This assumes that the protein doesn't protect the bound DNA from sonication, such that fragmentation can occur inside the footprint.
That assumption seems fair enough, otherwise we would routinely see empty spaces between the forward/reverse strand peaks, and we don't (for ChIP-seq data, anyway).
If such protection was present, the width should probably be set to 10-50 bp, as the estimated fragment length would already include the size of the footprint.

\subsubsection*{Window spacing}

The obvious case is when ext=1, where you would lose reads when windows are shifted by a spacing larger than the window size.
This can only be avoided by setting width=spacing.
If ext is large, you won't lose reads so long as spacing is less than ext. 
However, if spacing is much greater than the width, a binding site between two windows will not be represented ideally by either window.
All reads associated with that binding site (+- ext) will not be counted at either point.

This is mitigated by putting an upper bound of ext/N on the spacing, such that the maximum distance of the site to any window is ext/(2N).
I think that N=2 is a good compromise between resolution and computational efficiency.
If you set the spacing too low, the penalty from Simes will be such that you'll end up using the p-value from a suboptimal window anwyay.
You could avoid this with findMaxima(), in which case you can use any spacing you like (as you'll throw out > 99\% of the windows at the end).

In general, don't obsess about the window not getting all reads associated with a binding site.
You'll lose some even if the window was right on top of the site, due to some fragments having greater-than-average fragment lengths.
Practical usability (i.e., within memory constraints) is more important than some loss of power.

\subsubsection*{Cross-correlations}

The use of CCF plots to get the average fragment length makes some assumptions about the shape of each peak. 
In particular, it assumes that the peaks are symmetrical so that the distance between the modes is equal to the average fragment length. 
I mean, it's probably not even the mean, it's just the mode of the fragment lengths. 

The spike in the plot is due to uniqueness. 
For a given background read, most aligners will prefer (or demand) unique mapping locations. 
Any unique sequence in the genome will also have a unique reverse complement sequence which is also favoured by the aligner. 
Reads end up ``stacking'' on top of these unique sequences in both the forward and reverse directions. 
This results in an increase in correlations equal to the distance between the 5' ends of the forward and reverse reads i.e. the read length. 
A spike at zero is not observed as duplicates are removed.

For enrichment with any reasonably large width, we've assumed that the binding event provides no protection during sonication. 
If it does, you should see bimodality where there's an empty gap between the forward/reverse clusters. 
The estimated value from the cross-correlations then represents the length of the gap (i.e. the width of the binding site) plus a bit extra from variability in fragmentation sites at each end. 
If this is true, you could just use the estimated fragment length without any width value because it is modelled for free.

\subsubsection*{Variable fragment lengths}

There's no satisfactory way of dealing with differences in the strand-specific subpeak widths.
You'll need to shrink wide subpeaks towards the center of the subpeak.
This requires knowledge of where the read is in the subpeak, which is difficult to say without an explicit model.
Fortunately, I don't think is a major issue.
Variability in fragment lengths precludes identification of significant differences in lengths, if you were to treat forward/reverse strand peaks as distributions. 
In the worst case, with no variability, you'd just have read stacks where differences are clearcut.
As you add more variability, the waters become muddled and obvious differences disappear.

\subsubsection*{Mapping quality and read stacks}

Read stacks are observed as large pile-ups of (slightly offset) alignments within a small genomic interval.
These can be effectively eliminated by setting a non-negligble threshold on the mapping quality.
I suspect they're formed by repeat regions, and so MAPQ scores are generally low due to non-uniqueness.
Alternatively, they might be contaminants that are just trying to find a (poorly aligned) home on the genome build.
Use of \code{discard} to ignore these regions is less effective as they regions tend not to overlap unannotated repeats.
At the very least, you'd have to use RepeatMasker annotations, which are too aggressive for my taste.

In the worst case, direct duplicate removal may be required to get rid of stacks.
It'd be nice to have a softer removal algorithm that accounts for the neighbouring coverage to determine if reads are genuine duplicates or not.
However, I think this pre-processing is beyond the scope of csaw's responsibilities.
Alignment and read quality control is done before entry into csaw, so I'd like to avoid mission creep.
Besides, pure duplicate stacks (i.e., not driven by repeats) are only really problematic in cases where you don't have biological replicates.

\subsubsection*{Dealing with huge data sets}

Read depth isn't much of an issue, because everything gets counted into the same windows.
You might have more windows that don't get filtered, but that's capped at the total number of windows, and filtering afterwards will get rid of the background majority.
The bigger problem occurs when you have many samples (e.g., more than 100) such that the count matrix becomes uncomfortably large.
In such cases, it may be best to analyze each chromosome separately.
You'll lose some information for dispersion estimation because you don't get EB shrinkage across the genome, but when you have a lot of samples that's not a major problem.

\subsection*{Comments on normalization}

\subsubsection*{TMM on the binned counts}

<<echo=FALSE>>=
set.seed(238472)
@

Smaller counts do have greater variance in the resulting fold changes.
This is because it's more possible for smaller counts to achieve a large fold change than it is for larger counts (as the former is less stable). 
Clear evidence of this can be observed below, with concommitant effects on the estimates.

<<>>=
out <- (log(rnbinom(1000, mu=2, size=100)/rnbinom(1000, mu=2, size=100)))
var(out[is.finite(out)])
var(log(rnbinom(1000, mu=20, size=100)/rnbinom(1000, mu=20, size=100)))
@

The undefined nature of the log FC's with zero counts also complicates matters. Adding an offset doesn't help as the results will change dramatically with the value of the offset. 
This is because the choice of offset is a massive contributor to the magnitude of the resulting fold change, and thus determines where that FC is placed in the distribution.
Treatment as Inf or -Inf is suboptimal as a 1 vs 0 situation is usually not enriched (but will definitely be trimmed at Inf) whereas a 100 vs 10 situation is enriched but will be trimmed later. 

Removal is definitely suboptimal as it results in loss of information when you have a non-zero count paired with a zero count. 
The non-zero count can provide some evidence for undersampling (e.g. lots of [1, 0] pairs probably indicate undersampling). 
Zero counts can only be overcome by incorporating a probability distribution to model its frequency. 
However, this results in more pain due to the need to estimate distribution parameters and the possible lack of robustness to violation of the distributional assumptions.
More generally, discreteness reduces the accuracy of the normalization factor estimate.

<<>>=
# Simulating undersampling with spiked-in genes.
simulator <- function(ngenes, genes.spiked, mu.back, mu.spike, disp) {
    x <- matrix(rnbinom(ngenes, mu=mu.back, size=1/disp), nrow=ngenes, ncol=2)
    normed <- mu.back*ngenes/(mu.back*(ngenes-genes.spiked)+mu.spike*genes.spiked)
    x[,2] <- rnbinom(ngenes, mu=mu.back*normed, size=1/disp)
    spiked <- sample(ngenes, genes.spiked)
    x[spiked,2] <- rnbinom(genes.spiked, mu=mu.spike*normed, size=1/disp)
    return(list(counts=x, factor=normed, spiked=spiked))
}
ngenes <- 10000
x <- simulator(ngenes, 200, 2, 10, 0.05)
calcNormFactors(x$counts)
x <- simulator(ngenes, 200, 5, 25, 0.05)
calcNormFactors(x$counts)
x <- simulator(ngenes, 200, 50, 250, 0.05)
calcNormFactors(x$counts)
c(1/sqrt(x$factor), sqrt(x$factor)) # Truth.
@

As to the choice of bin size, if you have libraries of similar size, you could just throw in a bunch of bin sizes and take the most extreme (i.e. different from 1) factors that you can find.
This leverages off the fact that both too large and too small bins will underestimate the magnitude of the logarithm of the scaling factor; so, the most extreme intermediate is probably correct. 
In particular, zero counts should cause underestimation because the undersampled library should have more zero counts paired with non-zero counts in the other library. 
Thus, removal of zero counts is effectively removing evidence for undersampling. 
However, this falls apart when the library sizes are dramatically different:

<<echo=FALSE,eval=FALSE>>=
calcNormFactors(cbind(rnbinom(1000, mu=2, size=100), rnbinom(1000, mu=10, size=100)))
@

There actually isn't any undersampling but that doesn't stop us from producing an aggressive normalization estimate. 
I think it's because you only get a zero count when the smaller library is unlucky enough to get a low value. 
If you remove them, you're nibbling on just one side of the fold change distribution prior to the actual trimming. 
This means that you end up  getting a biased estimate of the normalization factor.  
So, we can't just rely on picking the biggest of the bunch anymore, particularly when the library sizes are hugely different.

\subsubsection*{Handling the bias of the TMM method}
Here I shall list the assorted failures in my attempts to truly solve this problem for low counts.

\begin{itemize}
\item Any sort of sitewise trimming procedure seemed to fail. 
I've tried trimming on the deviance and on the p-value, neither have gone down well.  
I suspect it is because of the sensitivity to the dispersion estimate, which I want to avoid estimating because it's a pain. 
In particular, different dispersions will mean that tags with the same fold change but different count sizes will get trimmed differently (whereas really they should be trimmed together).
Also, you basically convert the fold change distribution into a one-tailed thing where you only trim from one side. 
This is problematic when everything is discrete as it means you trim off big chunks at a time. 
Instability results as you will start to remove big chunks, each corresponding to opposite sides of the fold change distribution.

\item  Finding the offset with the minimal sum of deviances across sites. 
I used the tagwise dispersion estimate here.
The hope was that, for DB sites, the dispersion would be large so the deviance wouldn't change much when you altered the dispersions. 
However, this didn't work out too well as the influence of the truly DB sites would still be enough to underestimate the normalization factor. 
Indeed, when the number of DB sites is large, this tends to do worse than TMM. 
Looks like we have to remove them directly to avoid problems.

\item Finding the offset where the number of sites with increasing deviance matches the number of sites with decreasing deviance.
I've also tested variants thereof, all involving numbers of sites rather than the magnitude of the difference. 
The use of the number of sites was designed to be more robust to the presence of DB sites, assuming that the majority of sites where non-DB. 
However, this turned out to be quite imprecise as you could easily get a range of offsets with the same difference in the number of increasing/decreasing sites. 
The  presence of tied counts exacerbates this for small offsets, because a little change results in (potentially) a big number of sites switching from an increase to a decrease in their deviance.

\item Summing across background enriched regions. 
The idea was that background regions should be constant, so identifying and crunching them would give us the normalization factor fairly easily. 
Basically, we identify those bins that are likely to be background regions based on their average abundance in all libraries (trim away those high-abundance bins).
We then sum across those bins in each library, and use the ratio of those sums as the effective library sizes.
As we're dealing with a sum of counts, we can get a precise value without having to worry about discreteness. 

This works great for simulated data, but strange things happen in real data. 
The proportions doesn't change monotonically with increasing abundance, as you'd expect from increasing DE contamination. 
Rather, it peaks/troughs at a row sum of 3-5 before returning to the center line. 
This isn't just some random phenomenon - the standard errors of the proportions indicate that they are reliable (assuming independence), and it can be observed in at least three separate datasets. 
My best guess to why this is happening is that there is some correlation between the regions corresponding to those abundances.
This means that they change all at once which results in a smooth peak. 
Finally, I did these for biological replicates for which no undersampling should occur at all (though that might be optimistic given the vagaries of immunoprecipitation). 

All this results in considerable instability in the normalization procedure. 
An excellent example is with the mitochondrial genome, reads for which must be due to non-specific enrichment. 
Thus, the read count of the entire genome (without duplicate removal, because it's a bit small) can be used to get the normalization factor. 
However, in practice this is a mess, with massive scaling factors often proposed between replicates.  
The total counts are high enough for a precise estimate (~ several thousand) given independence; this suggests that our inaccuracies are due to strong correlations between counts.
While there may also be additional problems for mitochondria over nuclear DNA, I think this is reasonably demonstrative of what happens with low counts.

You can try to maximize the differences in the normalization factors within a range of thresholds to define the enriched regions.
This could get weird, because of the instability in the curves at low abundances.
Thus, you'll have to set a minimum on the threshold to avoid this.
The curve should pass through (and stay at) 0 as it switches from suppressed background to DE, so the greatest difference in the normalization factor will occur at the minimum threshold.
This means that there won't be any difference from the static method just using the minimum threshold directly.
\end{itemize}

\subsubsection*{TMM on high-abundance regions}

You can also run TMM on high-abundance bins instead of windows.
This uses larger counts that will be easier to visualize in a MA plot (can also see the behaviour of the entire genome on the plot).
However, I prefer running it on the same windows you're going to use for the analysis.
This synchronizes the normalization with the filtering, and guarantees that you're eliminating the biases in the counts of interest.

Using window counts for TMM is also a step closer to the procedure for non-linear normalization.
This means you can easily switch to that, e.g., if you didn't filter away all of the background and you want to normalize them as well.
However, I generally find that it's quite difficult to identify a trend when you have window-based counts, as the covariate range is usually too small.
This motivates the use of scaling normalization, even in the likely presence of some background windows.
(Non-linear methods also have some other problems that are discussed below.)

Note that each read is counted multiple times with windows instead of bins, though this isn't a real problem.
The only thing to worry about is that it will effectively upweight high-abundance peaks which get caught by multiple windows, but everyone should have high abundances so it shouldn't matter.

\subsubsection*{How to \textit{not} decide between normalization methods}

Just because a high-abundance mass at a non-zero M-value is discrete, doesn't mean it's caused by efficiency differences.
You could imagine a consistent fold increase in the counts from a genuine increase in binding, if the protein level or deposition rate doubled in one condition.
Obviously, if you don't see a non-zero blob, then the argument is academic, as there's no difference between the two normalization methods.

You might also imagine that the presence of a high-abundance mass at a non-zero M-value between replicates would represent efficiency bias.
This is true, but it doesn't mean that binding is constant between conditions.
The variability might mask any DB, and absence of evidence is not evidence of absence.
Eliminating the efficiency bias will compromise detection of systematic DB, so you'll have to pick the lesser of two evils.
I suppose the exception is if you've enough replicates to get the true overall DB without being clouded by the variability, but that's rare for ChIP-seq data.

Conversely, the presence of a non-zero mass between conditions but not between replicates does not mean that efficiency bias is absent.
While stochastic efficiency bias mightn't be generated between replicates, a systematic bias might be present between conditions.
This may be caused by condition-specific factors, e.g., differences in nuclear shape, changes in cross-linking due to serum concentrations.

\subsubsection*{The lesser of two evils in normalization choice}

Normalization to remove composition biases may be undoubtedly correct in some cases, e.g., where you reduce the levels of the target protein.
However, if the IP step is highly variable, you may end up failing to detect anything at all, as the dispersion will be inflated by this normalization strategy.
In such cases, it may be preferable to normalize to remove efficiency biases.
Then, only the DB sites that change in the expected direction should be retained.
This normalizes across the majority of the expected DB binding sites, sacrificing them in order to detect the top set of DB.
For experiments with variable IP efficiency, this may still detect more sites than normalizing for composition bias.

\subsubsection*{Caveats with non-linear methods}

The current implementation uses a fast approach, whereby all samples are compared to an average library.
This is valid as you're just using the average as a proxy between which libraries are compared.
As the covariate is also the average count, you're effectively comparing the trend for each library to the same diagonal line.
You don't need to worry about comparing each library to, e.g., the average of all other libraries.

However, there is a caveat with using the overall average as the covariate.
When you have DE, the concept of the overall average makes no sense.
This means that the point will get misplaced and normalized in strange ways.
Normally, this isn't a problem, as the correct normalization for DE genes isn't well defined.
However, if you have more than two groups, DE in the third group would affect normalization for non-DE genes between the first two groups.

This is shown in the example below.
The first gene displayed should have no differences between the first two samples upon normalization, but DE in the third sample prevents this from happening.
A similar case can be made for the second and third genes, in which there is DE in the second and first samples respectively.
In contrast, the last gene is non-DE across all groups and behaves properly.

<<echo=FALSE>>=
set.seed(231232)
@

<<>>=
ngenes <- 10000
true.means <- runif(ngenes, 1, 10)
y <- matrix(0, nrow=ngenes, ncol=3)
y[,1] <- rnorm(ngenes, mean=true.means)
y[,2] <- rnorm(ngenes, mean=2*true.means)
y[,3] <- rnorm(ngenes, mean=1.5*true.means)
y <- rbind(y, c(2, 4, 10), c(2, 10, 3), c(10, 4, 3), c(2,4,3))	
y.fast <- normalizeCyclicLoess(y, method="fast")
tail(y.fast, 4)
@

One might think to overcome this problem by using covariates computed in a pairwise manner.
This fixes the first gene, but the second and third genes still have spurious DE.
This is because the success of this procedure depends on the order with which the libraries are normalized.
In the first gene, the first two libraries are equalized first, which results in consistent behaviour for those two libraries across the remaining normalization steps.
This is not the case for the second and third genes, where DE inflates the covariate in the first pairwise normalization.
This distorts the normalized value and affects the outcome of all remaining normalization steps.

<<>>=
y.pair <- normalizeCyclicLoess(y, method="pair") 
tail(y.pair, 4)
@

Affymetrix-based normalization is a slight modification of the pairwise strategy, where the adjustment to the values is made after all pairwise normalization steps are performed.
This does somewhat better, though the essence of the problem still remains.
Note that changing the number of iterations in both cases doesn't seem to help.

<<>>=
y.affy <- normalizeCyclicLoess(y, method="affy") 
tail(y.affy, 4)
@

Comparisons to a reference don't help consistently, as it depends on the choice of reference.
In the code below, each reference fails for the gene in which DE is present in that reference.
Another disadvantage of pairwise covariates is that they are less stable than the overall mean.
This results in a modest increase in the normalization error, though nothing to really panic about.

<<>>=
for (ref in 1:3) {
    y.x <- y
    cat("Current reference is", ref, "\n")
    for (x in 1:3) { 
        if (x==ref) { next }
        fit <- loessFit(y[,x] - y[,ref], rowMeans(y[,c(x, ref)]))
        y.x[,x] <- y.x[,x] - fit$fitted
    }
    print(tail(y.x, 4))
}
@

In short, there's no obvious solution to this problem.
I'm going to stick with the what I've got right now, as it's easy to use and understand. 

\subsubsection*{Dealing with other biases}

Note that Cheung (i.e. Ahringer) go about dividing ChIP samples by input controls and stating that there are differences in the called regions between replicates. 
If they had used a model that accounted for replicates in the first place, different results should be avoided as the dispersion would be higher.  
Also, their method for avoiding biological signal in their background model depends on a powerful peak caller to mark enriched regions. 
Absence of evidence isn't evidence of absence, so if your peak caller is weak, the method will be confounded by biological signal that leaks through your filtering step.  
Risso's definition of bias involves comparisons between qPCR and RNA-seq fold changes. 
I suppose this is fair enough if you want to improve comparisons between technologies. 
However, this isn't particularly relevant to the correction of biases between samples.

Generally, it's worth dividing GC bias into two types:
\begin{itemize}
\item The first is where the true variances of independent sites are correlated with GC content. 
This suggests that you might get more appropriate smoothing if you partitioned the sites according to GC content first. 
This isn't too hard (find regions with desired GC content and split windows accordingly) but you will bleed degrees of freedom at high counts where the number of sites isn't that high to begin with. 
GC content is mostly interesting because of its effect on read abundance, and we can smooth directly on that anyway. 
Otherwise, it's just another metric. 
We could also smooth on other (equally arbitrary) parameters such as genomic context, position to promoter, etc. but we have to draw the line somewhere. 

\item The second type is that there is a systematic bias in the means of sites with similar GC contents.
This is what you usually see when someone talks about GC bias correction i.e. the boxplot of ratios of counts with similar GC contents need to be scaled to the y=0 line. 
It is reasonable to correct for these, though it may result in loss of power if GC content is correlated with DB.
Also, the correction uses offsets which might not be compatible with non-linear normalization.
Given a choice, I'd prefer to remove any trended bias directly, as I \textit{know} it would mess with my mean-dispersion trend estimation.
Besides, the GC content vs. M-value plots are pretty tame; there's not much of a trend as the bulk of points occur in a limited range of GC contents.
(Specifically, the noise around the trend is larger than the range of covariates, making it difficult to pull out a trend.)
\end{itemize}

The second type of GC biases will just be absorbed into the dispersion estimates, inflating them for affected windows along with the variability of the dispersions.
(The latter assumes that most windows are unaffected by GC biases after scaling normalization, such that the inflated dispersions will decrease the prior degrees of freedom.)
As such, we'd be erring on the side of conservativeness if we failed to normalize out GC content.
One could argue that the EB shrinkage statistics won't be precisely estimated if there's a stochastic sample-specific effect that hasn't been normalized out.
However, if only a minority of windows are affected (at the ends of the trend, generally with unusually high or low content), then this shouldn't be a problem.
In such cases, the GC bias would just introduce some correlations between a small subset of windows - no problems.

\subsubsection*{Why spike-ins work}

The key part of the spike-in procedure is that whatever spike-in you're using, it needs to contain the same target as your sample of interest.
Moreover, the antibody must be able to recognise the target in both species (see Discussion of Bonhoure \textit{et al.}, 2014).
This ensures that any differences in IP efficiency are reflected in the coverage of the spike-ins.
Otherwise, it wouldn't provide any advantage over normalizing across background regions.
This is fine for histone marks that tend to be uibiquitous across species, but it gets tougher if you're looking at specific proteins, e.g., transcription factors with species-specific epitopes.

\subsection*{Comments on filtering}

\subsubsection*{Further on the motivation for filtering}

The main argument for filtering - that low-abundance windows don't have enough power for DB - is true to a point, but not near the thresholds used in actual analyses.
You'll still be able reject the null for windows with consistently low counts in one group and zeroes in the other group.
Even accounting for the fact that the trended NB dispersion is very high at low counts, you can get low p-values if the sample size is large enough.
The real motivations include the need to avoid discreteness (and other oddities at low counts) during mean-variance modelling, where discreteness can inflate the prior d.f.;
    avoiding discreteness during normalization, which relies on some level of smoothness in the M-values;
    and removing windows that are unlikely (for biological/experimental reasons) to reject the null hypothesis, or are uninteresting when they do so, e.g., background regions.

\subsubsection*{Circular dependencies between normalization and filtering}
The NB mean depends on the effective library size, so filtering for efficiency bias becomes a bit circular.
However, it shouldn't be too bad, as the normalization procedure isn't so sensitive to irregularities at the filter boundary.
In practice, there's little-to-no effect; I'd only expect something if the normalization factors are really crazy.

<<echo=FALSE,eval=FALSE>>=
for (it in 1:3) {
	ab <- aveLogCPM(assay(me.demo), lib.sizes=me.demo$total*me.norm)
	keep <- rank(ab) > 0.99*length(ab)
	me.norm <- normalize(me.demo[keep,])
	cat("Iteration is", it, "\n")
	print(me.norm)
}
@

Filtering is also sensitive to the NB dispersion when library sizes are different.
However, dispersion estimation can only proceed after normalization and filtering. 
This results in another circular dependency that is resolved by using a sensible (but otherwise arbitrary) value for the NB dispersion in \code{aveLogCPM}. 
The alternative would be to iterate over the entire DB analysis, which would be prohibitively time-consuming in most circumstances.
Of course, this is not an issue when library sizes are the same.

\subsubsection*{Choosing the filter with maximal discoveries}

Choosing the filter which gets you the greatest number of DE regions isn't the best idea, simply because it means that it's not independent of the p-values any more. 
At any given filter value, the expected FDR is controlled by the BH method rather than the actual FDR. 
Trying to maximize the number of significant tests will result in loss of FDR control as it's not an unbiased expectation anymore. 
It's worst with strong correlations between the nulls.

<<echo=FALSE>>=
set.seed(1478343)
@

<<>>=
collected <- list()
max.collect <- list()
for (y in 1:50) {
	subcol <- NULL
	subfdr <- NULL
	n <- 10000
	infilter <- rep(runif(n, 0, 1), 10)
	allps <- rep(runif(n, 0, 1), 10)
	spike <- 500
	for (x in 1:9/10) {
		keep <- infilter>=x
		combined <- c(rep(0, spike), allps[keep])
		sig <- sum(p.adjust(combined, method="BH")<0.05)
		subcol <- append(subcol, sig)
		subfdr <- append(subfdr, 1-spike/sig)
	}
	collected[[y]] <- subfdr
	max.collect[[y]] <- subfdr[which.max(subcol)]
}
out <- do.call(cbind, collected)
rowMeans(out)
mean(unlist(max.collect))
@

In general, you'd need to know the number of true/false positives \textit{a priori} to pick the "optimal" filter beforehand.
Alternatively, the regularization stuff presented by Wolfgang at Bioconductor 2015 might help here.
However, this would assume that you're trying to maximize discoveries without caring what they actually are.
It's possible that some of the discoveries at low abundances are uninteresting (e.g., changes in chromatin state).
Lowering the filter to pick these up and maximize discoveries would end up reducing power for the enriched regions that we're interested in.

\subsubsection*{Using the FDR over the log-fold change for filtering} 

It is possible to develop a more sophisticated background-based filter where each window is tested against a null background model.
Windows with low adjusted $p$-values can then be selected to obtain a set of potentially bound intervals at a certain false discovery rate.
While this can be done in \pkgname{}, the philosophy of the background-based filters in \code{filterWindows} is to use the log-fold increase of each window relative to the background.
This is easier to compute, and the threshold is easier to pick and interpret for users.

\begin{itemize}
\item Consider the purest case, where you determine significance based on the enrichment values (use a constant variance, so same selection order).
If you pick a set based on the FDR, the selection of one region becomes dependent on how many other regions have higher enrichments.
This might be annoying if you're trying to use a stable pre-set threshold for ease of interpretation.
High FDR thresholds might also start to run into problems with monotonicity, where filter resolution is lost.

\item The use of a constant threshold for the p-value focuses on control of type I error and elimination of uninteresting features. 
This is fair enough but you're not controlling for the type II error rate, i.e., retention of interesting features.
For example, if the variability under the null is high, then all features will have large p-values and nothing will be retained.
Applying a constant threshold on the enrichment ensures retention of interesting features (with sufficient enrichment), no matter what the variability may be.
This is more generally applicable to different experiments and systems where the null model may change in its variability.
The test statistic is only more stable if effect size is positively correlated with the null variability, and it's hard to see how that would happen.

\item Another problem is that you're requiring rejection at a standard FDR of 5\% in order to get into the retained set.
This may be too conservative for preliminary selection, where windows that cannot reject here might have a lot of evidence for DB.
In the end, you might end up having to fiddle with non-conventional FDR thresholds (e.g., 10-20\%), which gets us back to the problem of arbitrariness.
Remember that the aim of filtering is not to do rigorous peak-calling, but to trim down the total number in order to detect DB regions.

\item Getting a FDR for your filter doesn't have any effect on FDR control for differential binding.
It will affect power, but not in a way that's easy to interpret, because it depends on how many true and false positives (re. DB, not absolute calling) were retained.
Conversely, having a minimum log-enrichment tells us a bit about the maximum scope of DB change (assuming that background is non-DB).
For example, a log-enrichment of 2 suggests that you can get, at most, a DB fold change of 2 for your regions that scrape past the filter.

\item Most analyses use a Poisson model with lambda set to the background coverage, but how do you model variability between replicates, or variability between genomic regions?
Using a NB model seems better but that assumes you can get a sensible dispersion estimate without biasing edgeR's inferences.
In both cases, the filter outcome is dependent on sequencing depth, as the NB model will eventually reject all locations if the counts are large enough.
Using an enrichment threshold (or \code{treat}) is more stable with respect to depth, and possibly gives more relevant regions by focusing on large enrichments.
For example, we could avoid detecting DB due to changes in chromatin state, as most of these shouldn't have strong enrichment above background (unless you do DNase/ATAC-seq).
\end{itemize}

% Using a local estimate of background will also result in correlations between the window count and the background.
% This will likely result in sub-Poisson variation and conservative p-values. 
% Not really a problem in itself, but it will affect the interpretation of the p-values if you then have to go and adjust the threshold again.
% It might be better to compute the average count across all libraries, and use this in a Poisson model against the count for an independent negative control.

\subsubsection*{Independence of the average abundance as a filter statistic}

The analogy to the normal case suggests that the average abundance is the independent filter statistic.
This seems to hold up for different library sizes, unbalanced designs and correlations.
Note that this assumes that the null design matrix can be parametrized with an intercept column.
The independence of the filter is important as most windows are low-abundance, which means that the density at the filter boundary is high relative to the number of retained windows.
Thus, any biases introduced during filtering will have a large effect on the DB analysis.
(Incidentally, this is another justification for the stringent peak-calling threshold in the simulations of 2014 NAR paper.)

With respect to clustering; it should be valid to cluster retained windows, as the clustering depends on coordinates and not the statistics of each window.
Or, you can think about it as conditioning on cluster formation where each constituent window must pass the filter threshold.
Independence ensures that the $p$-values of all windows are still uniformly distributed within each cluster, so it should be okay.
This doesn't work for clusters formed from the sign of the log-fold changes, though.
Even though the sign is independent of the $p$-value for each window, it's not so for correlated windows.
Regions with (spurious) DB are more likely to have windows with similar signs that form a single cluster, whereas regions with no DB will have random signs.

The situation is also more difficult for the local filtering methods.
Here, the filter statistic is defined as the difference in correlated abundances between each window and its neighbourhood.
Independence between the $p$-value and each abundance is not enough, as the example with the signs suggests.
Fortunately, analogies with the normal case suggest that any statistic computed from the overall means of correlated windows is independent of the $p$-values for those windows.

% Cut up the uniform distribution into two halves length-wise, and each of those halfs into two triangular slices.
% This gives you four slices; two upper triangles, and two lower triangles.
% Assume that each slice corresponds to a combination of two filter statistics, where both must be okay to retain.
% Filtering on either statistic alone could acquire one upper and lower triangle, such that the $p$-value distribution is still uniform.
% However, filtering on both statistics could yield only one triangle and a non-uniform distribution.
%
% A proof of validity requires consideration of the correlation structure -- 
%	it seems to work out under normality, where filtering on a difference in abundances will remove the same proportion from each hyperplane of the multivariate normal.
% Consider multiple windows with normally-distributed observation vectors of y_1, y_2, ... with mean 0.
% Assume that, for any pair of windows, all corresponding observations are correlated by some constant covariance.
% (The covariance doesn't have to be the same for each pair, and non-corresponding observations are still independent).
% Let y*_1 denote an instance of y_1, and t_1 denote a scalar added to all observations in y*_1.
% This means that the vector [y*_1+t_1, y*_2+t_2, ...] is a MVN with a mean vector of zeroes and a striated covariance matrix (only diagonals of each internal square are non-zero).
%
% Anyway, consider the PDF of obtaining the vector at a given t_1, t_2, ... (noting that the inverse covariance matrix is similarly striated).
% You can reformulate it as a distribution in terms of t_1, t_2, ...
% Specifically, the space obtained by varying t_1, t_2, ... is a hyperplane, and the probably density of sampled points follows a MVN.
% All points on this hyperplane have the same p-value and variance, as the values of t_1, etc. cancel out when computing these statistics.
% The mean vector is equal to the negative means of y*_1, y*_2, ..., while the inverse covariance matrix should only depend on the original covariances. 
% All this means that you can split up the original MVN into a bunch of hyperplanes that only vary in their mean.
% As long as you remove the same proportion of probability density from each hyperplane, the filter will be independent.
%
% This gets us to the crux of the matter, in that the filter function for each hyperplane should only vary with the location of the mean vector of the hyperplane MVN.
% This mean vector is equal to the sample means of the observed instances corresponding to that hyperplane.
% Thus, as long as your filter statistic only depends on the sample means, the same proportion will be removed from each hyperplane.
% For example, requiring that all windows are above a threshold would be equivalent to saying that mean(y*_i + t_i) > T.
% This becomes t_i > T - mean(y*_i), which will always be T above the mean vector for each hyperplane.
% The same applies for more complex filters involving interactions between windows, e.g., if mean(y*_1 + t_1) - mean(y*_2 + t_2) > T.

\subsubsection*{Misuse of the negative control libraries for filtering}

Some people have proposed filters based on detecting DB between the ChIP libraries and the input libraries.
This is unwise, because if you treat all ChIP smples as replicates, then those regions with DB between ChIP samples will have large dispersions.
This means that they will get filtered as they are not significantly enriched over the input, which makes the filter rather useless.
An alternative approach is to fit the proper model to the ChIP samples, and then test whether the average enrichment is greater than the input.
This avoids the problem with inflated dispersions -- however, a related but more subtle effect arises where regions with large dispersions are more likely to be filtered out.
This is an implicit form of filtering on the variance, which is not healthy for edgeR and friends which need to model the full range of dispersions.

\subsubsection*{Selective filtering based on the contrast}

One common complaint with the average abundance is that performance will deteriorate as the number of groups increases.
For example, if a peak was only present in one group, the average abundance of that peak would drop as the number of groups increases.
This would affect the ability of the filter to retain that peak.
To get around this, you can filter using the average abundance of the libraries involved in the DB contrast.
This ensures that you're only selecting for high abundances in the libraries of interest.

Selective filtering does require some care.
You need to re-filter for each contrast, which necessitates re-estimation of the NB dispersion and repeated GLM fitting.
Independence also requires that the null design has an all-ones column for those libraries involved in the contrast.
The behaviour of the other libraries shouldn't matter, as they will only contribute to EB shrinkage.
If libraries are independent, filtering on the involved subset shouldn't affect the information extracted from the other subset.

% Cleanest under the null, due to correlations in the underlying means between groups under the alternative.
% Selection of strong DB peaks might favour selection of other strong DB peaks in other groups that have lower dispersions in the mean-variance trend.
% Though, for this to be an actual problem, it would need to break the assumption of constant dispersions across all libraries.

\subsubsection*{Role of the prior count}

The prior count has two roles - to stabilize both the numerator and denominator in the calculation of the log-fold change.
This is because both of them are randomly sampled values.
For the denominator, stabilization is necessary to avoid spuriously large fold changes when the sampled value is near zero.
Application to the numerator is less obvious but is still required.
Consider a region with a low but precisely estimated (if not known) mean of the read counts. 
Most subintervals of this region are empty, but the subinterval with the fortune of obtaining a single count will get a very high log-fold change.
A prior count protects against this stochasticity.

For the global and local enrichment methods, the prior count focuses on stabilization of the numerator.
This is because the denominator is estimated from large bins and can be expected to be precise.
For the control method, both the numerator and denominator are estimated from small windows.
This means that the variability of the final log-fold change will be doubled, which will affect the discriminative power of the filter.
A larger prior count is used to prevent this.
More practically, it mandates a minimum count, e.g., a fold change of 3 with a prior count of 5 is only possible if the count is at least 10.

\subsection*{Comments about the statistical testing}

\subsubsection*{Batch effects and the prior d.f.}
The prior d.f. will be large if there is a systematic batch effect, e.g., due to efficiency biases.
This is because the variability of the dispersions will be low when all of them are consistently large to reflect the constant difference between replicates.
In contrast, the prior d.f. will be small if there is a stochastic batch effect, e.g., due to hidden factors of variation.
This is because the dispersions will be absorbing a batch effect that is differing randomly between windows.
As a result, the variability in the dispersions will be increased.
There mightn't be a large increase in the dispersions themselves due to a stochastic batch effect, as not all features will be heavily affected by the batch to cause a substantial increase in the fitted trend or the common estimate.
However, small prior d.f. can result in almost as much damage as large dispersions, so watch out.

\subsection*{Comments about FDR control}

\subsubsection*{Algorithm for splitting large clusters}

I suppose we could choose the number of clusters that maximises the average density. 
It's the most logical way to do it, as we are interested in high-density regions. 
This will also avoid chaining as clusters with a lot of empty space will be broken up. 
However, it also requires an arbitrary compromise between the number of clusters and the density. 
Specifically, there's liable to be a point at which chaining is acceptable, e.g., satellite window adjacent to a peak, regions of diffuse continuous enrichment. 
And that's without considering the fact that multiple peaks might need to be clustered together for proper FDR interpretation. 
So, the question becomes, how big of a density penalty should be applied to a new cluster? 
I think that this is more difficult to interpret compared to the existing method, where parameters are expressed as distances. 
And besides, the existing method is fast enough to run a couple of times to check the parameters.

\subsubsection*{Filtering on the sign of the log-fold change}
There are also provisions for clustering based on the sign of the log-fold change. 
The idea is that clusters will be broken up wherever the sign changes. 
This will separate binding sites that are close together but are changing in opposite directions. 
It will also make cluster definitions easier, as windows corresponding to a single event are likely to have the same fold change (as distinct from background, which should be more-or-less random).
A vector can be supplied in \code{sign} to indicate whether each window has a positive log-fold change.

<<eval=FALSE>>=
merged.sign <- mergeWindows(rowRanges(data), tol=1000L, sign=(results$table$logFC > 0))
@

For routine analyses, sign-based filtering is not recommended as the sign of windows within each cluster is not independent of their DB status. 
Windows in a genuine DB region will form one cluster (consistent sign) whereas those in non-DB regions will form many clusters (inconsistent sign, as the log-fold change is small).
This results in conservativeness as more clusters will have large $p$-values.
Furthermore, any attempt to filter away small clusters will cause liberalness if too many large $p$-values are lost.

If you want to work on the sign of the log-fold change, it's best to convert each two-sided $p$-value into a one-sided counterpart.
This can be done by setting the $p$-value to $p/2$ for the correct sign, and $1-p/2$ otherwise.
I'm not sure that filtering on signs would be sensible as it wouldn't be independent for correlated windows.
If you want to work on the signs of multiple log-FCs, the best approach might be to do each comparison independently and compute an IUT $p$-value across all one-sided $p$-values.
It is difficult to show that filtering on multiple signs for an ANOVA-like comparison would be independent.
In any case, the intersection is easier to interpret as it must be significant across all comparisons.

\subsubsection*{Arguments for proper consolidation}

The \textit{ad hoc} alternative is to just merge regions from multiple DB lists together.  
You'll end up with overlapping results, which means that you wouldn't be controlling the FDR across the merged set. 
In addition, merging DB lists that have been controlled at a given FDR assumes that there is at least one true positive in each list. 
This mightn't be true, in which case FDR control will be lost (you'll be increasing the FDR by adding to both the numerator and denominator).

To illustrate, split a bunch of $p$-values into two sets, and control the FDR within each set.
If you then merge the sets, the FDR should be controlled within the merged sets, right?
However, if you take this to its extreme, you could split those $p$-values into trivially small sets, e.g., of size 1.
Controlling the FDR within each set would not change the size of the $p$-values.
So, upon merging, you'd be defining significance from the raw $p$-values, which doesn't control the FDR.

<<echo=FALSE>>=
set.seed(141233)
@

<<>>=
all.p <- c(rep(1e-8, 100), runif(900))
sig <- p.adjust(all.p, method="BH") <= 0.05
sum(sig[-(1:100)])/sum(sig)
alt.sig <- all.p <= 0.05
sum(alt.sig[-(1:100)])/sum(alt.sig)
@

\subsubsection*{Using the highest-abundance window as a representative}

This avoids complex DB events by looking for regions that are changing at the site of maximum binding.
In this manner, you avoid detecting changes in shoulders or subpeaks of the main binding event.
This still has some advantages over straight-up peak calling, as you still maintain high resolution, i.e., you don't get contamination from miscalled peak boundaries.
I guess it turns into something similar to a summit-based analysis of peaks.

On a more philosophical note, why should we bother with complex events?
Simple DB will probably give a better correlation with transcriptional activity, as large-scale changes in marking should have some effect.
I'd argue that it's unwise to focus on this correlation when interpreting ChIP-seq data -- if you wanted a direct measure of transcription, then you might as well do RNA-seq.
It's worth tolerating some complex DB changes that are irrelevant, in order to find those that affect your genes (or more generally, regions) of interest.
You'd have to do so anyway with simple DB, as transcription is dependent on many things other than binding of a particular target protein.
In more mechanistic terms, changes outside of the highest-abundance window may expose or sequester binding motifs for other proteins, which would have important effects.
Complex DB may also result from simpler DB where the increase in enrichment results in an apparent spread of the mark.
The subtle change at the center of the site is missed, but the larger relative changes at the edges can still be detected.

\subsubsection*{Annotating regions for complex DB}

If you annotate a complex DB region, there's no guarantee that the DB subinterval actually overlaps the annotated feature of interest.
The better approach would be to combine $p$-values across the feature of interest, to get a direct summary for DB at that feature.
Trying to do so with annotation of \textit{de novo} clusters is not easy.
You'd need to overlap the features with significant windows within each cluster.
This would require some definition of a window-level error rate, but this makes no sense if you end up interpreting the results in terms of clusters or genes. 

Another related problem would be when someone, in looking inside a cluster, wants an error rate for a specific window.
The obvious approach would be to use a Bonferroni/Holm-corrected window-level $p$-value, where the correction is applied based on the number of windows in that cluster.
This maintains strong FWER control for any arbitrary selection of hypotheses.
The resulting value should also  be larger than the combined $p$-value of the cluster (for Bonferroni, at least).
The real question is whether it is necessary to also correct across clusters.
You can do so by replacing the combined $p$-value with the corrected value during the BH adjustment, for this cluster only.
This effectively replaces ``there is DB somewhere in the cluster'' with ``there is DB at this location in the cluster''.
We preserve a region-level interpretation, as it would be problematic to try to control the window-level FDR.

\end{document}

