---
title: Anticonservativeness in peak selection strategies
author: Aaron Lun
date: 19 August 2017
output: 
   html_document:
     fig_caption: false
---

```{r, echo=FALSE}
knitr::opts_chunk$set(error=FALSE, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(fig.path="figures-peak/")
```

# Background 

The 2014 NAR paper (https://doi.org/10.1093/nar/gku351) showed that many _ad hoc_ peak selection strategies result in conservativeness.
One could consider erring on the side of conservativeness to be acceptable, especially if more DB sites pass the filter.
However, this simulation demonstrates that the same strategies can also result in loss of type I error control.
The possibility of anticonservativeness means that any increased detection from _ad hoc_ strategies cannot be trusted.
Consider an experimental design with two replicates in each of two conditions.

```{r}
group <- rep(c("A", "B"), each=2)
nlibs <- length(group)
design <- model.matrix(~group)
```

We'll simulate some counts where 10% of the sites are DB.

```{r}
library(edgeR)
set.seed(90000)
P <- 1/0.1
n.sites <- 1e5
db.sites <- n.sites*0.1
is.null <- seq_len(n.sites)
counts <- rbind(matrix(rnbinom(n.sites*nlibs, mu=50, size=P), ncol=nlibs, byrow=TRUE),
                matrix(rnbinom(db.sites*nlibs, mu=c(100, 100, 0, 0), size=P), ncol=nlibs, byrow=TRUE))
```

We'll also set up a function to assess type I error control.

```{r}
plotAlpha <- function(pvals, ylab="Observed/specified", xlab="Specified", xlim=NULL, ...) {
    exp <- (seq_along(pvals) - 0.5)/length(pvals)
    n <- findInterval(exp, sort(pvals))
    obs <- n/length(pvals)
    if (is.null(xlim)) { # Stable at 20 observations.
        xlim <- c(exp[which(n >= 20)[1]], 1)
    }
    plot(exp, obs/exp, log="xy", xlim=xlim, ...)
}
```

# Applying the "at least 2" filter

Let's see what happens when we apply the "at least two" filter to retain the top proportion of sites.

```{r}
top.al2 <- apply(counts, 1, FUN=function(x) { sort(x)[nlibs-1] })
keep.al2 <- length(top.al2) - rank(top.al2, ties.method="random") +1 <= 20000
summary(which(keep.al2) %in% is.null)
```

Running these counts through _edgeR_, using standardized library sizes.
There is some inflation, but the presence of correct dispersion estimates for the DB sites keeps the dispersion low.

```{r}
y.al2 <- DGEList(counts[keep.al2,], lib.size=rep(1e6, nlibs))
y.al2 <- estimateDisp(y.al2, design)
y.al2$common.dispersion
```

We observe loss of type I error control at low _p_-values.
This is because the dispersion inflation is minimized _and_ the "at least two" filter selects for spurious DB sites.

```{r}
fit.al2 <- glmQLFit(y.al2, design, robust=TRUE)
res.al2 <- glmQLFTest(fit.al2)
mean(res.al2$table$PValue[which(keep.al2) %in% is.null] <= 0.001)
plotAlpha(res.al2$table$PValue[which(keep.al2) %in% is.null])
```

# Applying a union filter.

Repeating the dose with a union filter.
Here we retain fewer sites, which ensures that the DB percentage in the retained set is higher (see below).

```{r}
top.u <- apply(counts, 1, FUN=function(x) { max(x) })
keep.u <- length(top.u) - rank(top.u, ties.method="random") +1 <= 5000
summary(which(keep.u) %in% is.null)
```

Running these counts through _edgeR_.
The higher DB percentage keeps the dispersion inflation low.

```{r}
y.u <- DGEList(counts[keep.u,], lib.size=rep(1e6, nlibs))
y.u <- estimateDisp(y.u, design)
y.u$common.dispersion
```

Testing again results in the loss of type I error control.
Normally, the union approach enriches outliers and inflates the dispersion.
However, enough DB sites ensures that the inflation is minimized, encouraging spurious rejection of the null.

```{r}
fit.u <- glmQLFit(y.u, design, robust=TRUE)
res.u <- glmQLFTest(fit.u)
mean(res.u$table$PValue[which(keep.u) %in% is.null] <= 0.05)
plotAlpha(res.u$table$PValue[which(keep.u) %in% is.null])
```

# Applying the mean filter

Now, to demonstrate the correct way of doing it, we use a filter on the mean count.

```{r}
top.m <- rowMeans(counts)
keep.m <- length(top.m) - rank(top.m, ties.method="random") +1 <= 10000
summary(which(keep.m) %in% is.null)
```

Running these counts through _edgeR_.
The higher DB percentage keeps the dispersion inflation low.

```{r}
y.m <- DGEList(counts[keep.m,], lib.size=rep(1e6, nlibs))
y.m <- estimateDisp(y.m, design)
y.m$common.dispersion
```

Testing indicates that type I error control is mostly maintained.

```{r}
fit.m <- glmQLFit(y.m, design, robust=TRUE)
res.m <- glmQLFTest(fit.m)
mean(res.m$table$PValue[which(keep.m) %in% is.null] <= 0.01)
plotAlpha(res.m$table$PValue[which(keep.m) %in% is.null])
```

# Using the mean filter with variable dispersions

The sample mean is approxiamtely independent of the dispersion estimate and p-value for Poisson and NB-distributed counts.
However, this only applies to a single distribution.
Consider a mixture of NB distributions with different dispersions but the same mean.

```{r}
set.seed(100000)
P.n <- rchisq(n.sites, df=10)
P.db <- rchisq(db.sites, df=10)
counts <- rbind(matrix(rnbinom(n.sites*nlibs, mu=50, size=P.n), ncol=nlibs, byrow=TRUE),
                matrix(rnbinom(db.sites*nlibs, mu=c(100, 100, 0, 0), size=P.db), ncol=nlibs, byrow=TRUE))
```

Running on all the sites doesn't cause any issues other than some anticonservativeness at low _p_-values.
This is relatively modest and acceptable given that the inverse-chi-squared distribution for the NB dispersions only mimics the true QL model.
(In contrast, the _ad hoc_ filters were tested on purely NB counts, so they should not have any problems.)

```{r}
y.all <- DGEList(counts, lib.size=rep(1e6, nlibs))
y.all <- estimateDisp(y.all, design)
y.all$common.dispersion
fit.all <- glmQLFit(y.all, design, robust=TRUE)
res.all <- glmQLFTest(fit.all)
mean(res.all$table$PValue[is.null] <= 0.01)
plotAlpha(res.all$table$PValue[is.null])
```

However, applying a stringent mean filter will select for higher dispersions.
This is because high-dispersion features are more likely to achieve large sample means. 
The result is to encourage inflation of the dispersion estimate.

```{r}
top.m2 <- rowMeans(counts)
keep.m2 <- length(top.m2) - rank(top.m2, ties.method="random") +1 <= 1000
summary(which(keep.m2) %in% is.null)
y.m2 <- DGEList(counts[keep.m2,], lib.size=rep(1e6, nlibs))
y.m2 <- estimateDisp(y.m2, design)
y.m2$common.dispersion
```

Funnily enough, this doesn't actually hurt the type I error rate.
This is possibly because the same issue affects both DB and non-DB sites, i.e., DB sites with high dispersions will be similarly preferred.
This ensures that there is no enrichment for low-dispersion DB sites to suppress the variance inflation and lead to anticonservativeness.

```{r}
fit.m2 <- glmQLFit(y.m2, design, robust=TRUE)
res.m2 <- glmQLFTest(fit.m2)
mean(res.m2$table$PValue[which(keep.m2) %in% is.null] <= 0.01)
plotAlpha(res.m2$table$PValue[which(keep.m2) %in% is.null])
```

In practice, this is not much of an issue.
For ChIP-seq data, the prior degrees of freedom is usually quite high such that there is not much variability in the dispersions.
For RNA-seq data, the filter boundary is not dense so the specifics of filtering doesn't matter.
The same effect is present in the other filters anyway (in addition to their poor performance on the constant dispersion case),
    as high dispersions make it more likely to get one or two peaks above the threshold.

# Wrapping up

```{r}
sessionInfo()
```

