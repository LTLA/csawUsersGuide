\documentclass[12pt]{report}
\usepackage{fancyvrb,graphicx,natbib,url,comment,import,bm}
\usepackage{tikz}
\usepackage[hidelinks]{hyperref}

% Margins
\topmargin -0.1in
\headheight 0in
\headsep 0in
\oddsidemargin -0.1in
\evensidemargin -0.1in
\textwidth 6.5in
\textheight 8.3in

% Sweave options
\usepackage{Sweave}
\SweaveOpts{keep.source=TRUE,prefix.string=plots-ug/ug,png=TRUE,pdf=FALSE}

\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl,fontsize=\footnotesize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\footnotesize}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontsize=\footnotesize}
%\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{0pt}}{\vspace{0pt}}

\DefineVerbatimEnvironment{Rcode}{Verbatim}{fontsize=\footnotesize}
\newcommand{\edger}{edgeR}
\newcommand{\pkgname}{csaw}
\newcommand{\code}[1]{{\small\texttt{#1}}}
\newcommand{\R}{\textsf{R}}

\newcommand{\subread}{subread}
\newcommand{\macs}{MACS}
\newcommand{\rtracklayer}{rtracklayer}
\newcommand{\gviz}{Gviz}
\newcommand{\granges}{GenomicRanges}

% Defining a comment box.
\usepackage{framed,color}
\definecolor{shadecolor}{rgb}{0.9, 0.9, 0.9}
\newenvironment{combox}
{ \begin{shaded}\begin{center}\begin{minipage}[t]{0.95\textwidth} }
{ \end{minipage}\end{center}\end{shaded} }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\pkgname{}: ChIP-seq analysis with windows \\ \vspace{0.2in} User's Guide}
\author{Aaron Lun}

% Set to change date for document, not compile date.
\date{First edition 15 August 2012\\
\vspace{6pt}
Last revised 17 March 2015}

% Removing the bibliography title so it shows up in the contents.
\makeatletter
\renewenvironment{thebibliography}[1]{%
%     \section*{\refname}%
%      \@mkboth{\MakeUppercase\refname}{\MakeUppercase\refname}%
      \list{\@biblabel{\@arabic\c@enumiv}}%
           {\settowidth\labelwidth{\@biblabel{#1}}%
            \leftmargin\labelwidth
            \advance\leftmargin\labelsep
            \@openbib@code
            \usecounter{enumiv}%
            \let\p@enumiv\@empty
            \renewcommand\theenumiv{\@arabic\c@enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
      \endlist}
\makeatother

\begin{document}
\pagenumbering{roman}
\maketitle
\tableofcontents

<<results=hide,echo=FALSE>>=
picdir <- "plots-ug"
if (file.exists(picdir)) { unlink(picdir, recursive=TRUE) }
dir.create(picdir)
@

\newpage
\pagenumbering{arabic}

\chapter{Introduction}
\section{Scope}
This document gives an overview of the Bioconductor package \pkgname{} for detecting differential binding (DB) in ChIP-seq experiments.
Specifically, \pkgname{} uses sliding windows to identify significant changes in binding patterns for transcription factors (TFs) or histone marks across different biological conditions \citep{lun2014}.
However, it can also be applied to any sequencing technique where reads represent coverage of enriched genomic regions.
The statistical methods described here are based upon those in the \edger{} package \citep{robinson2010}. 
Knowledge of \edger{} is useful but not a prerequesite for reading this guide.

\section{How to get help}
Most questions about \pkgname{} should be answered by the documentation. 
Every function mentioned in this guide has its own help page. 
For example, a detailed description of the arguments and output of the \code{windowCounts} function can be obtained by typing \code{?windowCounts} or \code{help(windowCounts)} at the \R{} prompt. 
Further detail on the methods or the underlying theory can be found in the references at the bottom of each help page.

The authors of the package always appreciate receiving reports of bugs in the package functions or in the documentation. 
The same goes for well-considered suggestions for improvements. 
Other questions about how to use \pkgname{} are best sent to the Bioconductor support site at \url{https://support.bioconductor.org}.
Please send requests for general assistance and advice to the support site, rather than to the individual authors. 

Users posting to the support site for the first time may find it helpful to read the posting guide at \url{http://www.bioconductor.org/help/support/posting-guide}.

\section{Quick start}
A typical ChIP-seq analysis in \pkgname{} would look something like that described below. 
This assumes that a vector of file paths to sorted and indexed BAM files is provided in \code{bam.files} and a design matrix in supplied in \code{design}.
The code is split across several steps:

<<results=hide,echo=FALSE,label=bamdef>>=
bam.files <- c("es_1.bam", "es_2.bam", "tn_1.bam", "tn_2.bam")
design <- model.matrix(~factor(c('es', 'es', 'tn', 'tn')))
colnames(design) <- c("intercept", "cell.type")
@

\begin{enumerate}
\item Loading in data from BAM files.
<<>>=
require(csaw)
data <- windowCounts(bam.files, ext=110, width=10)
@
\item Filtering out uninteresting regions.
<<>>=
require(edgeR)
keep <- aveLogCPM(asDGEList(data)) >= -1
data <- data[keep,]
@
\item Calculating normalization factors.
<<>>=
binned <- windowCounts(bam.files, bin=TRUE, width=10000)
normfacs <- normalize(binned)
@
\item Identifying DB windows.
<<>>=
y <- asDGEList(data, norm.factors=normfacs)
y <- estimateDisp(y, design)
fit <- glmQLFit(y, design, robust=TRUE)
results <- glmQLFTest(fit)
@
\item Correcting for multiple testing.
<<>>=
merged <- mergeWindows(rowRanges(data), tol=1000L)
tabcom <- combineTests(merged$id, results$table)
@
\end{enumerate}

In this guide, the behaviour of each step will be demonstrated with some publicly available data.
The dataset below focuses on changes in the binding profile of the NFYA protein between embryonic stem cells and terminal neurons \citep{tiwari2012}. 
This will be used as a case study for most of the code examples throughout the guide.

<<>>=
<<bamdef>>
@
\label{data:main}

A comprehensive listing of the datasets used in this guide is provided in Section~\ref{sec:dataset}, along with instructions on how to obtain and process them for entry into the \pkgname{} pipeline.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Converting reads to counts}
\label{chap:count}
\begin{combox}
Hello, reader. 
A little box like this will be present at the start of each chapter. 
It's intended to tell you which objects from previous chapters are needed to get the code in the current chapter to work.  
At this point, all we need are the \code{bam.files} that we defined in the introduction above. 
\end{combox}

\section{Types of input data}
Sorted and indexed BAM (i.e., binary SAM) files are required as input into the read counting functions in \pkgname{}. 
Sorting should be performed on the genomic position of the mapped read.
For a given BAM file named \code{xxx.bam}, the corresponding index file should be named as \code{xxx.bam.bai} such that both files are in the same directory. 
Users should be aware that the sensibility of the supplied index is not checked prior to counting. 
A common mistake is to replace or update the BAM file without updating the index. 
This will cause \pkgname{} to return incorrect results when it attempts to load alignments from new BAM file.

\section{Counting reads into windows}

\subsection{Overview}
The \code{windowCounts} function uses a sliding window approach to count fragments for a set of libraries. 
For single-end data, the fragment corresponding to a read is imputed by directionally extending each read to the average fragment length. 
The number of fragments overlapping a genomic window is counted. 
This is repeated after sliding the window along the genome to a new position. 
A count is then obtained for each window in each library. 

<<>>=
frag.len <- 110
window.width <- 10
data <- windowCounts(bam.files, ext=frag.len, width=window.width)
@

The data is returned as a \code{SummarizedExperiment} object.
The matrix of counts is stored as the first entry in the \code{assays} slot.
Each row corresponds to a genomic window while each column corresponds to a library.
The coordinates of each window are stored in the \code{rowData}.
The total number of reads in each library are stored as \code{totals} in the \code{colData}.

<<>>=
head(assay(data))
head(rowRanges(data))
data$totals
@

For single-end data, suitable values for the average fragment length in \code{ext} can be estimated from the primary peak in a cross-correlation plot (see Section~\ref{sec:ccf}). 
Alternatively, the length can be estimated from diagnostics during ChIP or library preparation, e.g., post-fragmentation gel electrophoresis images. 
Typical values range from 100 to 300 bp, depending on the efficiency of sonication and the use of size selection steps in library preparation.

\begin{center}
\begin{tikzpicture}[font=\sffamily]
% Filling the left region.
\filldraw[fill=black!20] (0,0) rectangle (10,1);
\draw (0,-0.1)--(0,-0.5)--(10,-0.5)--(10,-0.1);
\draw (5,-0.5)--(5,-1);
\node[below] at (5, -1) {width};

% Adding left read.
\draw[line width=3pt,color=red!30] (-2,1.5)--(2.5,1.5);
\draw[line width=3pt,color=red] (-2,1.5)--(-0.5,1.5)--(-0.8,1.8);
\draw (-2,1.8)--(-2,2)--(2.5,2)--(2.5,1.8);
\node[below] at (-2,1.45) {\begin{tabular}{c} forward \\ read \end{tabular}};

% Adding the right read
\draw[line width=3pt,color=blue!30] (12,1.8)--(7.5,1.8);
\draw[line width=3pt,color=blue] (12,1.8)--(10.5,1.8)--(10.8,2.1);
\draw (12,2.1)--(12,2.3)--(7.5,2.3)--(7.5,2.1);
\node[below] at (12,1.75) {\begin{tabular}{c} reverse \\ read \end{tabular}};

\draw (0,2)--(0,3.2)--(3.1,3.2);
\draw (10,2.3)--(10,3.2)--(6.9,3.2);
\node[above] at (5,2.8) {fragment length (ext)};
\end{tikzpicture}
\end{center}

The specified \code{width} of each window controls the compromise between spatial resolution and count size. 
Larger windows will yield higher read counts which can provide more power for DB detection. 
However, spatial resolution is also lost for large windows whereby adjacent features can no longer be distinguished. 
Reads from a DB site may be counted alongside reads from a non-DB site (e.g., non-specific background) or even those from an adjacent site that is DB in the opposite direction. 
This will result in the loss of DB detection power.

The window size can be interpreted as a measure of the width of the binding site. 
Thus, TF analyses will typically use a small window size, e.g., 10 - 20 bp.
This maximizes spatial resolution to allow optimal detection of narrow regions of enrichment. 
For histone marks, widths of at least 150 bp are recommended \citep{humburg2011}. 
This corresponds to the length of DNA wrapped up in each nucleosome, i.e., the smallest relevant unit for histone mark enrichment. 
For diffuse marks, the sizes of enriched regions are more variable and so the compromise between resolution and power is more arbitrary. 
Analyses with multiple widths can be combined to examine DB across several resolutions (see Section~\ref{sec:bin_integrate}).

For TF analyses with small windows, the choice of spacing interval will also have an effect on the choice of window size.
See Section~\ref{sec:efficiency} for more details.

\subsection{Filtering out low-quality reads}
Read extraction from the BAM files is controlled with the \code{param} argument in \code{windowCounts}.
This takes a \code{readParam} object which specifies a number of extraction parameters.
The idea is to define the \code{readParam} object once in the pipeline.
It can then be reused for all relevant functions, to ensure that read loading is consistent throughout the analysis.

<<>>=
default.param <- readParam()
default.param
@

Reads that have been marked as PCR duplicates in the SAM flag can be ignored by setting \code{dedup=TRUE}. 
This can reduce the variability caused by inconsistent duplication between replicates. 
However, it also caps the number of reads at each position. 
This can lead to loss of DB detection power in high abundance regions. 
Spurious differences may also be introduced when the same upper bound is applied to libraries of varying size. 
Thus, duplicate removal is not recommended for routine DB analyses. 
Of course, removal may be unavoidable in some cases, e.g., involving libraries generated from low quantities of DNA.

Reads can also be filtered out based on the minimum mapping score with the \code{minq} argument. 
Low mapping scores are indicative of incorrectly and/or non-uniquely aligned sequences. 
Removal of these reads is highly recommended as it will ensure that only the reliable alignments are supplied to \pkgname{}.
The exact value of the threshold depends on the range of scores provided by the aligner. 
The \subread{} program \citep{liao2013} was used to align the reads in this dataset, so a value of 50 might be appropriate.

An example of read counting with more stringent parameters is shown below.
Comparison to the values in \code{data\$totals} indicates that fewer reads are used in each library.

<<>>=
strict.param <- readParam(minq=50, dedup=TRUE)
demo <- windowCounts(bam.files, ext=frag.len, width=window.width, param=strict.param)
demo$totals
@

\subsection{Avoiding problematic genomic regions}
Read extraction and counting can be restricted to particular chromosomes by specifying the names of the chromosomes of interest in \code{restrict}. 
This avoids the need to count reads on unassigned contigs or uninteresting chromosomes, e.g., the mitochondrial genome for ChIP-seq studies targeting nuclear factors. 
Alternatively, it allows \code{windowCounts} to work on huge datasets or in limited memory by analyzing only one chromosome at a time.

Reads lying in certain regions can also be removed by specifying the coordinates of those regions in \code{discard}. 
This is intended to remove reads that are wholly aligned within known repeat regions but were not removed by the \code{minq} filter. 
Repeats are problematic as changes in repeat copy number or accessibility between conditions can lead to spurious DB. 
Removal of reads within repeat regions can avoid detection of these irrelevant differences. 

<<>>=
repeats <- GRanges("chr1", IRanges(3000001, 3002128))
new.param <- readParam(discard=repeats, restrict=c("chr1", "chr10", "chrX"))
demo <- windowCounts(bam.files, ext=frag.len, width=window.width, param=new.param)
head(rowRanges(demo))
@

Coordinates of annotated repeats can be obtained from several different sources.
A curated blacklist of problematic regions is available from the ENCODE project \citep{dunham2012}, and can be obtained by following this \href{https://sites.google.com/site/anshulkundaje/projects/blacklists}{\textbf{link}}.
This list is constructed empirically from the ENCODE datasets and includes obvious offenders like telomeres, microsatellites and some rDNA genes.
Alternatively, repeats can be predicted from the genome sequence using software like RepeatMasker.
These calls are available from the UCSC website (e.g., for \href{hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/chromOut.tar.gz}{\texttt{mouse}}) or they can be extracted from an appropriate masked \code{BSgenome} object. 

Using \code{discard} is safer than simply ignoring windows that overlap the repeats. 
For example, a large window might contain both repeat regions and non-repeat regions. 
Discarding the window because of the former will compromise detection of DB features in the latter. 
Of course, any DB sites within the discarded regions will be lost from downstream analyses.  
Some caution is therefore required when specifying the regions of disinterest.
For example, many more repeats are called by RepeatMasker than are present in the ENCODE blacklist, so the use of the former may result in loss of potentially interesting features.

\subsection{Additional notes about parameter specification}
Users can modify an existing \code{readParam} object using the \code{reform} method.
The example below copies \code{param} and replaces \code{minq} and \code{discard} with new values.
This is safer than directly modifying the slots, as appropriate type/value checking of each class member is performed.

<<>>=
another.param <- reform(default.param, minq=20, discard=repeats)
another.param
@

The \code{windowCounts} function will also accept a list of \code{readParam} objects. 
Different read extraction parameters can then be specified for each library.
Use of library-specific settings is dangerous as spurious differences can be introduced between libraries.
Nonetheless, it may be unavoidable, e.g., if the dataset is composed of a mixture of single- and paired-end libraries.
A hypothetical example is shown below with one single- and one paired-end library.
To mimic single-end data in the second library, only the first read of the pair is used.

<<>>=
paramlist <- list(readParam(), readParam(pe="first"))
checkList(paramlist)
@

For convenience, the \code{checkList} function is provided to identify those parameters that are different across the list.
This can be useful to ensure that only the intended parameters are varied across libraries.
Lists can also be manipulated with the \code{reformList} function, which sets parameters to their specified values in all elements of the list.
In the code below, coercion of \code{pe} to a common value removes any differences between \code{readParam} objects.

<<>>=
paramlist <- reformList(paramlist, pe="none")
checkList(paramlist)
@

For simplicity, most of the calls to \code{windowCounts} in this guide will use the default settings for \code{param}, i.e., \code{readParam()}.
However, users are encouraged to construct their own \code{readParam} objects (or lists thereof) and apply them consistently throughout their analyses.
A good check for synchronisation is to ensure that the values of \code{...\$totals} are identical between calls. 
This means that the same reads are extracted from the BAM file in each call. 

\subsection{Increasing speed and memory efficiency}
\label{sec:efficiency}
The \code{spacing} parameter controls the distance between adjacent windows in the genome.
By default, this is set to 50 bp, i.e., sliding windows are shifted 50 bp forward at each step.
Using a higher value will reduce computational work as fewer features need to be counted.
This may be useful when machine memory is limited. 
Of course, spatial resolution is lost with larger spacings.
Adjacent positions are not counted and thus cannot be distinguished. 

<<>>=
demo <- windowCounts(bam.files, spacing=100, ext=frag.len, width=window.width)
head(rowRanges(demo))
@

For analyses with large windows, it is worth increasing the \code{spacing} to a fraction of the specified \code{width}. 
This reduces the computational work by decreasing the number of windows and extracted counts. 
Any loss in spatial resolution due to a larger spacing interval is negligble when compared to that already lost by using a large window size. 
Conversely, \code{spacing} should not be larger than \code{ext/2} for analyses with small windows.
This ensures that a narrow binding site will not be overlooked if it falls between two windows.
If \code{ext} is also very small, \code{spacing} should be set to \code{width} to avoid loading too many small windows.

Windows that are overlapped by few fragments can be filtered out using the \code{filter} argument. 
This improves memory efficiency by discarding the majority of low-abundance windows corresponding to uninteresting background regions. 
Specifically, any window is removed if the sum of counts across all libraries is below \code{filter}.
The default value of the filter threshold is 10, though it can be raised to reduce memory usage for large libraries.
More sophisticated filtering is recommended and should be applied later (see Chapter~\ref{chap:filter}).

<<>>=
demo <- windowCounts(bam.files, ext=frag.len, width=window.width, filter=30)
head(assay(demo))
@

Setting \code{bin=TRUE} will cause \code{windowCounts} to count reads into contiguous bins across the genome.
Specifically, \code{spacing} is set to \code{width} and only the $5'$ end of each read is used for counting. 
No filtering is performed such that a count value will be returned for each genomic bin. 
Users should set \code{width} to a reasonably large value, e.g., above 1000 bp.
Otherwise, reads will be counted and reported for every single base in the genome by default.

<<>>=
demo <- windowCounts(bam.files, width=1000, bin=TRUE)
head(rowRanges(demo))
@

\section{Experiments involving paired-end data}
ChIP experiments with paired-end sequencing can be accomodated by setting \code{pe="both"} in the \code{param} object supplied to \code{windowCounts}. 
Read extension is not required as the genomic interval spanned by the originating fragment is explicitly defined between the $5'$ positions of the paired reads.
The number of fragments overlapping each window is then counted as described. 
By default, only proper pairs are used whereby the two reads are on the same chromosome, face inward and are no more than \code{max.frag} apart.

<<>>=
pe.bam <- "example-pet.bam"
pe.param <- readParam(max.frag=400, pe="both")
demo <- windowCounts(pe.bam, param=pe.param)
demo$totals
@
\label{data:pet}

A suitable value for \code{max.frag} can be chosen by examining the distribution of fragment sizes using the \code{getPESizes} function. 
In this example, a user might use a value of around 500 bp as it covers most of the fragment size distribution. 
The plot can also be used to examine the quality of the PE sequencing procedure. 
The location of the peak should be consistent with the fragmentation and size selection steps in library preparation. 

<<label=frag,eval=FALSE>>=
out <- getPESizes(pe.bam)
frag.sizes <- out$sizes[out$sizes<=800]
hist(frag.sizes, breaks=50, xlab="Fragment sizes (bp)", ylab="Frequency", main="")
abline(v=400, col="red")
@

\setkeys{Gin}{width=0.7\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE>>=
<<frag>>
@
\end{center}

The number of fragments exceeding the maximum size can be recorded for quality control. 
The \code{getPESizes} function also returns the number of single reads, pairs with one unmapped read, improperly orientated pairs and inter-chromosomal pairs.
A non-negligble proportion of these reads may be indicative of problems with paired-end alignment or sequencing. 

<<>>=
c(out$diagnostics, too.large=sum(out$sizes > 400))
@

In datasets where many read pairs are invalid, the reads in those pairs can be rescued by setting \code{rescue.ext} to a positive integer.
For each invalid intra-chromosomal read pair, the read with the higher mapping quality score will be directionally extended by \code{rescue.ext} to impute the fragment. 
The other read in the pair is ignored.
For inter-chromosomal read pairs, both reads are extended in this manner. 
Counting will then be performed with these fragments in addition to those from the valid pairs. 
An appropriate value of \code{rescue.ext} can be chosen based on the mode of the fragment size distribution, above.

<<>>=
rescue.param <- reform(pe.param, rescue.ext=200)
demo <- windowCounts(pe.bam, param=rescue.param)
demo$totals
@

Paired-end data can also be treated as single-end data by specifiying \code{pe="first"} or \code{pe="second"} in the \code{readParam} constructor. 
This will only use the first or second read of each read pair, regardless of the validity of the pair or the relative quality of the read alignments.
This setting may be useful for comparing paired-end analyses with single-end analyses, or in truly disastrous situations where paired-end sequencing has failed.

<<>>=
first.param <- readParam(pe="first")
demo <- windowCounts(pe.bam, param=first.param)
demo$totals
@

Note that all of the paired-end methods in \pkgname{} depend on the synchronisation of mate information for each alignment in the BAM file. 
Any file manipulations that might break this synchronisation should be corrected prior to read counting.

\section{Estimating the average fragment length}
\label{sec:ccf}

\subsection{Using cross-correlation plots}
Cross-correlation plots can be generated directly from BAM files using the \code{correlateReads} function. 
This provides a measure of the immunoprecipitation (IP) efficiency of a ChIP-seq experiment \citep{kharchenko2008}. 
Efficient IP should yield a smooth peak at a delay distance corresponding to the average fragment length. 
This reflects the strand-dependent bimodality of reads around narrow regions of enrichment, e.g., TF binding sites. 
The location of the peak can then be used as an estimate of the fragment length for read extension in \code{windowCounts}. 
For this dataset, an estimate of $\sim$110 bp is obtained from the plot below.

<<label=ccf,eval=FALSE>>=
max.delay <- 500
dedup.on <- readParam(dedup=TRUE, minq=50)
x <- correlateReads(bam.files, max.delay, param=dedup.on)
plot(0:max.delay, x, type="l", ylab="CCF", xlab="Delay (bp)")
@

\setkeys{Gin}{width=0.7\textwidth}
\begin{center}
\centering
<<fig=TRUE,echo=FALSE>>=
<<ccf>>
@
\end{center}

A sharp spike may also observed in the plot at a distance corresponding to the read length. 
This is thought to be an artifact, caused by the preference of aligners towards uniquely mapped reads. 
Duplicate removal is typically required here (i.e., set \code{dedup=TRUE} in \code{readParam}) to reduce the size of this spike. 
Otherwise, the fragment length peak will not be visible as a separate entity.
The size of the smooth peak can also be compared to the height of the spike to assess the signal-to-noise ratio of the data \citep{landt2012}. 
Poor IP efficiency will result in a smaller or absent peak as bimodality is less pronounced. 

Cross-correlation plots can also be used for fragment length estimation of narrow histone marks such as histone acetylation and H3K4 methylation.
However, they are less effective for regions of diffuse enrichment where bimodality is not obvious (e.g., H3K27 trimethylation).

<<label=histone_ccf,eval=FALSE>>=
n <- 10000
dedup.on <- readParam(dedup=TRUE)
h3ac <- correlateReads("h3ac.bam", n, param=dedup.on)
h3k27me3 <- correlateReads("h3k27me3.bam", n, param=dedup.on)
h3k4me2 <- correlateReads("h3k4me2.bam", n, param=dedup.on)
plot(0:n, h3ac, col="blue", ylim=c(0, 0.1), xlim=c(0, 1000),
    xlab="Delay (bp)", ylab="CCF", pch=16, type="l", lwd=2)
lines(0:n, h3k27me3, col="red", pch=16, lwd=2)
lines(0:n, h3k4me2, col="forestgreen", pch=16, lwd=2)
legend("topright", col=c("blue", "red", "forestgreen"),
    c("H3Ac", "H3K27me3", "H3K4me2"), pch=16)
@
\label{data:ccf}

\setkeys{Gin}{width=0.7\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE>>=
<<histone_ccf>>
@
\end{center}

\subsection{Variable fragment lengths between libraries}
The \code{windowCounts} function also supports the use of library-specific fragment lengths.
For example, libraries with larger fragment lengths will have wider peaks.
The single-end reads in those peaks will then require more extension, in order to impute a fragment interval that covers the binding site.
This is done by supplying a vector to the \code{ext} argument.
Each entry specifies the average fragment length to be used for the corresponding library.
The extension lengths are also stored in the \code{SummarizedExperiment} output for future reference. 

<<>>=
multi.frag.lens <- c(100, 150, 200, 250)
demo <- windowCounts(bam.files, ext=multi.frag.lens, filter=30)
demo$ext
@

Caution is required to avoid detecting irrelevant DB from differences in peak widths.
Some protection is provided by scaling extended reads to the same length in all libraries.
Consider a bimodal peak across several libraries.
Scaling ensures that the subpeak on the forward strand is centered at the same location in each library.
The same applies for the subpeak on the reverse strand.
This removes most of the differences in width between libraries.
The final fragment length is taken from the \code{final.ext} attribute of \code{multi.frag.lens}.
This can be set with \code{makeExtVector}, and is defined as the mean of the fragment lengths by default.
If \code{final.ext} is not present or is \code{NA}, no rescaling is performed.

<<>>=
scaled.frag.lens <- makeExtVector(multi.frag.lens)
attributes(scaled.frag.lens)$final.ext
@

In general, use of different extension lengths is unnecessary in well-controlled datasets.
Difference in lengths between libraries are usually smaller than 50 bp.
This is less than the inherent variability in fragment lengths within each library (see the histogram for the paired-end data in Section~\ref{data:pet}).
Such variability will affect the read coverage profile more than any difference in lengths, and is likely to mask the latter.
Thus, an \code{ext} vector should only specified for datasets that exhibit large differences in the fragment sizes.

\section{Choosing an appropriate window size}
The coverage profile around potential binding sites can be obtained with the \code{profileSites} function.
Here, the binding sites are defined by taking high-abundance 50 bp windows and identifying those that are locally maximal using \code{findMaxima}.
For each selected window, \code{profileSites} records the coverage across the flanking regions as a function of the distance from the edge of the window.
This is divided by the count for the window itself to obtain a relative coverage, based on the specification of \code{weight}.
The values are then averaged across all windows to obtain an aggregated coverage profile for each library.

<<label=profiling,eval=FALSE>>=
all.bam <- c("h3ac.bam", "h3k4me2.bam", "es_1.bam")
collected <- list()
for (curbam in all.bam) {
    windowed <- windowCounts(curbam, spacing=50, width=50, param=dedup.on, filter=20)
    rwsms <- rowSums(assay(windowed))
    maxed <- findMaxima(rowRanges(windowed), range=1000, metric=rwsms)
    collected[[curbam]] <- profileSites(curbam, rowRanges(windowed)[maxed], 
        param=dedup.on, weight=1/rwsms[maxed])
}
xranged <- as.integer(names(collected[[1]]))
plot(xranged, collected[[1]], type="l", col="blue", xlim=c(-1000, 1000), lwd=2, 
    xlab="Distance (bp)", ylab="Relative coverage per base")
lines(xranged, collected[[2]], col="forestgreen", lwd=2)
lines(xranged, collected[[3]], col=rgb(0,0,0,0.5), lwd=2)
legend("topright", col=c("blue", "forestgreen", rgb(0,0,0,0.5)),
    c("H3Ac", "H3K4me2", "NF-YA"), pch=16)
abline(v=c(-150,200), col="dodgerblue", lty=2)
@

\setkeys{Gin}{width=0.7\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE>>=
<<profiling>>
@
\end{center}

In the example above, enrichment for the two histone marks is mostly contained within a 350 bp region around the maxima (dashed lines).
This suggests that a window size of $\sim$150 bp is ideal, given that directional extension by 100 bp has been performed on both sides of the peak.
Most of the coverage can then be captured without including too much background noise.
In contrast, the NF-YA profile drops off more sharply.
This indicates that a smaller window size ($< 50$ bp) is probably adequate, consistent with sharp TF binding.
See the \code{wwhm} function for selection of a window width from the coverage profile.

In practice, a clear-cut choice of distance/window size is rarely found in real datasets.
For many non-TF targets, the widths of the enriched regions can be highly variable.
This manifests as a long tail in the coverage profile plot and suggests that no single window size is optimal.
Indeed, even if all enriched regions were of constant width, the width of the DB events occurring within those regions may be variable.
Thus, it may be preferable to err on the side of smaller windows to maintain spatial resolution for such events.

The performance of this approach also deteriorates when background enrichment is prevalent.
This makes it difficult to determine when the coverage becomes negligble.
The problem is compounded for diffuse marks where the transition between enrichment and background is unclear.
Indeed, local maxima are unlikely to be well-defined within diffuse regions.
Thus, users are advised to exercise caution when picking a window size from these plots.

\section{Miscellaneous functions for non-standard counting}

\subsection{Counting over manually specified regions}
The \pkgname{} package focuses on counting reads into windows. 
However, it may be occasionally desirable to use the same conventions (e.g., duplicate removal, quality score filtering) when counting reads into pre-specified regions. 
This can be performed with the \code{regionCounts} function, which is largely a wrapper for \code{countOverlaps} from the \granges{} package.

<<>>=
my.regions <- GRanges(c("chr11", "chr12", "chr15"),
    IRanges(c(75461351, 95943801, 21656501), 
    c(75461610, 95944810, 21657610)))
reg.counts <- regionCounts(bam.files, my.regions, ext=frag.len, param=strict.param)
head(assay(reg.counts))
@ 

\subsection{Strand-specific counting}
Techniques like CLIP-seq, MeDIP-seq or CAGE provide strand-specific sequence information.
The \pkgname{} package can analyze these datasets through strand-specific counting.
This can be done manually setting the \code{forward} slot in the \code{readParam} object to \code{TRUE} or \code{FALSE}, to count only forward- or reverse-strand reads respectively.
Alternatively, the \code{strandedCounts} wrapper function can be used to obtain strand-specific counts for each window or region.
The strand of the output regions indicates the strand on which reads were counted for that row.
Up to two rows can be generated for each window or region, depending on filtering.

<<>>=
ss.param <- reform(default.param, forward=NULL)
ss.counts <- strandedCounts(bam.files, ext=frag.len, width=window.width, param=ss.param)
strand(rowRanges(ss.counts))
@

Note that \code{strandedCounts} operates internally by calling \code{windowCounts} (or \code{regionCounts}) twice with different settings for \code{param\$forward}. 
Any value for \code{forward} in the input \code{param} object will be ignored.
In fact, the function will \textit{only} accept a \code{NULL} value for this slot.
This is intended to protect the user, as any attempt to re-use the \code{ss.param} object in functions that are not designed for strand specificity will (correctly) raise an error.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Filtering prior to correction}
\label{chap:filter}

\begin{combox}
This chapter will require \code{frag.len} and \code{data} defined in Chapter~\ref{chap:count}.
We will also need the \code{normfacs} vector from Chapter~\ref{chap:norm}. 
%Oh, and the \code{me.demo} list as well, just for a demonstration at the end of this chapter.
Finally, we'll need the \code{aveLogCPM} function from the \edger{} package (that we've already loaded, unless you skipped the last chapter).
\end{combox}

\section{Independent filtering for count data}
Many of the low abundance windows in the genome correspond to background regions in which DB is not expected. 
Indeed, windows with low counts will not provide enough evidence against the null hypothesis to obtain sufficiently low $p$-values for DB detection. 
Similarly, some approximations used in the statistical analysis will fail at low counts. 
Removing such uninteresting or ineffective tests reduces the severity of the multiple testing correction, increases detection power amongst the remaining tests and reduces computational work.

Filtering is valid so long as it is independent of the test statistic under the null hypothesis \citep{bourgon2010}. 
In the negative binomial (NB) framework, this (probably) corresponds to filtering on the overall NB mean. 
The DB $p$-values retained after filtering on the overall mean should be uniform under the null hypothesis, by analogy to the normal case. 
Row sums can also be used for datasets where the effective library sizes are not very different, or where the counts are assumed to be Poisson-distributed between biological replicates. 

In \edger{}, the log-transformed overall NB mean is referred to as the average abundance.
This is computed with the \code{aveLogCPM} function, as shown below for each region.

<<>>=
abundances <- aveLogCPM(asDGEList(data))
summary(abundances)
@

For demonstration purposes, an arbitrary threshold of -1 is used here to filter the window abundances. 
This restricts the analysis to windows with abundances above this threshold.
While filtering can be performed at any stage of the analysis prior to the multiple testing correction, doing so at earlier steps is recommended to reduce computational work. 
Estimates of downstream statistics are also more relevant when they are based on the windows of interest. 
That said, one should retain enough points for information sharing in Chapter~\ref{chap:stats}.

<<>>=
keep <- abundances > -1
filtered.data <- data[keep,]
summary(keep)
@

The exact choice of filter threshold may not be obvious.
In particular, there is often no clear distinction in abundances between genuine binding and background events, e.g., due to the presence of many weak but genuine binding sites.
A threshold that is too small will be ineffective, whereas a threshold that is too large may decrease power by removing true DB sites.
Arbitrariness is unavoidable when balancing these opposing considerations.

Nonetheless, several strategies for defining the threshold are described below.
Users should start by choosing \textbf{one} of these filtering approaches for their analyses.
Different filters can also be combined in more advanced applications, e.g., by running \code{data[keep1 \& keep2,]} for filter vectors \code{keep1} and \code{keep2}.
However, any benefit will depend on the type of filters involved.
The greatest effect is observed for filters that operate on different principles.

\section{By proportion}
One approach is to to assume that only a certain proportion - say, 0.1\% - of the genome is genuinely bound. 
This corresponds to the top proportion of high-abundance windows.
The total number of windows is calculated from the genome length and the \code{spacing} interval used in \code{windowCounts}. 
The \code{filterWindows} function returns the ratio of the rank of each window to this total, where higher-abundance windows have larger ranks.
Users can then retain those windows with rank ratios above the unbound proportion of the genome.

<<>>=
keep <- filterWindows(data, type="proportion")$filter > 0.999
sum(keep)
@

This approach is simple and has the practical advantage of maintaining a constant number of windows for the downstream analysis. 
However, it may not adapt well to different datasets where the proportion of bound sites can vary.
Using an inappropriate percentage of binding sites will result in the loss of potential DB regions or inclusion of background regions.

\section{By global enrichment}
\label{sec:global_filter}
An alternative approach involves choosing a filter threshold based on the fold change over the level of non-specific enrichment.
The degree of background enrichment can be estimated by counting reads into large bins across the genome.
Binning is necessary here to increase the size of the counts when examining low-density background regions. 
This ensures that precision is maintained when estimating the background abundance.

<<>>=
bin.size <- 2000L
binned <- windowCounts(bam.files, bin=TRUE, width=bin.size)
@

The median of the average abundances across all bins can be computed and used as a global estimate of the background coverage.
This global background can then be compared to the window-based abundances.
However, some care is required as the sizes of the regions used for read counting are different between bins and windows.
The average abundance of each bin must be scaled down to be comparable to those of the windows.

With \code{type="global"}, the \code{filterWindows} function returns the increase in the abundance of each window over the global background.
Windows can be filtered by setting some minimum threshold on this increase.
Here, a fold change of 3 is necessary for a window to be considered as containing a binding site. 
This approach has an intuitive and experimentally relevant interpretation that adapts to the level of non-specific enrichment in the dataset. 

<<>>=
filter.stat <- filterWindows(data, background=binned, type="global")
keep <- filter.stat$filter > log2(3)
sum(keep)
@

The effect of filtering can also be visualized with a histogram. 
This allows users to confirm that the bulk of (assumed) background bins are discarded upon filtering. 
Note that bins containing genuine binding sites will usually not be visible on such plots.
This is due to the dominance of the background-containing bins throughout the genome.

<<eval=FALSE,label=binhist>>=
hist(filter.stat$back.abundances, xlab="Adjusted bin log-CPM", breaks=100, main="", 
    xlim=c(min(filter.stat$back.abundances), 0))
global.bg <- filter.stat$abundances - filter.stat$filter
abline(v=global.bg[1], col="red")
abline(v=global.bg[1]+log2(3), col="blue")
@

\setkeys{Gin}{width=0.7\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE>>=
<<binhist>>
@
\end{center}

Of course, the pre-specified minimum fold change may be too aggressive when binding is weak. 
For TF data, a large cut-off works well as narrow binding sites will have high read densities and are unlikely to be lost during filtering. 
Smaller minimum fold changes are recommended for diffuse marks where the difference from background is less obvious. 

\section{By local enrichment}

\subsection{Mimicking single-sample peak callers}
Local background estimators can also be constructed.
This avoids inappropriate filtering when there are differences in background coverage across the genome. 
Here, the 2 kbp region surrounding each window will be used as the ``neighbourhood'' over which a local estimate of non-specific enrichment for that window can be obtained. 
The counts for this region can be obtained with the aptly-named \code{regionCounts} function.
This should be synchronized with \code{windowCounts} by using the same \code{param}, if any non-default settings were used.

<<>>=
surrounds <- 2000
neighbour <- suppressWarnings(resize(rowRanges(data), surrounds, fix="center"))
wider <- regionCounts(bam.files, regions=neighbour, ext=frag.len)
@

Counts for each window are subtracted from the counts for its neighbourhood. 
This ensures that any enriched regions or binding sites inside the window will not interfere with estimation of its local background. 
The width of the window is also subtracted to reflect the effective size of the neighbourhood.
Again, the abundance of the neighbourhood is scaled down for a valid comparison to that of the corresponding window.
This work is done by setting \code{type="local"} for \code{filterWindows}, which returns the enrichment values i.e., the increase in the abundance of each window over its neighbourhood.

<<>>=
filter.stat <- filterWindows(data, wider, type="local")
@

Filtering can then be performed using a quantile- or fold change-based threshold on the enrichment values. 
In this scenario, a 3-fold increase in enrichment over the neighbourhood abundance is required for retention of each window.
This roughly mimics the behaviour of single-sample peak-calling programs such as \macs{} \citep{zhang2008}. 

<<>>=
keep <- filter.stat$filter > log2(3)
sum(keep)
@

Note that this procedure also assumes that no other enriched regions are present in each neighbourhood. 
Otherwise, the local background will be overestimated and windows may be incorrectly filtered out. 
This may be problematic for diffuse histone marks or TFBS clusters where enrichment may be observed in both the window and its neighbourhood.

If this seems too complicated, an alternative is to identify locally enriched regions using peak-callers like \macs{}. 
Filtering can then be performed to retain only windows within called peaks.  
However, peak calling must be done independently of the DB status of each window. 
If libraries are of similar size or biological variability is low, reads can be pooled into one library for single-sample peak calling \citep{lun2014}. 
This is equivalent to filtering on the average count and avoids loss of the type I error control from data snooping.

\subsection{Identifying local maxima}
Another approach uses the \code{findMaxima} function to identify local maxima in the read density across the genome.
The code below will determine if each window is a local maximum, i.e., whether it has the highest average abundance within 1 kbp on either side.
The data can then be filtered to retain only these locally maximal windows.
This can also be combined with other filters to ensure that the retained windows have high absolute abundance.

<<>>=
maxed <- findMaxima(rowRanges(data), range=1000, metric=abundances)
summary(maxed)
@

This approach is very aggressive and should only be used (sparingly) in datasets where binding is sharp, simple and isolated.
Complex binding events involving diffuse enrichment or adjacent binding sites will not be handled well.
For example, DB detection will fail if a low-abundance DB window is ignored in favour of a high-abundance non-DB neighbour.

\subsection{With negative controls}
Negative controls for ChIP-seq refer to input or IgG libraries where the IP step has been skipped or compromised with an irrelevant antibody, respectively. 
This accounts for sequencing/mapping biases in ChIP-seq data. 
IgG controls also quantify the amount of non-specific enrichment throughout the genome. 
These controls are mostly irrelevant when testing for DB between ChIP samples. 
However, they can be used to filter out windows where the average abundance across the ChIP samples is below the abundance of the control. 

<<>>=
in.demo <- windowCounts(c(bam.files, "IgG.bam"), ext=frag.len)
chip <- in.demo[,1:4]
control <- in.demo[,5]
@

The \code{filterWindows} function computes the enrichment of the ChIP counts over the control counts for each window.
A larger \code{prior.count} of 5 is used to compute the average abundance.
This protects against inflated log-fold changes when the count for the window in the control sample is near zero.
Note that the global and local background estimates require less protection (\code{prior.count} of 2) as they are derived from  larger bins with more counts.
The example below requires a 3-fold or greater increase over the control to retain the window.

<<>>=
filter.stat <- filterWindows(chip, control, type="control", prior.count=5)
keep <- filter.stat$filter > log2(3)
@

% If you have a large bin, you get a precise estimate of the background rate, so you don't need a prior when computing the log-fold increase.
% You'll still be susceptible when the background is completely empty; but in that case, there's probably nothing there, so the true fold change may actually be ~Inf.
% If you have a small bin, the number of background reads fluctuates by sampling, so you need a large prior to avoid selecting for regions with randomly low background.
% This is actually the case here; if you look for things that are kept with prior.count=2, you'll find a lot of things with low counts in the ChIP sample.

The \pkgname{} pipeline can also be applied to search for ``DB'' between ChIP libraries and control libraries. 
The ChIP and control libraries can be treated as separate groups, in which most ``DB'' events are expected to be enriched in the ChIP samples. 
If this is the case, the filtering procedure described above is inappropriate as it will select for windows with differences between ChIP and control samples. 
This compromises the assumption of the null hypothesis during testing, resulting in loss of type I error control.

% This is the best way to deal with GC biases, as you can just grab any old input off the web and use it (citation?).
% Alternatively, you could estimate the expected background from a GC content vs. abundance curve on the ChIP data, but that's susceptible to inflation from genuine binding.

\section{By prior information}
When only a subset of genomic regions are of interest, DB detection power can be improved by removing windows lying outside of these regions. 
Such regions could include promoters, enhancers, gene bodies or exons. 
Alternatively, sites could be defined from a previous experiment or based on the genome sequence, e.g., TF motif matches.
The example below retrieves the coordinates of the broad gene bodies from the mouse genome, including the 3 kbp region upstream of the TSS that represents the putative promoter region for each gene. 

<<>>=
require(TxDb.Mmusculus.UCSC.mm10.knownGene)
broads <- genes(TxDb.Mmusculus.UCSC.mm10.knownGene)
broads <- resize(broads, width(broads)+3000, fix="end")
head(broads)
@
 
Windows can be filtered to only retain those which overlap with the regions of interest. 
Discerning users may wish to distinguish between full and partial overlaps, though this should not be a significant issue for small windows.
This could also be combined with abundance filtering to retain windows that contain putative binding sites in the regions of interest.

<<>>=
suppressWarnings(keep <- overlapsAny(rowRanges(data), broads))
sum(keep)
@

Any information used here should be independent of the DB status under the null in the current dataset. 
For example, DB calls from a separate dataset and/or independent annotation can be used without problems. 
However, using DB calls from the same dataset to filter regions would violate the null assumption and compromise type I error control.

\section{Some additional comments about filtering}
It should be stressed that these filtering strategies do not eliminate subjectivity.
Some thought is still required in selecting an appropriate proportion of bound sites or minimum fold change above background for each method.
Rather, these filters provide a relevant interpretation for what would otherwise be an arbitrary threshold on the abundance.

As a general rule, users should filter less aggressively if there is any uncertainty about the features of interest.
In particular, the thresholds shown in this chapter for each filtering statistic are fairly mild.
This ensures that more potential DB windows are retained for testing.
Of course, if no such DB windows exist, then the greater number of tests will result in loss of power for the DB windows that would be retained by an aggressive filter.
This is a necessary cost, as there is no way to know the DB status without actually testing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Calculating normalization factors}
\label{chap:norm}
\begin{combox}
This next chapter will need the \code{bam.files} vector again. 
You'll notice that that a number of other BAM files are used in this chapter. 
However, these are just present for demonstration purposes and aren't necessary for the main NFYA example.
\end{combox}

\section{Overview}
The complexity of the ChIP-seq technique gives rise to a number of different biases in the data.
For a DB analysis, library-specific biases are of particular interest as they can introduce spurious differences between conditions.
This includes composition biases, efficiency biases and trended biases.
Thus, normalization between libraries is required to remove these biases prior to any statistical analysis.
Several normalization strategies are presented here, though users should only pick \textbf{one} to use for any given analysis.
Advice on choosing the most appropriate method is scattered throughout the chapter, so read carefully.

\section{Eliminating composition biases}

\subsection{Using the TMM method on binned counts}
\label{sec:compo_norm}
As the name suggests, composition biases are formed when there are differences in the composition of sequences across libraries. 
Highly enriched regions consume more sequencing resources and thereby suppress the representation of other regions. 
Differences in the magnitude of suppression between libraries can lead to spurious DB calls. 
Scaling by library size fails to correct for this as composition biases can still occur in libraries of the same size. 

To remove composition biases in \pkgname{}, reads are counted in large bins and the counts are used for normalization with the \code{normalize} wrapper function. 
This uses the trimmed mean of M-values (TMM) method \citep{oshlack2010} to correct for any sytematic fold change in the coverage of the bins. 
The assumption here is that most bins represent non-DB background regions so any consistent difference across bins must be spurious.

<<>>=
binned <- windowCounts(bam.files, bin=TRUE, width=10000)
normfacs <- normalize(binned)
normfacs
@

The TMM method trims away putative DB bins (i.e., those with extreme M-values) and computes normalization factors from the remainder to use in \edger{}. 
The size of each library is scaled by the corresponding factor to obtain an effective library size for modelling. 
A larger normalization factor results in a larger effective library size and is conceptually equivalent to scaling each individual count downwards, given that the ratio of that count to the (effective) library size will be smaller. 
Check out the \edger{} user's guide for more information.

Note that the \code{normalize} method skips the precision weighting step in the TMM method.
Weighting aims to increase the contribution of bins with high counts.
However, these bins are more likely to contain binding sites and thus are more likely to be DB. 
If any DB regions should survive trimming, upweighting them would be counterproductive. 

% By default, the top 5% of most abundant elements are already removed by TMM.
% You can ask for more removal, which could help; but in general, binding sites
% are so negligble in quantity compared to the background regions, it doesn't
% really matter too much, so long as weighting isn't in play.

\subsection{Choosing a bin size}
By definition, read coverage is low for background regions. 
This can result in a large number of zero counts and undefined M-values when reads are counted into small windows. 
Adding a prior count is only a superficial solution as the chosen prior will have undue influence on the estimate of the normalization factor when many counts are low. 
The variance of the fold change distribution is also higher for low counts. 
This reduces the effectiveness of the trimming procedure during normalization. 
These problems can be overcome by using large bins to increase the size of the counts prior to TMM normalization. 

Of course, this strategy requires the user to supply a bin size. 
If the bins are too large, background and enriched regions will be included in the same bin. 
This makes it difficult to trim away bins corresponding to enriched regions.
On the other hand, the counts will be too low if the bins are too small.
Testing multiple bin sizes is recommended to ensure that the estimates are robust to any changes. 
A value of 10000 bp is suitable for most datasets.

<<>>=
demo <- windowCounts(bam.files, bin=TRUE, width=5000)
normalize(demo)
demo <- windowCounts(bam.files, bin=TRUE, width=15000)
normalize(demo)
@

These factors are consistently close to unity, which suggests that composition bias is negligble in this dataset.
See Section~\ref{sec:eff_norm_ma} for some examples with greater bias.

\subsection{Visualizing normalization efforts with MA plots}
The effectiveness of normalization can be examined using a MA plot. 
A single main cloud of points should be present, consisting primarily of background regions.
Separation into multiple discrete points indicates that the counts are too low and that larger bin sizes should be used. 
Composition biases manifest as a vertical shift in the position of this cloud. 
Ideally, the log-ratios of the corresponding normalization factors should correspond to the centre of the cloud. 
This indicates that undersampling has been identified and corrected.

% Genuine binding sites are mixed in with the background in this example, as
% there's no clear distinction between the two. As one might expect, the bins
% containing binding sites tend to be those with higher A-values. Most of the
% bins should still be free of binding, though (8000/260000 bins, based on 
% those that overlap globally filtered windows in Chapter 4).

<<eval=FALSE,label=normplot>>=
par(mfrow=c(1, 3), mar=c(5, 4, 2, 1.5))
adj.counts <- cpm(asDGEList(binned), log=TRUE)
for (i in 1:(length(bam.files)-1)) {
    cur.x <- adj.counts[,1]
    cur.y <- adj.counts[,1+i]
    smoothScatter(x=(cur.x+cur.y)/2+6*log2(10), y=cur.x-cur.y,
        xlab="A", ylab="M", main=paste("1 vs", i+1))
    all.dist <- diff(log2(normfacs[c(i+1, 1)]))
    abline(h=all.dist, col="red")
}
@

\setkeys{Gin}{width=0.98\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE,width=8,height=3>>=
<<normplot>>
@
\end{center}

\section{Eliminating efficiency biases}
\label{sec:eff_norm}

\subsection{Using the TMM method on high-abundance regions}
Efficiency biases are commonly observed in ChIP-seq data. 
This refers to fold changes in enrichment that are introduced by variability in IP efficiencies between libraries. 
These technical differences are of no biological interest and must be removed. 
This can be achieved by assuming that high-abundance windows or bins contain binding sites. 
In the examples below, reads are counted into 150 bp windows for histone mark data.
High-abundance windows are chosen using a global filtering approach described in Section~\ref{sec:global_filter}. 
The TMM method can then be applied to eliminate systematic differences across those windows.

<<>>=
me.files <- c("h3k4me3_mat.bam", "h3k4me3_pro.bam")
me.demo <- windowCounts(me.files, width=150)
me.bin <- windowCounts(me.files, bin=TRUE, width=10000) 
keep <- filterWindows(me.demo, me.bin, type="global")$filter > log2(3)
me.eff <- normalize(me.demo[keep,])
me.eff
ac.files <- c("h3ac.bam", "h3ac_2.bam")
ac.demo <- windowCounts(ac.files, width=150)
ac.bin <- windowCounts(ac.files, bin=TRUE, width=10000)
keep <- filterWindows(ac.demo, ac.bin, type="global")$filter > log2(5)
ac.eff <- normalize(ac.demo[keep,])
ac.eff
@
\label{data:norm}

This method assumes that most high-abundance bins are not DB. 
Any systematic changes must be caused by differences in IP efficiency or some other technical issue. 
However, genuine biological differences may be removed when the assumption of a non-DB majority does not hold, e.g., overall binding is truly lower in one condition. 
Also, some care is required when choosing the top percentage of bins or windows to use for normalization.
Using too many bins (or windows) will include background regions, while using too few will result in unstable estimates.
See Chapter~\ref{chap:filter} for more details on filtering to select enriched regions.

Note that window counts can be used directly as they should be large enough for fold change-based normalization.
Indeed, users should perform normalization \textit{after} any necessary filtering on the windows (see Chapter~\ref{chap:filter}).
This ensures that there is no systematic difference between libraries across these windows during DB testing.
In practice, the estimated normalization factors are usually robust to the choice of window/bin size. 

\subsection{Checking normalization with MA plots}
\label{sec:eff_norm_ma}

The effect of normalization can be visualized with MA plots. 
Plots are constructed using counts for 10 kbp bins, rather than with those from the windows.
This is useful as the behaviour of the entire genome can be examined, rather than just that of the high-abundance windows.
It also allows calculation of and comparison to the factors for composition bias.

<<>>=
me.comp <- normalize(me.bin)
me.comp
ac.comp <- normalize(ac.bin)
ac.comp
@

The clouds at low and high A-values represent the background and bound regions, respectively.
The normalization factors from removal of composition bias (dashed) pass through the former, whereas the factors to remove efficiency bias (full) pass through the latter.
A non-zero M-value location for the high A-value cloud represents a systematic difference between libraries for the bound regions, either due to genuine DB or variable IP efficiency. 
This also induces composition bias, leading to a non-zero M-value for the background cloud.

<<eval=FALSE,label=methplot,strip.white=false>>=
par(mfrow=c(1,2))
for (main in c("H3K4me3", "H3ac")) { 
    if (main=="H3K4me3") { bins <- me.bin; comp <- me.comp; eff <- me.eff }
    else { bins <- ac.bin; comp <- ac.comp; eff <- ac.eff }
    adjc <- cpm(asDGEList(bins), log=TRUE)
    smoothScatter(x=rowMeans(adjc), y=adjc[,1]-adjc[,2], 
        xlab="A", ylab="M", main=main)
    abline(h=log2(eff[1]/eff[2]), col="red")
    abline(h=log2(comp[1]/comp[2]), col="red", lty=2)
}
@

\setkeys{Gin}{width=0.98\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE,width=10,height=5>>=
<<methplot>>
@
\end{center}

These two normalization strategies are mutually exclusive, as only one set of factors will ultimately be used.
The choice between the two methods depends on whether one assumes that the systematic differences at high abundances represent genuine DB events.
If so, composition biases should be removed to preserve the assumed DB.
Otherwise, the differences must represent efficiency biases and should be removed.
Some understanding of the biological context is useful in making this decision, e.g., comparing a wild-type against a knock-out for the target protein should result in systematic DB, while overall levels of histone marking are generally expected to be consistent in most conditions.

\section{Dealing with trended biases}
In more extreme cases, the bias may vary with the average abundance to form a trend. 
One possible explanation is that changes in IP efficiency will have little effect at low-abundance background regions and more effect at high-abundance binding sites. 
Thus, the magnitude of the bias between libraries will change with abundance. 
The trend cannot be corrected with scaling methods as no single scaling factor will remove differences at all abundances.
Rather, non-linear methods are required, such as cyclic loess or quantile normalization.

One such implementation is provided in \code{normalize} by setting \code{type="loess"}. 
This is based on the fast loess algorithm \citep{ballman2004} with minor adaptations to handle low counts. 
A matrix is produced that contains an offset term for each bin/window in each library.
This offset matrix can then be directly used in \edger{}, assuming that the bins or windows used in normalization are also the ones to be tested for DB.
Thus, any filtering that needs to be done (see Chapter~\ref{chap:filter}) should be carried out \textit{before} this normalization step.

The example below operates on the filtered counts for 2000 bp windows.
This window size is chosen purely for aesthetics in this demonstration, as the trend is less obvious at smaller widths.
Obviously, users should pick a more appropriate value for their analysis.

<<>>=
ac.demo2 <- windowCounts(c("h3ac.bam", "h3ac_2.bam"), width=2000L)
filtered <- filterWindows(ac.demo2, ac.bin, type="global")
keep <- filtered$filter > log2(4)
ac.demo2 <- ac.demo2[keep,]
ac.off <- normalize(ac.demo2, type="loess")
head(ac.off)
@

MA plots can be examined to determine whether normalization was successful.
Any abundance-dependent trend in the M-values should be eliminated. 
Filtering is strongly recommended to remove low-abundance regions where loess curve fitting is inaccurate. 
When doing so, the filter statistic should be based on the average abundance from \code{aveLogCPM}.
An average abundance threshold will act as a clean vertical cutoff in the plots below. 
Spurious trends that might affect normalization will not be introduced at the filter boundary. 

<<eval=FALSE,keep.source=true,label=trendplot>>=
par(mfrow=c(1,2))
abval <- filtered$abundances[keep]
o <- order(abval)
adjc <- cpm(asDGEList(ac.demo2), log=TRUE)
mval <- adjc[,1]-adjc[,2]
fit <- loessFit(x=abval, y=mval)
smoothScatter(abval, mval, ylab="M", xlab="Average logCPM", 
   main="Raw", ylim=c(-2,2), xlim=c(0, 7))
lines(abval[o], fit$fitted[o], col="red")
#
## Repeating after normalization.
re.adjc <- log2(assay(ac.demo2)+0.5) - ac.off/log(2)
mval <- re.adjc[,1]-re.adjc[,2]
fit <- loessFit(x=abval, y=mval)
smoothScatter(abval, re.adjc[,1]-re.adjc[,2], ylab="M", 
   xlab="Average logCPM", main="Normalized", ylim=c(-2,2), xlim=c(0, 7))
lines(abval[o], fit$fitted[o], col="red")
@

\setkeys{Gin}{width=0.98\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE,width=10,height=5>>=
<<trendplot>>
@
\end{center}

Note that all non-linear methods assume that most bins/windows are not DB at each abundance. 
This is a stronger assumption than that for scaling methods, which only require a non-DB majority across all features.
Removal of the trend may not be appropriate if it represents some genuine biological phenomenon, e.g., involving changes in overall binding. 
In addition, the computed offsets are not compatible with the normalization factors from the scaling methods.
Only one of these sets of values will ultimately be used by \edger{}.

% I could iterate here, whereby I recompute the average abundance using the offsets.
% Now, I'm not sure whether this would be any better or worse. 
% On a practical level, though, the aveLogCPM calls in later functions do not use offset data.
% So, you want to make sure you're removing the trend with respect to the average abundance that'll be used later on.
% That's why I don't bother to iterate.

\section{A word on other biases}
No normalization is performed to adjust for differences in mappability or sequencability between different regions of the genome. 
Region-specific biases are assumed to be constant between libraries. 
This is generally reasonable as the biases depend on fixed properties of the genome sequence such as GC content. 
Thus, biases should cancel out during DB comparisons.
Any variability between samples will just be absorbed into the dispersion estimate. 

That said, explicit normalization to correct these biases can improve results for some datasets.
Procedures like GC correction could decrease the observed variability by removing systematic differences between replicates. 
Of course, this also assumes that the targeted differences have no biological relevance.
Detection power may be lost if this is not true. 
For example, differences in the GC content distribution can be driven by technical bias as well as biology, e.g., when protein binding is associated with a specific GC composition.

\section{Examining replicate similarity with MDS plots}
On a semi-related note, the binned counts can be used to examine the similarity of replicates through multi-dimensional scaling (MDS) plots. 
The distance between each pair of libraries is computed as the square root of the mean squared log-fold change across the top set of bins with the highest absolute log-fold changes.
A small top set visualizes the most extreme differences whereas a large set visualizes overall differences.
Again, counting with large bins is recommended as fold changes will be undefined in the presence of zero counts.

<<label=mds,eval=false>>=
par(mfrow=c(2,2), mar=c(5,4,2,2))
binned <- windowCounts(bam.files, bin=TRUE, width=2000L)
adj.counts <- cpm(asDGEList(binned), log=TRUE)
for (top in c(100, 500, 1000, 5000)) {
    out <- plotMDS(adj.counts, main=top, col=c("blue", "blue", "red", "red"), 
        labels=c("es", "es", "tn", "tn"), top=top)
}
@

\setkeys{Gin}{width=0.8\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE>>=
<<mds>>
@
\end{center}

Replicates from different groups should form separate clusters in the plot.
This indicates that the results are reproducible and that the effect sizes are large.
Mixing between replicates of different conditions indicates that the biological difference has no effect on protein binding, or that the data is too variable for any effect to manifest.
Any outliers should also be noted as their presence may confound the downstream analysis.
In the worst case, the removal of the corresponding libraries may be necessary to obtain sensible results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Testing for differential binding}
\label{chap:stats}

\begin{combox}
For this next section, we'll be needing the \code{data} list that was generated in Chapter~\ref{chap:count} and filtered in Chapter~\ref{chap:filter}. 
Just let me assign the filtered list back to \code{data}, because I put it in the dummy variable \code{demo} in the previous chapter:

<<>>=
original <- data
data <- filtered.data
@

You'll also need the \code{normfacs} vector from Chapter~\ref{chap:norm}, as well as the \code{design} matrix from the introduction. 
Finally - it should be obvious, but several \edger{} functions will be needed here.
Make sure the \edger{} package is loaded if you've been skipping chapters.
\end{combox}

\section{Introduction to \edger{}}

\subsection{Overview}
Low counts per window are typically observed in ChIP-seq datasets, even for genuine binding sites. 
Any statistical analysis to identify DB sites must be able to handle discreteness in the data. 
Count-based models are ideal for this purpose. 
In this guide, the quasi-likelihood (QL) framework in the \edger{} package is used \citep{lund2012}. 
Counts are modelled using NB distributions that account for overdispersion between biological replicates \citep{robinson2008}. 
Each window can then be tested for significant DB between conditions.

Of course, any statistical method can be used if it is able to accept a count matrix and a vector of normalization factors (or more generally, a matrix of offsets). 
The choice of \edger{} is primarily motivated by its performance relative to some published alternatives \citep{law2014}.
This author's desire to increase his h-index may also be a factor \citep{chen2014}.

\subsection{Setting up the data}
A \code{DGEList} object is first formed from the count matrix, library sizes and normalization factors.
Here, the \code{normfacs} vector from TMM normalization of background bins is used. 
If an offset matrix is necessary (e.g., from non-linear normalization), this can be assigned into \code{y\$offset} for later use in the various \edger{} functions.

<<>>=
y <- asDGEList(data, norm.factors=normfacs)
@

The experimental design is described by a design matrix. 
In this case, the only relevant factor is the cell type of each sample. 
A generalized linear model (GLM) will be fitted to the counts for each window using the specified design matrix \citep{mccarthy2012}. 
This provides a general framework for the analysis of complex experiments with multiple factors. 
Readers are referred to the user's guide in \edger{} for more details on parametrization.

<<>>=
design
@

\section{Estimating the dispersions}

\subsection{Stabilising estimates with empirical Bayes}
\label{sec:dispest}
Under the QL framework, both the QL and NB dispersions are used to model biological variability in the data \citep{lund2012}. 
The former ensures that the NB mean-variance relationship is properly specified with appropriate contributions from the Poisson and Gamma components. 
The latter accounts for variability and uncertainty in the dispersion estimate. 
However, limited replication in most ChIP-seq experiments means that each window does not contain enough information for precise estimation of either dispersion. 

% Both parameters need to be estimated for optimal performance; using too high
% a value for the NB dispersion means that the QL dispersion can't recover (as
% it's very sensitive to the former). Also, using a constant value (e.g. 0, a
% la quasi-poisson) puts a lot of pressure on the trend fitting as you're
% trying to shoehorn a NB mean-variance relationship into a QL mean-varince
% relationship (asymptotically the same, but different at low counts).

This problem is overcome in \edger{} by sharing information across windows. 
For the NB dispersions, a mean-dispersion trend is fitted across all windows to model the mean-variance relationship \citep{mccarthy2012}. 
The raw QL dispersion for each window is estimated after fitting a GLM with the trended NB dispersion. 
Another mean-dependent trend is fitted to the raw QL estimates.  
An empirical Bayes (EB) strategy is then used to stabilize the raw QL dispersion estimates by shrinking them towards the second trend \citep{lund2012}. 
The ideal amount of shrinkage is determined from the heteroskedasticity of the data.

<<eval=FALSE,label=shrinkx>>=
par(mfrow=c(1,2))
y <- estimateDisp(y, design)
o <- order(y$AveLogCPM)
plot(y$AveLogCPM[o], sqrt(y$trended.dispersion[o]), type="l", lwd=2,
  ylim=c(0, 1), xlab=expression("Ave."~Log[2]~"CPM"), 
  ylab=("Biological coefficient of variation"))
fit <- glmQLFit(y, design, robust=TRUE)
plotQLDisp(fit)
@

The effect of EB stabilisation can be visualized by examining the biological coefficient of variation (for the NB dispersion) and the quarter-root deviance (for the QL dispersion). 
These plots can also be used to decide whether the fitted trend is appropriate. 
Sudden irregulaties may be indicative of an underlying structure in the data which cannot be modelled with the mean-dispersion trend. 
Discrete patterns in the raw dispersions are indicative of low counts and suggest that more aggressive filtering is required.

\setkeys{Gin}{width=0.99\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE,width=10,height=5>>=
<<shrinkx>>
@
\end{center}

A strong trend may also be observed where the dispersion drops sharply with increasing average abundance.
This is due to the disproportionate impact of artifacts such as mapping errors and PCR duplicates at low counts. 
It is difficult to accurately fit an empirical curve to these strong trends.  
As a consequence, the dispersions at high abundances may be overestimated. 
Filtering of low-abundance regions (as described in Chapter~\ref{chap:filter}) provides some protection by removing the strongest part of the trend.
Users can compare raw and filtered results to see whether this makes any difference.
Such filtering has an additional benefit of removing those tests that have low power due to the magnitude of the dispersions.

<<label=rmchecker,eval=FALSE>>=
relevant <- rowSums(assay(original)) >= 20 # some filtering; otherwise, it takes too long.
yo <- asDGEList(original[relevant], norm.factors=normfacs)
yo <- estimateDisp(yo, design)
oo <- order(yo$AveLogCPM)
plot(yo$AveLogCPM[oo], sqrt(yo$trended.dispersion[oo]), type="l", lwd=2,
  ylim=c(0, max(sqrt(yo$trended))), xlab=expression("Ave."~Log[2]~"CPM"), 
  ylab=("Biological coefficient of variation"))
lines(y$AveLogCPM[o], sqrt(y$trended[o]), lwd=2, col="grey")
legend("topright", c("raw", "filtered"), col=c("black", "grey"), lwd=2)
@

\setkeys{Gin}{width=0.8\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE>>=
<<rmchecker>>
@
\end{center}

\subsection{Modelling heteroskedasticity}
The heteroskedasticity of the data is modelled in \edger{} by the prior degrees of freedom (d.f.).
A large value for the prior d.f. indicates that heteroskedasticity is low. 
This means that more EB shrinkage can be performed to reduce uncertainty and maximize power. 
However, strong shrinkage is not appropriate if the dispersions are highly variable. 
Fewer prior degrees of freedom (and less shrinkage) are required to maintain type I error control. 

<<>>=
summary(fit$df.prior)
@

On occasion, the estimated prior degrees of freedom will be infinite. 
This is indicative of a strong batch effect where the dispersions are consistently large.
A typical example involves uncorrected differences in IP efficiency across replicates. 
In severe cases, the trend may fail to pass through the bulk of points as the variability is too low to be properly modelled in the QL framework.
This problem is usually resolved with appropriate normalization.

Note that the prior degrees of freedom should be robustly estimated \citep{phipson2013}. 
Obviously, this protects against large positive outliers (e.g., highly variable windows) but it also protects against near-zero dispersions at low counts. 
These will manifest as large negative outliers after a log transformation step during estimation \citep{smyth2004}. 
Without robustness, incorporation of these outliers will inflate the observed variability in the dispersions.
This results in a lower estimated prior d.f. and reduced DB detection power.

% If you've forgotten, you get near-zero dispersions because counts can be exactly equal.

\section{Testing for DB windows}
The effect of specific factors can be tested to identify windows with significant differential binding. 
In the QL framework, $p$-values are computed using the F-test \citep{lund2012}. 
This is more appropriate than using the likelihood ratio test as the F-test accounts for uncertainty in the dispersion estimates. 
Associated statistics such as log-fold changes and log-counts per million are also computed for each window.

<<>>=
results <- glmQLFTest(fit, contrast=c(0, 1))
head(results$table)
@

The null hypothesis here is that the cell type has no effect. 
The \code{contrast} argument in the \code{glmQLFTest} function specifies which factors are of interest. 
In this case, a contrast of \code{c(0, 1)} defines the null hypothesis as \code{0*intercept + 1*cell.type = 0}, i.e., that the log-fold change between cell types is zero. 
DB windows can then be identified by rejecting the null. 
Specification of the contrast is explained in greater depth in the \edger{} user's manual. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Correction for multiple testing}

\begin{combox}
All right, we're almost there. 
This chapter needs the \code{results} object from the last chapter. 
You'll also need the filtered \code{data} list from Chapter~\ref{chap:count} (see the comment box in the last chapter), as well as the \code{broads} object from Chapter~\ref{chap:filter}. 
\end{combox}

\section{Problems with false discovery rate control}
The false discovery rate (FDR) is usually the most appropriate measure of error for high-throughput experiments. 
Control of the FDR can be provided by applying the Benjamini-Hochberg (BH) method \citep{benjamini1995} to a set of $p$-values. 
This is less conservative than the alternatives (e.g., Bonferroni) yet still provides some measure of error control. 
The most obvious approach is to apply the BH method to the set of $p$-values across all windows. 
This will control the FDR across the set of putative DB windows.

However, the FDR across all detected windows is not necessarily the most relevant error rate. 
Interpretation of ChIP-seq experiments is more concerned with regions of the genome in which (differential) protein binding is found, rather than the individual windows. 
In other words, the FDR across all detected DB regions is usually desired. 
This is not equivalent to that across all DB windows as each region will often consist of multiple overlapping windows.
Control of one will not guarantee control of the other \citep{lun2014}.

To illustrate this difference, consider an analysis where the FDR across all window positions is controlled at 10\%. 
In the results, there are 18 adjacent window positions forming one cluster and 2 windows forming a separate cluster.
Each cluster represents a region. 
The first set of windows is a truly DB region whereas the second set is a false positive. 
A window-based interpretation of the FDR is correct as only 2 of the 20 window positions are false positives.
However, a region-based interpretation results in an actual FDR of 50\%.

% The BH method is particularly popular as it is simple to apply and robust to
% correlations \citep{reiner2003,kim2008}.  Simes' is also pretty robust to
% correlations \citep{samuel1996,sarkar1997}, in the same respect as the FDR.
% Say you control the FDR within a cluster using the BH method, so
% E(FDR)<=0.05. Now, the probability of all false positives (i.e. FDR=1) must
% be under 0.05 as well. So, if the BH method works, so does Simes' method.

\section{Restoring FDR control with clustered windows}
Misinterpretation of the FDR can be avoided by obtaining a single $p$-value for each region.
In particular, several strategies can be used to cluster adjacent windows into regions.
A combined $p$-value can then be computed for each cluster, based on the $p$-values of the constituent windows \citep{simes1986}.
This tests the joint null hypothesis for each cluster, i.e., that no enrichment is observed across any sites within the corresponding region. 
The combined $p$-values are then adjusted using the BH method to control the region-level FDR.

An alternative approach is to choose a single window to represent each cluster/region.
For example, the window with the highest average abundance in each cluster can be used.
This is sensible for analyses involving sharp binding events, where each cluster is expected to be small and contain no more than one binding site.
Thus, a single window (and $p$-value) can reasonably be used as a representative of the entire region.
The BH method can then be applied to the corresponding $p$-values of the representative windows from all clusters.

Both approaches are available in the \pkgname{} package.
The combining procedure is known as Simes' method and is implemented in the \code{combineTests} function.
Similarly, selection of a representative window can be performed using the \code{getBestTest} function.
Examples of their usage are shown below, along with demonstrations of the different clustering strategies.

\section{Clustering with external information}
Combined $p$-values can be computed for a pre-defined set of regions based on the windows overlapping those regions. 
The most obvious source of pre-defined regions is that of annotated features such as promoters or gene bodies.
Alternatively, called peaks can be used provided that sufficient care has been taken to avoid loss of error control from data snooping \citep{lun2014}.
In either case, the \code{findOverlaps} function from the \granges{} package can be used to identify all windows in or overlapping each specified region. 

<<>>=
olap <- findOverlaps(broads, rowRanges(data))
olap
@

The \code{combineTests} function can be used to combine the $p$-values for all windows in each region. 
This yields a single combined $p$-value (and its BH-adjusted value) for each region. 
The average log-CPM and log-FC across all windows in each region are also computed.

<<>>=
tabbroad <- combineTests(queryHits(olap), results$table[subjectHits(olap),])
head(tabbroad)
@

The row names of the output table correspond to the cluster identifiers supplied in \code{ids}.
These should, in turn, act as indices for the regions in \code{broads}.
It is usually worth subsetting \code{broads} so that each entry matches a row of \code{tabcom}, prior to any downstream operations.
This is because not all entries in \code{broads} may be overlapped by a window in \code{data}.

<<>>=
used.broads <- broads[as.integer(rownames(data))]
@

At this point, one might imagine that it would be simpler to just collect and analyze counts over the pre-defined regions. 
This is a valid strategy but will yield different results. 
Consider a promoter containing two separate sites that are identically DB in opposite directions. 
Counting reads across the promoter will give equal counts for each condition so changes within the promoter will not be detected. 
Similarly, imprecise boundaries for called peaks can lead to loss of DB detection power due to ``contamination'' by reads from background regions. 
In both cases, window-based methods may be more robust as each interval of the promoter/peak region is examined separately \citep{lun2014}.

\section{Quick and dirty clustering}
\label{sec:cluster}
Clustering can also be performed inside \pkgname{} with a simple single-linkage algorithm, implemented in the \code{mergeWindows} function.
This approach is useful as it avoids potential problems with the other clustering strategies, e.g., peak-calling errors, incorrect or incomplete annotation. 
Briefly, all high-abundance windows that are less than some distance apart - say, 1 kbp - are put in the same cluster. 
The chosen distance reflects some arbitrary minimum distance at which two binding events are considered to be separate sites.

<<>>=
merged <- mergeWindows(rowRanges(data), tol=1000L)
merged$region
@

A combined $p$-value is computed for each cluster with the \code{combineTests} function.
The BH method is then applied to control the FDR across all detected clusters.
Like before, the row names in the output table are indices for the corresponding clusters in \code{merged\$regions}. 
However, no subsetting is required here as all clusters reported by \code{mergeWindows} should be non-empty.
This ensures that each cluster will be represented by a row in the table.

<<>>=
tabcom <- combineTests(merged$id, results$table)
head(tabcom)
@

If many adjacent windows are present, very large clusters may be formed that are difficult to interpret. 
A simple check can be used to determine whether most clusters are of an acceptable size. 
Huge clusters indicate that more aggressive filtering from Chapter~\ref{chap:filter} is required.  
This mitigates chaining effects by reducing the density of windows in the genome.

% Note that several large clusters may still be present due to high coverage within long tandem repeat loci.  
% In general, chaining isn't as bad as single-linkage on the reads themselves, because windows that survive weak filtering should have reasonably high read counts. 

<<>>=
summary(width(merged$region))
@

Alternatively, chaining can be limited by setting \code{max.width} to restrict the size of the merged intervals. 
Clusters substantially larger than \code{max.width} are split into several smaller subclusters of roughly equal size.
The chosen value should be small enough so as to separate DB regions from unchanged neighbours, yet large enough to avoid misinterpretation of the FDR.
Any value from 2000 to 10000 bp is recommended. 
This paramater can also interpreted as the maximum distance at which two binding sites are considered part of the same event.

<<>>=
merged.max <- mergeWindows(rowRanges(data), tol=1000L, max.width=5000L)
summary(width(merged.max$region))
@

There are also provisions for clustering based on the sign of the log-fold change. 
The idea is that clusters will be broken up wherever the sign changes. 
This will separate binding sites that are close together but are changing in opposite directions. 
A vector can be supplied in \code{sign} to indicate whether each window has a positive log-fold change.

<<>>=
merged.sign <- mergeWindows(rowRanges(data), tol=1000L, sign=(results$table$logFC > 0))
summary(width(merged.sign$region))
@

For routine analyses, sign-based filtering is not recommended as the sign of windows within each cluster is not independent of their DB status. 
Windows in a genuine DB region will form one cluster (consistent sign) whereas those in non-DB regions will form many clusters (inconsistent sign, as the log-fold change is small).
This results in conservativeness as more clusters will have large $p$-values.
Furthermore, any attempt to filter away small clusters will cause liberalness if too many large $p$-values are lost.

\section{Integrating results from multiple window sizes}
\label{sec:bin_integrate}
The sensitivity of the analysis to the choice of window size can be mitigated by testing a range of different widths.
DB results from each width can be integrated by clustering adjacent windows together (even if they are of differing sizes), and combining $p$-values within each of the resulting clusters.
The example below uses the H3 acetylation data from Chapter~\ref{chap:norm}.
Some filtering is performed to avoid excessive chaining in this demonstration.
Corresponding tables of DB results should also be obtained (for brevity, these will be dummied up here).

<<>>=
h3.files <-  c("h3ac.bam", "h3ac_2.bam")
ac.small <- windowCounts(h3.files, width=150L, spacing=100L, filter=25)
ac.large <- windowCounts(h3.files, width=1000L, spacing=500L, filter=35)
ns <- nrow(ac.small)
dummy.small <- data.frame(logFC=rnorm(ns), logCPM=0, PValue=runif(ns)) 
nl <- nrow(ac.large)
dummy.large <- data.frame(logFC=rnorm(nl), logCPM=0, PValue=runif(nl)) 
@

The \code{consolidateSizes} function can then be applied to combine these results.
This merges windows of all sizes into a single set of clusters, and computes a combined $p$-value from the associated $p$-values for each cluster.
However, if a cluster contains many small windows, the DB results for the small window size will contribute most to the combined $p$-value.
This is not ideal when results from all window sizes are of equal interest.
Equal contributions from each window size can be enforced by setting \code{equiweight=TRUE}, whereby a weighted version of Simes' method \citep{benjamini1997} is used.
The weight assigned to each window is inversely proportional to the number of windows of that size in the same cluster.

<<>>=
cons <- consolidateSizes(data.list=list(ac.small, ac.large), 
    result.list=list(dummy.small, dummy.large), equiweight=TRUE)
cons$region
@

In this manner, DB results from multiple window widths can be gathered together and reported as a single set of regions.
This is most useful for histone marks and other analyses involving diffuse regions of enrichment.
For such studies, the ideal bin size is not known or may not even exist, e.g., if the widths of the enriched regions are variable.

\section{Choosing a single representative window}

\subsection{Based on differential binding}
In some cases, it may be necessary to also report the window in which the strongest DB is found.
This information can be useful for identifying the change in binding within large clusters, or to narrow down the relevant sequence for motif discovery.
Identification of the most significant (i.e., ``best'') window can be performed using the \code{getBestTest} function.
This reports the index of the window with the lowest $p$-value in each cluster.

<<>>=
tab.best <- getBestTest(merged$id, results$table)
head(tab.best)
@

A common use of the \code{best} index is to obtain the log-fold change of the best window in each cluster.
This fold change is often more useful than that reported by \code{combineTests}.
The latter is an average across all windows in the cluster and will have little meaning for large clusters.
Reporting the \code{best} fold change may be preferable as it focuses on the DB interval.

<<>>=
tabcom$best.fc <- results$table$logFC[tab.best$best]
tabcom$best.start <- start(rowRanges(data)[tab.best$best])
head(tabcom[,c("best.fc", "best.start")])
@

Typically, the best window will only be used as a descriptive measure for each cluster.
However, more statistical rigour is necessary if they are treated as the features of interest over which the error rate is to be controlled.
A Bonferroni correction is applied to the $p$-value of each best window to obtain the corresponding \code{PValue} in \code{tab.best}.
This is necessary to account for the implicit multiple testing across all windows in each cluster.

\subsection{Based on average abundance}
Alternatively, the best window can be defined as the one with the highest average abundance in each cluster.
This represents the window with the strongest binding for the target protein, though not necessarily the strongest DB.
As the average abundance is independent of the $p$-value, no correction for multiple testing within each cluster is necessary.
For sharp binding events, it may be preferable to restrict the DB analysis to these windows.
This will usually provide more power as it avoids the conservativeness of Simes' method.

<<>>=
tab.ave <- getBestTest(merged$id, results$table, by.pval=FALSE)
head(tab.ave)
@

The obvious drawback is that information is lost when the analysis is restricted to a single window.
Detection will fail if the DB event does not occur at the most abundant window.
This is relevant for consideration of complex events, e.g., adjacent TF binding sites.
Loss of information also increases the sensitivity of the analysis to the clustering procedure.
For example, a DB site will be ignored if it is clustered alongside a stronger non-DB site.

A more graduated approach uses the weighted version of Simes' method, where the window with the highest abundance is upweighted.
In the example below, the weight assigned to the top window is increased relative to that of other windows in the same cluster.
This means that the behaviour of the top window will have a greater influence on the final combined $p$-value.
That said, the other windows in the cluster still have unity weights.
Any DB events in those windows will still be considered when the $p$-values are combined.

<<>>=
weights <- upweightSummit(merged$id, tab.ave$best)
head(weights)
tabcom.w <- combineTests(merged$id, results$table, weight=weights)
head(tabcom.w)
@

Finally, a less formal approach is to simply report the log-fold change of the most abundant window in each cluster.
This will indicate whether any DB is present at the strongest binding site within the corresponding genomic interval.
If not, the DB event may be complex, e.g., involving multiple peaks or peaks that change in shape or position between conditions.
These require more careful interpretation than simpler ``binary'' (i.e., on/off) changes.

<<>>=
# 'mab' stands for most abundant.
tabcom$mab.fc <- results$table$logFC[tab.ave$best]
head(tabcom$mab.fc)
@

\chapter{Post-processing steps}

\begin{combox}
This is where we bring it all together.
We'll need the \code{merged} list and the \code{tabcom} table from the previous chapter. 
There's a bit about visualization at the end where we need the \code{y} object from Chapter~\ref{chap:stats} and the \code{bam.files} that we started off with.
Oh, and the \code{org.Mm.eg.db} object that we loaded in Chapter~\ref{chap:filter}.
\end{combox}

\section{Adding gene-based annotation}
Annotation can be added to a given set of regions using the \code{detailRanges} function. 
This will identify overlaps between the regions and annotated genomic features such as exons, introns and promoters. 
Here, the promoter region of each gene is defined as some interval 3 kbp up- and 1 kbp downstream of the TSS for that gene. 
Any exonic features within \code{dist} on the left or right side of each supplied region will also be reported.

<<>>=
require(org.Mm.eg.db)
anno <- detailRanges(merged$region, txdb=TxDb.Mmusculus.UCSC.mm10.knownGene,
   orgdb=org.Mm.eg.db, promoter=c(3000, 1000), dist=5000)
head(anno$overlap)
head(anno$left)
head(anno$right)
@

Character vectors of compact string representations are provided to summarize the features overlapped by each supplied region. 
Each pattern contains \code{GENE|EXONS|STRAND} to describe the strand and overlapped exons of that gene. 
Promoters are labelled as exon \code{0} whereas introns are labelled as \code{I}. 
For \code{left} and \code{right}, an additional \code{DISTANCE} field is included.
This indicates the gap between the annotated feature and the supplied region.

While the string representation saves space in the output, it is not easy to work with.
If the annotation needs to manipulated directly, users can obtain it from the \code{detailRanges} command by not specifying the regions of interest. 
This can then be used for interactive manipulation, e.g., to identify all genes where the promoter contains DB sites.

<<>>=
anno.ranges <- detailRanges(txdb=TxDb.Mmusculus.UCSC.mm10.knownGene, orgdb=org.Mm.eg.db)
anno.ranges
@

\section{Saving the results to file}
It is a simple matter to save the results for later perusal. 
This is done here in the \code{*.tsv} format where all detail is preserved. 
Compression is used to reduce the file size. 

<<>>=
ofile <- gzfile("clusters.gz", open="w")
write.table(data.frame(as.data.frame(merged$region)[,1:3], tabcom, anno), 
    file=ofile, row.names=FALSE, quote=FALSE, sep="\t")
close(ofile)
@

Of course, other formats can be used depending on the purpose of the file. 
For example, significantly DB regions can be exported to BED files through the \rtracklayer{} package for visual inspection with genomic browsers.
A transformed FDR is used here for the score field.

<<>>=
is.sig <- tabcom$FDR <= 0.05
require(rtracklayer)
test <- merged$region[is.sig]
test$score <- -10*log10(tabcom$FDR[is.sig])
names(test) <- paste0("region", 1:sum(is.sig))
export(test, "clusters.bed")
head(read.table("clusters.bed"))
@

\section{Simple visualization of genomic coverage}
Visualization of the read depth around interesting features is often desired.
This is facilitated by the \code{extractReads} function, which pulls out the reads from the BAM file.
The returned \code{GRanges} object can then be used to plot the sequencing coverage or any other statistic of interest.
Note that the \code{extractReads} function also accepts a \code{readParam} object.
This ensures that the same reads used in the analysis will be pulled out during visualization.

<<>>=
cur.region <- GRanges("chr18", IRanges(77806807, 77807165))
extractReads(cur.region, bam.files[1], param=readParam())
@

Here, coverage is visualized as the number of reads covering each base pair in the interval of interest.
Specifically, the reads-per-million is shown to allow comparisons between libraries of different size.
The plots themselves are constructed using methods from the \gviz{} package.
The blue and red tracks represent the coverage on the forward and reverse strands, respectively. 
Strong strand bimodality is consistent with a genuine TF binding site.
For paired-end data, coverage can be similarly plotted for fragments, i.e., proper read pairs.

<<eval=FALSE,label=regionplot>>=
require(Gviz)
collected <- list()
for (i in 1:length(bam.files)) { 
    reads <- extractReads(cur.region, bam.files[i])
    pcov <- as(coverage(reads[strand(reads)=="+"])/data$totals[i]*1e6, "GRanges")
    ncov <- as(coverage(reads[strand(reads)=="-"])/data$totals[i]*1e6, "GRanges")
    ptrack <- DataTrack(pcov, type="histogram", lwd=0, fill=rgb(0,0,1,.4), ylim=c(0,1.1), 
        name=bam.files[i], col.axis="black", col.title="black")
    ntrack <- DataTrack(ncov, type="histogram", lwd=0, fill=rgb(1,0,0,.4), ylim=c(0,1.1))
    collected[[i]] <- OverlayTrack(trackList=list(ptrack,ntrack))
}
gax <- GenomeAxisTrack(col="black")
plotTracks(c(gax, collected), from=start(cur.region), to=end(cur.region))
@

\setkeys{Gin}{width=.8\textwidth}
\begin{center}
<<fig=TRUE,echo=FALSE>>=
<<regionplot>>
@
\end{center}

\chapter{Epilogue}

\begin{combox}
Congratulations on getting to the end. Here's a poem for your efforts.
\begin{quote}
There once was a man named Will \\
Who never ate less than his fill. \\
He ate meat and bread \\
Until he was fed \\
But died when he saw the bill. 
\end{quote}
\end{combox}

\section{Datasets}
\label{sec:dataset}

\subsection{Obtaining the FastQ files}
The main NFYA dataset used throughout the guide was first mentioned in Section~\ref{data:main}. 
This was generated by \cite{tiwari2012} and is available from the NCBI Gene Expression Omnibus (GEO) with the accession number GSE25532. 
FastQ files can be obtained from the Sequence Read Archive (SRA) with accession numbers of SRR074398 for \code{es\_1.bam}, SRR074399 for \code{es\_2.bam}, SRR074417 for \code{tn\_1.bam} and SRR074418 for \code{tn\_2.bam}.

The paired-end dataset used in Section~\ref{data:pet} was generated by \cite{pal2013} and is available from the NCBI GEO under the accession GSE43212.
The lone FastQ file can be obtained from the SRA with the accession SRR642390 for \code{example-pet.bam}.

All libraries used in Section~\ref{data:ccf} were generated by \cite{zhang2012} and are available from the NCBI GEO under the accession GSE31233. 
FastQ files can be obtained from the SRA under the accessions SRR330784 and SRR330785 for \code{h3ac.bam}; SRR330800 and SRR330801 for \code{h3k4me2.bam}; and SRR330814, SRR330815 and SRR330816 for \code{h3k27me3.bam}. 
Multiple FastQ files represent technical replicates that were merged into a single BAM file.

Finally, the H3K4me3 dataset in Section~\ref{data:norm} was generated by \cite{domingo2012} and is available under the accession GSE38046. 
FastQ files can be obtained from the SRA under the accessions SRR499732 and SRR499733 for \code{h3k4me3\_pro.bam}, and SRR499716 and SRR499717 for \code{h3k4me3\_mat.bam}. 
Again, technical replicates were merged together.
For H3ac, the FastQ file at SRR330786 was also downloaded and used as \code{h3ac\_2.bam}.
 
\subsection{Alignment and processing to produce BAM files}
Technically, each of the libraries described above are downloaded in the SRA format. 
These can be unpacked to yield FastQ files using the \code{fastq-dump} program from the SRA Toolkit (\url{http://www.ncbi.nlm.nih.gov/Traces/sra/?view=software}). 
For the lone paired-end library, users will need to specify \code{fastq-dump --split-files} to ensure that two separate files are produced, i.e., containing sequences from either end of each fragment.

Reads in the FastQ files were then aligned to the mm10 build of the mouse genome using subread v1.4.6 \citep{liao2013}. 
The subread software can be obtained from Bioconductor as the Rsubread package, or as a standalone C program from \url{http://subread.sourceforge.net}. 
The consensus threshold for alignment was set at 2 to accommodate short read lengths ($<$ 45 bp in all datasets).
Only unique alignments were reported, and any tied alignments were split by Hamming distance.
Default values were used for all other parameters.
Paired-end data was aligned by supplying both FastQ files to subread within the same run.

Once aligned, SAM files were converted to BAM files using SAMtools v0.1.19 \citep{li2009}. 
BAM files were position-sorted with the \code{samtools sort} command, and duplicate reads were marked using the \code{MarkDuplicates} command from the Picard suite v1.117 (\url{http://broadinstitute.github.io/picard}).
Any technical replicates were merged together using  \code{samtools merge} to form a single library. 
Indexing was performed using \code{samtools index}.

\section{Session information}
<<>>=
sessionInfo()
@

\section{References}

\bibliographystyle{plainnat}
\bibliography{ref_ug}

\end{document}

